horizon=96
maxconcurrent=1
gpu_fraction=$(echo "scale=2; 1/$maxconcurrent" | bc)  # Calculate GPU fraction with 2 decimal places
start_time=$(date +%s)  # Get the current time in seconds
python3 tune_timemixer.py \
    --model TimeMixer \
    --data ETTh1 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh1.csv \
    --features M \
    --seq_len 96 \
    --label_len 0 \
    --pred_len $horizon \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --d_model 16 \
    --d_ff 32 \
    --down_sampling_layers 3 \
    --down_sampling_window 2 \
    --down_sampling_method avg \
    --decomp_method moving_avg \
    --moving_avg 25 \
    --train_epochs 8 \
    --patience 3 \
    --num_workers 1 \
    --gpu 0 \
    --tune_search_algorithm random_search \
    --tune_trial_scheduler fifo \
    --tune_storage_path ./checkpoints/hptunning/random_search/ \
    --tune_experiment_name ETTh1_96_${horizon}_test \
    --tune_objective best_valid_loss \
    --tune_num_samples 10 \
    --tune_max_trial_time_s 100 \
    --tune_time_budget_s 14400 \
    --tune_max_concurrent $maxconcurrent \
    --tune_gpu_resources $gpu_fraction \
    --tune_cpu_resources 1 \
    --tune_default_config "{
        \"batch_size\": 128, \
        \"learning_rate\": 0.01, \
        \"down_sampling_method\": \"avg\", \
        \"d_model\": 16, \
        \"alpha_d_ff\": 2, \
        \"decomp_method\": \"moving_avg\", \
        \"moving_avg\": 25, \
        \"e_layers\": 2, \
        \"dropout\": 0.1
    }" \
    --tune_param_space "{
        \"batch_size\": [\"choice\", [16, 32, 64, 128]], \
        \"learning_rate\": [\"loguniform\", [0.0005, 0.012]], \
        \"down_sampling_method\": [\"choice\", [\"avg\", \"conv\"]], \
        \"d_model\": [\"choice\", [8, 16, 32, 64, 128, 256, 512]], \
        \"alpha_d_ff\": [\"choice\", [2, 3, 4]], \
        \"decomp_method\": [\"choice\", [\"moving_avg\", \"dft_decomp\"]], \
        \"moving_avg\": [\"choice\", [15, 25, 35, 55, 75]], \
        \"e_layers\": [\"choice\", [1, 2, 3, 4]], \
        \"dropout\": [\"normal\", [0.1, 0.025]]
    }" \
    --seed 123;
end_time=$(date +%s)  # Get the current time in seconds
elapsed_time=$((end_time - start_time))  # Calculate the elapsed time
echo ""
echo ""
echo "Time taken ($maxconcurrent parallel trials): $elapsed_time seconds"
echo ""
echo ""
done2024-08-23 05:15:12,666	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-23 05:15:13,132	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-23 05:15:13,138	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-23 05:15:16,282	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000000)
2024-08-23 05:15:17,012	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000001)
2024-08-23 05:15:17,740	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000002)
2024-08-23 05:15:18,472	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000003)
2024-08-23 05:15:19,240	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000004)
2024-08-23 05:15:20,009	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000005)
2024-08-23 05:15:20,787	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000006)
2024-08-23 05:15:21,560	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh1_96_96_test      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator â”‚
â”‚ Scheduler                        FIFOScheduler         â”‚
â”‚ Number of trials                 10                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-23_05-15-12_148610_13771/artifacts/2024-08-23_05-15-13/ETTh1_96_96_test/driver_artifacts`

Trial status: 1 PENDING
Current time: 2024-08-23 05:15:13. Total running time: 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00000   PENDING             128              0.01   avg                             16              2   moving_avg                  25            2         0.1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-e8d8b_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00000 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  2 â”‚
â”‚ batch_size                                128 â”‚
â”‚ d_model                                    16 â”‚
â”‚ decomp_method                      moving_avg â”‚
â”‚ down_sampling_method                      avg â”‚
â”‚ dropout                                   0.1 â”‚
â”‚ e_layers                                    2 â”‚
â”‚ learning_rate                            0.01 â”‚
â”‚ moving_avg                                 25 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=15980)[0m configuration
[36m(_train_fn pid=15980)[0m {'batch_size': 128, 'learning_rate': 0.01, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'moving_avg', 'moving_avg': 25, 'e_layers': 2, 'dropout': 0.1, 'd_ff': 32}
[36m(_train_fn pid=15980)[0m Use GPU: cuda:0
[36m(_train_fn pid=15980)[0m train 8449
[36m(_train_fn pid=15980)[0m val 2785
[36m(_train_fn pid=15980)[0m start_epoch 0
[36m(_train_fn pid=15980)[0m max_epoch 8
[36m(_train_fn pid=15980)[0m Updating learning rate to 0.01
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Validation loss decreased (inf --> 1.0509).  Saving model state dict ...
[36m(_train_fn pid=15980)[0m Epoch: 1 cost time: 0.8730509281158447
[36m(_train_fn pid=15980)[0m Epoch: 1, Steps: 66 | Train Loss: 0.6187816 Vali Loss: 1.0508947 Best vali loss: 1.0508947
[36m(_train_fn pid=15980)[0m Updating learning rate to 0.005
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Validation loss decreased (1.0509 --> 0.7180).  Saving model state dict ...
[36m(_train_fn pid=15980)[0m Epoch: 2 cost time: 0.6026251316070557
[36m(_train_fn pid=15980)[0m Epoch: 2, Steps: 66 | Train Loss: 0.3990616 Vali Loss: 0.7180091 Best vali loss: 0.7180091
[36m(_train_fn pid=15980)[0m Updating learning rate to 0.0025
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Validation loss decreased (0.7180 --> 0.7077).  Saving model state dict ...
[36m(_train_fn pid=15980)[0m Epoch: 3 cost time: 0.6015112400054932
[36m(_train_fn pid=15980)[0m Epoch: 3, Steps: 66 | Train Loss: 0.3565136 Vali Loss: 0.7076579 Best vali loss: 0.7076579
[36m(_train_fn pid=15980)[0m Updating learning rate to 0.00125
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Validation loss decreased (0.7077 --> 0.7049).  Saving model state dict ...
[36m(_train_fn pid=15980)[0m Epoch: 4 cost time: 0.6002216339111328
[36m(_train_fn pid=15980)[0m Epoch: 4, Steps: 66 | Train Loss: 0.3485572 Vali Loss: 0.7048644 Best vali loss: 0.7048644
[36m(_train_fn pid=15980)[0m Updating learning rate to 0.000625
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Validation loss decreased (0.7049 --> 0.7007).  Saving model state dict ...
[36m(_train_fn pid=15980)[0m Epoch: 5 cost time: 0.6426486968994141
[36m(_train_fn pid=15980)[0m Epoch: 5, Steps: 66 | Train Loss: 0.3443627 Vali Loss: 0.7007423 Best vali loss: 0.7007423
[36m(_train_fn pid=15980)[0m Updating learning rate to 0.0003125
[36m(_train_fn pid=15980)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Epoch: 6 cost time: 0.6471347808837891
[36m(_train_fn pid=15980)[0m Epoch: 6, Steps: 66 | Train Loss: 0.3421747 Vali Loss: 0.7044440 Best vali loss: 0.7007423
[36m(_train_fn pid=15980)[0m Updating learning rate to 0.00015625
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Validation loss decreased (0.7007 --> 0.7000).  Saving model state dict ...
[36m(_train_fn pid=15980)[0m Epoch: 7 cost time: 0.6519320011138916
[36m(_train_fn pid=15980)[0m Epoch: 7, Steps: 66 | Train Loss: 0.3408162 Vali Loss: 0.7000190 Best vali loss: 0.7000190

Trial trial-e8d8b_00000 completed after 8 iterations at 2024-08-23 05:15:21. Total running time: 8s
[36m(_train_fn pid=15980)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00000_0_2024-08-23_05-15-13/checkpoint_000007)
2024-08-23 05:15:44,755	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=16717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00001_1_alpha_d_ff=2,batch_size=16,d_model=512,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1144,e_lay_2024-08-23_05-15-21/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.77081 â”‚
â”‚ time_total_s                                 6.67042 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.69813 â”‚
â”‚ train_loss                                   0.33929 â”‚
â”‚ valid_loss                                   0.69813 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=15980)[0m Updating learning rate to 7.8125e-05
[36m(_train_fn pid=15980)[0m saving checkpoint...
[36m(_train_fn pid=15980)[0m Validation loss decreased (0.7000 --> 0.6981).  Saving model state dict ...
[36m(_train_fn pid=15980)[0m Epoch: 8 cost time: 0.6481449604034424
[36m(_train_fn pid=15980)[0m Epoch: 8, Steps: 66 | Train Loss: 0.3392925 Vali Loss: 0.6981350 Best vali loss: 0.6981350

Trial trial-e8d8b_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00001 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  2 â”‚
â”‚ batch_size                                 16 â”‚
â”‚ d_model                                   512 â”‚
â”‚ decomp_method                      moving_avg â”‚
â”‚ down_sampling_method                     conv â”‚
â”‚ dropout                               0.11443 â”‚
â”‚ e_layers                                    2 â”‚
â”‚ learning_rate                         0.00059 â”‚
â”‚ moving_avg                                 15 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=16717)[0m configuration
[36m(_train_fn pid=16717)[0m {'batch_size': 16, 'learning_rate': 0.0005932727207615168, 'down_sampling_method': 'conv', 'd_model': 512, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 2, 'dropout': 0.11442759478143129, 'd_ff': 1024}
[36m(_train_fn pid=16717)[0m Use GPU: cuda:0
[36m(_train_fn pid=16717)[0m train 8449
[36m(_train_fn pid=16717)[0m val 2785
[36m(_train_fn pid=16717)[0m start_epoch 0
[36m(_train_fn pid=16717)[0m max_epoch 8
[36m(_train_fn pid=16717)[0m 	iters: 100, epoch: 1 | loss: 0.4652256
[36m(_train_fn pid=16717)[0m 	speed: 0.0390s/iter; left time: 161.0393s
[36m(_train_fn pid=16717)[0m 	iters: 200, epoch: 1 | loss: 0.6465580
[36m(_train_fn pid=16717)[0m 	speed: 0.0341s/iter; left time: 137.4470s
[36m(_train_fn pid=16717)[0m 	iters: 300, epoch: 1 | loss: 0.5664296
[36m(_train_fn pid=16717)[0m 	speed: 0.0340s/iter; left time: 133.5684s
[36m(_train_fn pid=16717)[0m 	iters: 400, epoch: 1 | loss: 0.5403999
[36m(_train_fn pid=16717)[0m 	speed: 0.0341s/iter; left time: 130.4029s
[36m(_train_fn pid=16717)[0m 	iters: 500, epoch: 1 | loss: 0.5624627
[36m(_train_fn pid=16717)[0m 	speed: 0.0341s/iter; left time: 126.9668s

Trial status: 1 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:15:43. Total running time: 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00000 with best_valid_loss=0.6981349622919446 and params={'batch_size': 128, 'learning_rate': 0.01, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'moving_avg', 'moving_avg': 25, 'e_layers': 2, 'dropout': 0.1, 'd_ff': 32}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00001   RUNNING                16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428                                                                               â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=16717)[0m Updating learning rate to 0.0005932727207615168
[36m(_train_fn pid=16717)[0m saving checkpoint...
[36m(_train_fn pid=16717)[0m Validation loss decreased (inf --> 0.9614).  Saving model state dict ...
[36m(_train_fn pid=16717)[0m Epoch: 1 cost time: 18.21211290359497
[36m(_train_fn pid=16717)[0m Epoch: 1, Steps: 528 | Train Loss: 0.5519864 Vali Loss: 0.9613778 Best vali loss: 0.9613778
[36m(_train_fn pid=16717)[0m 	iters: 100, epoch: 2 | loss: 0.3068194
[36m(_train_fn pid=16717)[0m 	speed: 0.0669s/iter; left time: 240.7141s
[36m(_train_fn pid=16717)[0m 	iters: 200, epoch: 2 | loss: 0.4598771
[36m(_train_fn pid=16717)[0m 	speed: 0.0343s/iter; left time: 119.9199s
[36m(_train_fn pid=16717)[0m 	iters: 300, epoch: 2 | loss: 0.3016947
[36m(_train_fn pid=16717)[0m 	speed: 0.0343s/iter; left time: 116.5025s
[36m(_train_fn pid=16717)[0m 	iters: 400, epoch: 2 | loss: 0.4358748
2024-08-23 05:16:05,191	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=16717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00001_1_alpha_d_ff=2,batch_size=16,d_model=512,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1144,e_lay_2024-08-23_05-15-21/checkpoint_000001)
2024-08-23 05:16:25,737	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=16717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00001_1_alpha_d_ff=2,batch_size=16,d_model=512,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1144,e_lay_2024-08-23_05-15-21/checkpoint_000002)
[36m(_train_fn pid=16717)[0m 	speed: 0.0343s/iter; left time: 113.2386s
[36m(_train_fn pid=16717)[0m 	iters: 500, epoch: 2 | loss: 0.3104034
[36m(_train_fn pid=16717)[0m 	speed: 0.0344s/iter; left time: 109.8882s
[36m(_train_fn pid=16717)[0m Updating learning rate to 0.0002966363603807584
[36m(_train_fn pid=16717)[0m saving checkpoint...
[36m(_train_fn pid=16717)[0m Validation loss decreased (0.9614 --> 0.7225).  Saving model state dict ...
[36m(_train_fn pid=16717)[0m Epoch: 2 cost time: 18.13183355331421
[36m(_train_fn pid=16717)[0m Epoch: 2, Steps: 528 | Train Loss: 0.3947398 Vali Loss: 0.7225034 Best vali loss: 0.7225034
[36m(_train_fn pid=16717)[0m 	iters: 100, epoch: 3 | loss: 0.2739897
[36m(_train_fn pid=16717)[0m 	speed: 0.0673s/iter; left time: 206.4128s
[36m(_train_fn pid=16717)[0m 	iters: 200, epoch: 3 | loss: 0.3532106
[36m(_train_fn pid=16717)[0m 	speed: 0.0345s/iter; left time: 102.3192s
Trial status: 1 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:16:13. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00000 with best_valid_loss=0.6981349622919446 and params={'batch_size': 128, 'learning_rate': 0.01, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'moving_avg', 'moving_avg': 25, 'e_layers': 2, 'dropout': 0.1, 'd_ff': 32}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00001   RUNNING                16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        2           41.348         0.39474        0.722503            0.722503 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=16717)[0m 	iters: 300, epoch: 3 | loss: 0.2421341
[36m(_train_fn pid=16717)[0m 	speed: 0.0345s/iter; left time: 99.0126s
[36m(_train_fn pid=16717)[0m 	iters: 400, epoch: 3 | loss: 0.3132412
[36m(_train_fn pid=16717)[0m 	speed: 0.0346s/iter; left time: 95.7425s
[36m(_train_fn pid=16717)[0m 	iters: 500, epoch: 3 | loss: 0.3448349
[36m(_train_fn pid=16717)[0m 	speed: 0.0345s/iter; left time: 92.1247s
[36m(_train_fn pid=16717)[0m Updating learning rate to 0.0001483181801903792
[36m(_train_fn pid=16717)[0m saving checkpoint...
[36m(_train_fn pid=16717)[0m Validation loss decreased (0.7225 --> 0.6986).  Saving model state dict ...
[36m(_train_fn pid=16717)[0m Epoch: 3 cost time: 18.234097003936768
[36m(_train_fn pid=16717)[0m Epoch: 3, Steps: 528 | Train Loss: 0.3596822 Vali Loss: 0.6986048 Best vali loss: 0.6986048
[36m(_train_fn pid=16717)[0m 	iters: 100, epoch: 4 | loss: 0.3192931
[36m(_train_fn pid=16717)[0m 	speed: 0.0676s/iter; left time: 171.6914s
[36m(_train_fn pid=16717)[0m 	iters: 200, epoch: 4 | loss: 0.3122381
[36m(_train_fn pid=16717)[0m 	speed: 0.0346s/iter; left time: 84.5410s
[36m(_train_fn pid=16717)[0m 	iters: 300, epoch: 4 | loss: 0.3266008
[36m(_train_fn pid=16717)[0m 	speed: 0.0346s/iter; left time: 81.0419s
[36m(_train_fn pid=16717)[0m 	iters: 400, epoch: 4 | loss: 0.4287880
[36m(_train_fn pid=16717)[0m 	speed: 0.0346s/iter; left time: 77.5681s
[36m(_train_fn pid=16717)[0m 	iters: 500, epoch: 4 | loss: 0.3286416
[36m(_train_fn pid=16717)[0m 	speed: 0.0346s/iter; left time: 74.1557s
Trial status: 1 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:16:43. Total running time: 1min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00000 with best_valid_loss=0.6981349622919446 and params={'batch_size': 128, 'learning_rate': 0.01, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'moving_avg', 'moving_avg': 25, 'e_layers': 2, 'dropout': 0.1, 'd_ff': 32}
2024-08-23 05:16:46,319	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=16717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00001_1_alpha_d_ff=2,batch_size=16,d_model=512,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1144,e_lay_2024-08-23_05-15-21/checkpoint_000003)
2024-08-23 05:17:06,946	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=16717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00001_1_alpha_d_ff=2,batch_size=16,d_model=512,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1144,e_lay_2024-08-23_05-15-21/checkpoint_000004)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00001   RUNNING                16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        3           61.8888        0.359682       0.698605            0.698605 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=16717)[0m Updating learning rate to 7.41590900951896e-05
[36m(_train_fn pid=16717)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=16717)[0m saving checkpoint...
[36m(_train_fn pid=16717)[0m Epoch: 4 cost time: 18.296037197113037
[36m(_train_fn pid=16717)[0m Epoch: 4, Steps: 528 | Train Loss: 0.3524557 Vali Loss: 0.7190252 Best vali loss: 0.6986048
[36m(_train_fn pid=16717)[0m 	iters: 100, epoch: 5 | loss: 0.3570281
[36m(_train_fn pid=16717)[0m 	speed: 0.0673s/iter; left time: 135.5750s
[36m(_train_fn pid=16717)[0m 	iters: 200, epoch: 5 | loss: 0.3677946
[36m(_train_fn pid=16717)[0m 	speed: 0.0346s/iter; left time: 66.2511s
[36m(_train_fn pid=16717)[0m 	iters: 300, epoch: 5 | loss: 0.3345264
[36m(_train_fn pid=16717)[0m 	speed: 0.0346s/iter; left time: 62.8104s
[36m(_train_fn pid=16717)[0m 	iters: 400, epoch: 5 | loss: 0.3621140
[36m(_train_fn pid=16717)[0m 	speed: 0.0348s/iter; left time: 59.6759s
[36m(_train_fn pid=16717)[0m 	iters: 500, epoch: 5 | loss: 0.4230628
[36m(_train_fn pid=16717)[0m 	speed: 0.0347s/iter; left time: 55.9244s

Trial trial-e8d8b_00001 completed after 5 iterations at 2024-08-23 05:17:06. Total running time: 1min 53s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                            20.62297 â”‚
â”‚ time_total_s                               103.08875 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                               0.6986 â”‚
â”‚ train_loss                                   0.34915 â”‚
â”‚ valid_loss                                   0.70033 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=16717)[0m Updating learning rate to 3.70795450475948e-05
[36m(_train_fn pid=16717)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=16717)[0m saving checkpoint...

Trial trial-e8d8b_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00002 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  4 â”‚
â”‚ batch_size                                 16 â”‚
â”‚ d_model                                    16 â”‚
â”‚ decomp_method                      dft_decomp â”‚
â”‚ down_sampling_method                      avg â”‚
â”‚ dropout                               0.10243 â”‚
â”‚ e_layers                                    4 â”‚
â”‚ learning_rate                         0.00941 â”‚
â”‚ moving_avg                                 75 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=17254)[0m configuration
[36m(_train_fn pid=17254)[0m {'batch_size': 16, 'learning_rate': 0.009405484035133074, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'dft_decomp', 'moving_avg': 75, 'e_layers': 4, 'dropout': 0.10242918296676143, 'd_ff': 64}
[36m(_train_fn pid=17254)[0m Use GPU: cuda:0
[36m(_train_fn pid=17254)[0m train 8449
[36m(_train_fn pid=17254)[0m val 2785
[36m(_train_fn pid=17254)[0m start_epoch 0
[36m(_train_fn pid=17254)[0m max_epoch 8
[36m(_train_fn pid=17254)[0m 	iters: 100, epoch: 1 | loss: 0.4688734
[36m(_train_fn pid=17254)[0m 	speed: 0.0219s/iter; left time: 90.3299s
[36m(_train_fn pid=17254)[0m 	iters: 200, epoch: 1 | loss: 0.4064465
[36m(_train_fn pid=17254)[0m 	speed: 0.0139s/iter; left time: 55.7906s

Trial status: 2 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:17:13. Total running time: 2min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00000 with best_valid_loss=0.6981349622919446 and params={'batch_size': 128, 'learning_rate': 0.01, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'moving_avg', 'moving_avg': 25, 'e_layers': 2, 'dropout': 0.1, 'd_ff': 32}
2024-08-23 05:17:17,962	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=17254)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00002_2_alpha_d_ff=4,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1024,e_layer_2024-08-23_05-17-06/checkpoint_000000)
[36m(_train_fn pid=17254)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00002_2_alpha_d_ff=4,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1024,e_layer_2024-08-23_05-17-06/checkpoint_000001)
[36m(_train_fn pid=17254)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00002_2_alpha_d_ff=4,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1024,e_layer_2024-08-23_05-17-06/checkpoint_000002)
[36m(_train_fn pid=17254)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00002_2_alpha_d_ff=4,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1024,e_layer_2024-08-23_05-17-06/checkpoint_000003)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00002   RUNNING                16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429                                                                               â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089         0.349154       0.70033             0.698605 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=17254)[0m 	iters: 300, epoch: 1 | loss: 0.4616687
[36m(_train_fn pid=17254)[0m 	speed: 0.0137s/iter; left time: 53.9191s
[36m(_train_fn pid=17254)[0m 	iters: 400, epoch: 1 | loss: 0.3983294
[36m(_train_fn pid=17254)[0m 	speed: 0.0137s/iter; left time: 52.4763s
[36m(_train_fn pid=17254)[0m 	iters: 500, epoch: 1 | loss: 0.3231424
[36m(_train_fn pid=17254)[0m 	speed: 0.0138s/iter; left time: 51.2923s
[36m(_train_fn pid=17254)[0m Updating learning rate to 0.009405484035133074
[36m(_train_fn pid=17254)[0m saving checkpoint...
[36m(_train_fn pid=17254)[0m Validation loss decreased (inf --> 0.7087).  Saving model state dict ...
[36m(_train_fn pid=17254)[0m Epoch: 1 cost time: 7.787015676498413
[36m(_train_fn pid=17254)[0m Epoch: 1, Steps: 528 | Train Loss: 0.4257562 Vali Loss: 0.7086708 Best vali loss: 0.7086708
[36m(_train_fn pid=17254)[0m 	iters: 100, epoch: 2 | loss: 0.4112108
[36m(_train_fn pid=17254)[0m 	speed: 0.0258s/iter; left time: 92.6745s
[36m(_train_fn pid=17254)[0m 	iters: 200, epoch: 2 | loss: 0.3663611
[36m(_train_fn pid=17254)[0m 	speed: 0.0128s/iter; left time: 44.9150s
[36m(_train_fn pid=17254)[0m 	iters: 300, epoch: 2 | loss: 0.4561366
[36m(_train_fn pid=17254)[0m 	speed: 0.0128s/iter; left time: 43.5158s
[36m(_train_fn pid=17254)[0m 	iters: 400, epoch: 2 | loss: 0.4147856
[36m(_train_fn pid=17254)[0m 	speed: 0.0128s/iter; left time: 42.2235s
[36m(_train_fn pid=17254)[0m 	iters: 500, epoch: 2 | loss: 0.3370486
[36m(_train_fn pid=17254)[0m 	speed: 0.0129s/iter; left time: 41.2567s
[36m(_train_fn pid=17254)[0m Updating learning rate to 0.004702742017566537
[36m(_train_fn pid=17254)[0m saving checkpoint...
[36m(_train_fn pid=17254)[0m Validation loss decreased (0.7087 --> 0.7022).  Saving model state dict ...
[36m(_train_fn pid=17254)[0m Epoch: 2 cost time: 6.819452524185181
[36m(_train_fn pid=17254)[0m Epoch: 2, Steps: 528 | Train Loss: 0.3619268 Vali Loss: 0.7021947 Best vali loss: 0.7021947
[36m(_train_fn pid=17254)[0m 	iters: 100, epoch: 3 | loss: 0.3385947
[36m(_train_fn pid=17254)[0m 	speed: 0.0257s/iter; left time: 78.9381s
[36m(_train_fn pid=17254)[0m 	iters: 200, epoch: 3 | loss: 0.3622136
[36m(_train_fn pid=17254)[0m 	speed: 0.0127s/iter; left time: 37.6701s
[36m(_train_fn pid=17254)[0m 	iters: 300, epoch: 3 | loss: 0.3593127
[36m(_train_fn pid=17254)[0m 	speed: 0.0126s/iter; left time: 36.1071s
[36m(_train_fn pid=17254)[0m 	iters: 400, epoch: 3 | loss: 0.2768235
[36m(_train_fn pid=17254)[0m 	speed: 0.0126s/iter; left time: 34.9185s
[36m(_train_fn pid=17254)[0m 	iters: 500, epoch: 3 | loss: 0.3626947
[36m(_train_fn pid=17254)[0m 	speed: 0.0126s/iter; left time: 33.7414s
[36m(_train_fn pid=17254)[0m Updating learning rate to 0.0023513710087832685
[36m(_train_fn pid=17254)[0m saving checkpoint...
[36m(_train_fn pid=17254)[0m Validation loss decreased (0.7022 --> 0.6816).  Saving model state dict ...
[36m(_train_fn pid=17254)[0m Epoch: 3 cost time: 6.729980945587158
[36m(_train_fn pid=17254)[0m Epoch: 3, Steps: 528 | Train Loss: 0.3400265 Vali Loss: 0.6816124 Best vali loss: 0.6816124
[36m(_train_fn pid=17254)[0m 	iters: 100, epoch: 4 | loss: 0.4202636
[36m(_train_fn pid=17254)[0m 	speed: 0.0255s/iter; left time: 64.8562s
[36m(_train_fn pid=17254)[0m 	iters: 200, epoch: 4 | loss: 0.2873419
[36m(_train_fn pid=17254)[0m 	speed: 0.0125s/iter; left time: 30.4561s
[36m(_train_fn pid=17254)[0m 	iters: 300, epoch: 4 | loss: 0.3844236
[36m(_train_fn pid=17254)[0m 	speed: 0.0125s/iter; left time: 29.2246s
[36m(_train_fn pid=17254)[0m 	iters: 400, epoch: 4 | loss: 0.3013977
[36m(_train_fn pid=17254)[0m 	speed: 0.0125s/iter; left time: 28.0024s
[36m(_train_fn pid=17254)[0m 	iters: 500, epoch: 4 | loss: 0.2849246
[36m(_train_fn pid=17254)[0m 	speed: 0.0125s/iter; left time: 26.7722s
[36m(_train_fn pid=17254)[0m Updating learning rate to 0.0011756855043916342
[36m(_train_fn pid=17254)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=17254)[0m saving checkpoint...
[36m(_train_fn pid=17254)[0m Epoch: 4 cost time: 6.6504762172698975
[36m(_train_fn pid=17254)[0m Epoch: 4, Steps: 528 | Train Loss: 0.3233292 Vali Loss: 0.7059691 Best vali loss: 0.6816124
[36m(_train_fn pid=17254)[0m 	iters: 100, epoch: 5 | loss: 0.3008446
[36m(_train_fn pid=17254)[0m 	speed: 0.0264s/iter; left time: 53.0602s
Trial status: 2 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:17:43. Total running time: 2min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
[36m(_train_fn pid=17254)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00002_2_alpha_d_ff=4,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1024,e_layer_2024-08-23_05-17-06/checkpoint_000004)
[36m(_train_fn pid=17254)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00002_2_alpha_d_ff=4,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1024,e_layer_2024-08-23_05-17-06/checkpoint_000005)
Current best trial: e8d8b_00002 with best_valid_loss=0.6816124145326943 and params={'batch_size': 16, 'learning_rate': 0.009405484035133074, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'dft_decomp', 'moving_avg': 75, 'e_layers': 4, 'dropout': 0.10242918296676143, 'd_ff': 64}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00002   RUNNING                16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429        4           31.93          0.323329       0.705969            0.681612 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089         0.349154       0.70033             0.698605 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=17254)[0m 	iters: 200, epoch: 5 | loss: 0.3065104
[36m(_train_fn pid=17254)[0m 	speed: 0.0138s/iter; left time: 26.4010s
[36m(_train_fn pid=17254)[0m 	iters: 300, epoch: 5 | loss: 0.2943210
[36m(_train_fn pid=17254)[0m 	speed: 0.0137s/iter; left time: 24.8015s
[36m(_train_fn pid=17254)[0m 	iters: 400, epoch: 5 | loss: 0.3051448
[36m(_train_fn pid=17254)[0m 	speed: 0.0136s/iter; left time: 23.2552s
[36m(_train_fn pid=17254)[0m 	iters: 500, epoch: 5 | loss: 0.3857946
[36m(_train_fn pid=17254)[0m 	speed: 0.0136s/iter; left time: 21.9237s
[36m(_train_fn pid=17254)[0m Updating learning rate to 0.0005878427521958171
[36m(_train_fn pid=17254)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=17254)[0m saving checkpoint...
[36m(_train_fn pid=17254)[0m Epoch: 5 cost time: 7.265033483505249
[36m(_train_fn pid=17254)[0m Epoch: 5, Steps: 528 | Train Loss: 0.3067220 Vali Loss: 0.7101677 Best vali loss: 0.6816124
[36m(_train_fn pid=17254)[0m 	iters: 100, epoch: 6 | loss: 0.3238751
[36m(_train_fn pid=17254)[0m 	speed: 0.0268s/iter; left time: 39.7531s
[36m(_train_fn pid=17254)[0m 	iters: 200, epoch: 6 | loss: 0.2740746
[36m(_train_fn pid=17254)[0m 	speed: 0.0137s/iter; left time: 18.9863s
[36m(_train_fn pid=17254)[0m 	iters: 300, epoch: 6 | loss: 0.2883014
[36m(_train_fn pid=17254)[0m 	speed: 0.0137s/iter; left time: 17.5749s
[36m(_train_fn pid=17254)[0m 	iters: 400, epoch: 6 | loss: 0.2741421
[36m(_train_fn pid=17254)[0m 	speed: 0.0136s/iter; left time: 16.1181s
[36m(_train_fn pid=17254)[0m 	iters: 500, epoch: 6 | loss: 0.2531621
[36m(_train_fn pid=17254)[0m 	speed: 0.0136s/iter; left time: 14.7589s

Trial trial-e8d8b_00002 completed after 6 iterations at 2024-08-23 05:17:57. Total running time: 2min 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             8.12454 â”‚
â”‚ time_total_s                                48.19692 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.68161 â”‚
â”‚ train_loss                                    0.2951 â”‚
â”‚ valid_loss                                   0.71792 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=17254)[0m Updating learning rate to 0.00029392137609790856
[36m(_train_fn pid=17254)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=17254)[0m saving checkpoint...
[36m(_train_fn pid=17254)[0m Epoch: 6 cost time: 7.266459703445435
[36m(_train_fn pid=17254)[0m Epoch: 6, Steps: 528 | Train Loss: 0.2950958 Vali Loss: 0.7179208 Best vali loss: 0.6816124
[36m(_train_fn pid=17254)[0m Early stopping

Trial trial-e8d8b_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00003 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  2 â”‚
â”‚ batch_size                                 32 â”‚
â”‚ d_model                                   256 â”‚
â”‚ decomp_method                      moving_avg â”‚
â”‚ down_sampling_method                      avg â”‚
â”‚ dropout                                0.1383 â”‚
â”‚ e_layers                                    1 â”‚
â”‚ learning_rate                         0.00686 â”‚
â”‚ moving_avg                                 55 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=17797)[0m configuration
[36m(_train_fn pid=17797)[0m {'batch_size': 32, 'learning_rate': 0.006864319999340584, 'down_sampling_method': 'avg', 'd_model': 256, 'decomp_method': 'moving_avg', 'moving_avg': 55, 'e_layers': 1, 'dropout': 0.1383008269907199, 'd_ff': 512}
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000000)
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000001)
2024-08-23 05:18:10,565	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:18:15,516	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000002)
2024-08-23 05:18:20,491	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000003)
2024-08-23 05:18:25,450	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000004)
2024-08-23 05:18:30,410	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=17797)[0m Use GPU: cuda:0
[36m(_train_fn pid=17797)[0m train 8449
[36m(_train_fn pid=17797)[0m val 2785
[36m(_train_fn pid=17797)[0m start_epoch 0
[36m(_train_fn pid=17797)[0m max_epoch 8
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 1 | loss: 0.4996690
[36m(_train_fn pid=17797)[0m 	speed: 0.0223s/iter; left time: 44.8312s
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 1 | loss: 0.3830439
[36m(_train_fn pid=17797)[0m 	speed: 0.0168s/iter; left time: 32.0589s
[36m(_train_fn pid=17797)[0m Updating learning rate to 0.006864319999340584
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Validation loss decreased (inf --> 0.7347).  Saving model state dict ...
[36m(_train_fn pid=17797)[0m Epoch: 1 cost time: 4.673220634460449
[36m(_train_fn pid=17797)[0m Epoch: 1, Steps: 264 | Train Loss: 0.4525853 Vali Loss: 0.7347151 Best vali loss: 0.7347151
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 2 | loss: 0.5413591
[36m(_train_fn pid=17797)[0m 	speed: 0.0338s/iter; left time: 59.0617s
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 2 | loss: 0.4345971
[36m(_train_fn pid=17797)[0m 	speed: 0.0163s/iter; left time: 26.8155s
[36m(_train_fn pid=17797)[0m Updating learning rate to 0.003432159999670292
[36m(_train_fn pid=17797)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Epoch: 2 cost time: 4.338157415390015
[36m(_train_fn pid=17797)[0m Epoch: 2, Steps: 264 | Train Loss: 0.8197809 Vali Loss: 0.7850561 Best vali loss: 0.7347151
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 3 | loss: 0.3808959
[36m(_train_fn pid=17797)[0m 	speed: 0.0331s/iter; left time: 49.1589s

Trial status: 3 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:18:13. Total running time: 3min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00002 with best_valid_loss=0.6816124145326943 and params={'batch_size': 16, 'learning_rate': 0.009405484035133074, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'dft_decomp', 'moving_avg': 75, 'e_layers': 4, 'dropout': 0.10242918296676143, 'd_ff': 64}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00003   RUNNING                32       0.00686432    avg                            256              2   moving_avg                  55            1    0.138301        2           10.6907        0.819781       0.785056            0.734715 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089         0.349154       0.70033             0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429        6           48.1969        0.295096       0.717921            0.681612 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 3 | loss: 0.3271168
[36m(_train_fn pid=17797)[0m 	speed: 0.0163s/iter; left time: 22.5536s
[36m(_train_fn pid=17797)[0m Updating learning rate to 0.001716079999835146
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Validation loss decreased (0.7347 --> 0.7263).  Saving model state dict ...
[36m(_train_fn pid=17797)[0m Epoch: 3 cost time: 4.325517892837524
[36m(_train_fn pid=17797)[0m Epoch: 3, Steps: 264 | Train Loss: 0.3848904 Vali Loss: 0.7263495 Best vali loss: 0.7263495
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 4 | loss: 0.3406711
[36m(_train_fn pid=17797)[0m 	speed: 0.0333s/iter; left time: 40.7029s
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 4 | loss: 0.4007152
[36m(_train_fn pid=17797)[0m 	speed: 0.0164s/iter; left time: 18.3309s
[36m(_train_fn pid=17797)[0m Updating learning rate to 0.000858039999917573
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Validation loss decreased (0.7263 --> 0.7037).  Saving model state dict ...
[36m(_train_fn pid=17797)[0m Epoch: 4 cost time: 4.348988056182861
[36m(_train_fn pid=17797)[0m Epoch: 4, Steps: 264 | Train Loss: 0.3686373 Vali Loss: 0.7037388 Best vali loss: 0.7037388
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 5 | loss: 0.2765587
[36m(_train_fn pid=17797)[0m 	speed: 0.0333s/iter; left time: 31.8659s
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 5 | loss: 0.3064637
[36m(_train_fn pid=17797)[0m 	speed: 0.0163s/iter; left time: 13.9646s
[36m(_train_fn pid=17797)[0m Updating learning rate to 0.0004290199999587865
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Validation loss decreased (0.7037 --> 0.7018).  Saving model state dict ...
[36m(_train_fn pid=17797)[0m Epoch: 5 cost time: 4.33091139793396
[36m(_train_fn pid=17797)[0m Epoch: 5, Steps: 264 | Train Loss: 0.3630354 Vali Loss: 0.7018459 Best vali loss: 0.7018459
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 6 | loss: 0.4182266
[36m(_train_fn pid=17797)[0m 	speed: 0.0333s/iter; left time: 23.0989s
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 6 | loss: 0.4161104
[36m(_train_fn pid=17797)[0m 	speed: 0.0163s/iter; left time: 9.6715s
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000005)
2024-08-23 05:18:35,372	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000006)
2024-08-23 05:18:40,340	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=17797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00003_3_alpha_d_ff=2,batch_size=32,d_model=256,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1383,e_laye_2024-08-23_05-17-57/checkpoint_000007)
[36m(_train_fn pid=17797)[0m Updating learning rate to 0.00021450999997939325
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Validation loss decreased (0.7018 --> 0.6951).  Saving model state dict ...
[36m(_train_fn pid=17797)[0m Epoch: 6 cost time: 4.336833715438843
[36m(_train_fn pid=17797)[0m Epoch: 6, Steps: 264 | Train Loss: 0.3600433 Vali Loss: 0.6950511 Best vali loss: 0.6950511
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 7 | loss: 0.4145881
[36m(_train_fn pid=17797)[0m 	speed: 0.0333s/iter; left time: 14.2961s
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 7 | loss: 0.3073496
[36m(_train_fn pid=17797)[0m 	speed: 0.0163s/iter; left time: 5.3750s
[36m(_train_fn pid=17797)[0m Updating learning rate to 0.00010725499998969663
[36m(_train_fn pid=17797)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Epoch: 7 cost time: 4.342990875244141
[36m(_train_fn pid=17797)[0m Epoch: 7, Steps: 264 | Train Loss: 0.3586493 Vali Loss: 0.7009417 Best vali loss: 0.6950511
[36m(_train_fn pid=17797)[0m 	iters: 100, epoch: 8 | loss: 0.3527523
[36m(_train_fn pid=17797)[0m 	speed: 0.0333s/iter; left time: 5.4930s
[36m(_train_fn pid=17797)[0m 	iters: 200, epoch: 8 | loss: 0.3371700
[36m(_train_fn pid=17797)[0m 	speed: 0.0164s/iter; left time: 1.0635s

Trial trial-e8d8b_00003 completed after 8 iterations at 2024-08-23 05:18:40. Total running time: 3min 27s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              4.9654 â”‚
â”‚ time_total_s                                40.44609 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.69505 â”‚
â”‚ train_loss                                   0.35797 â”‚
â”‚ valid_loss                                    0.6973 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=17797)[0m Updating learning rate to 5.362749999484831e-05
[36m(_train_fn pid=17797)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=17797)[0m saving checkpoint...
[36m(_train_fn pid=17797)[0m Epoch: 8 cost time: 4.347811698913574
[36m(_train_fn pid=17797)[0m Epoch: 8, Steps: 264 | Train Loss: 0.3579677 Vali Loss: 0.6973025 Best vali loss: 0.6950511

Trial trial-e8d8b_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00004 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  2 â”‚
â”‚ batch_size                                 64 â”‚
â”‚ d_model                                   256 â”‚
â”‚ decomp_method                      dft_decomp â”‚
â”‚ down_sampling_method                     conv â”‚
â”‚ dropout                                0.1207 â”‚
â”‚ e_layers                                    1 â”‚
â”‚ learning_rate                         0.00104 â”‚
â”‚ moving_avg                                 55 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=18478)[0m configuration
[36m(_train_fn pid=18478)[0m {'batch_size': 64, 'learning_rate': 0.0010448460213083896, 'down_sampling_method': 'conv', 'd_model': 256, 'decomp_method': 'dft_decomp', 'moving_avg': 55, 'e_layers': 1, 'dropout': 0.12069803603896843, 'd_ff': 512}
[36m(_train_fn pid=18478)[0m Use GPU: cuda:0
[36m(_train_fn pid=18478)[0m train 8449
[36m(_train_fn pid=18478)[0m val 2785
[36m(_train_fn pid=18478)[0m start_epoch 0
[36m(_train_fn pid=18478)[0m max_epoch 8

Trial status: 4 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:18:43. Total running time: 3min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00002 with best_valid_loss=0.6816124145326943 and params={'batch_size': 16, 'learning_rate': 0.009405484035133074, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'dft_decomp', 'moving_avg': 75, 'e_layers': 4, 'dropout': 0.10242918296676143, 'd_ff': 64}
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000000)
2024-08-23 05:18:53,245	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000001)
2024-08-23 05:18:57,989	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000002)
2024-08-23 05:19:02,747	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000003)
2024-08-23 05:19:07,503	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000004)
2024-08-23 05:19:12,255	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000005)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00004   RUNNING                64       0.00104485    conv                           256              2   dft_decomp                  55            1    0.120698                                                                               â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089         0.349154       0.70033             0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429        6           48.1969        0.295096       0.717921            0.681612 â”‚
â”‚ trial-e8d8b_00003   TERMINATED             32       0.00686432    avg                            256              2   moving_avg                  55            1    0.138301        8           40.4461        0.357968       0.697302            0.695051 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 1 | loss: 0.6027313
[36m(_train_fn pid=18478)[0m 	speed: 0.0385s/iter; left time: 36.8475s
[36m(_train_fn pid=18478)[0m Updating learning rate to 0.0010448460213083896
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Validation loss decreased (inf --> 1.1435).  Saving model state dict ...
[36m(_train_fn pid=18478)[0m Epoch: 1 cost time: 4.548217296600342
[36m(_train_fn pid=18478)[0m Epoch: 1, Steps: 132 | Train Loss: 0.5839082 Vali Loss: 1.1434979 Best vali loss: 1.1434979
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 2 | loss: 0.3352814
[36m(_train_fn pid=18478)[0m 	speed: 0.0474s/iter; left time: 39.1151s
[36m(_train_fn pid=18478)[0m Updating learning rate to 0.0005224230106541948
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Validation loss decreased (1.1435 --> 0.7154).  Saving model state dict ...
[36m(_train_fn pid=18478)[0m Epoch: 2 cost time: 4.104220390319824
[36m(_train_fn pid=18478)[0m Epoch: 2, Steps: 132 | Train Loss: 0.4086172 Vali Loss: 0.7154088 Best vali loss: 0.7154088
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 3 | loss: 0.3444054
[36m(_train_fn pid=18478)[0m 	speed: 0.0474s/iter; left time: 32.8536s
[36m(_train_fn pid=18478)[0m Updating learning rate to 0.0002612115053270974
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Validation loss decreased (0.7154 --> 0.7105).  Saving model state dict ...
[36m(_train_fn pid=18478)[0m Epoch: 3 cost time: 4.102745294570923
[36m(_train_fn pid=18478)[0m Epoch: 3, Steps: 132 | Train Loss: 0.3661556 Vali Loss: 0.7104658 Best vali loss: 0.7104658
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 4 | loss: 0.3850935
[36m(_train_fn pid=18478)[0m 	speed: 0.0476s/iter; left time: 26.6867s
[36m(_train_fn pid=18478)[0m Updating learning rate to 0.0001306057526635487
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Validation loss decreased (0.7105 --> 0.7095).  Saving model state dict ...
[36m(_train_fn pid=18478)[0m Epoch: 4 cost time: 4.116583824157715
[36m(_train_fn pid=18478)[0m Epoch: 4, Steps: 132 | Train Loss: 0.3617190 Vali Loss: 0.7094638 Best vali loss: 0.7094638
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 5 | loss: 0.3336519
[36m(_train_fn pid=18478)[0m 	speed: 0.0476s/iter; left time: 20.4021s
[36m(_train_fn pid=18478)[0m Updating learning rate to 6.530287633177435e-05
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Validation loss decreased (0.7095 --> 0.7037).  Saving model state dict ...
[36m(_train_fn pid=18478)[0m Epoch: 5 cost time: 4.1148388385772705
[36m(_train_fn pid=18478)[0m Epoch: 5, Steps: 132 | Train Loss: 0.3587025 Vali Loss: 0.7036713 Best vali loss: 0.7036713
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 6 | loss: 0.3727043
[36m(_train_fn pid=18478)[0m 	speed: 0.0475s/iter; left time: 14.1178s
[36m(_train_fn pid=18478)[0m Updating learning rate to 3.2651438165887175e-05
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Validation loss decreased (0.7037 --> 0.7009).  Saving model state dict ...
[36m(_train_fn pid=18478)[0m Epoch: 6 cost time: 4.111898183822632
[36m(_train_fn pid=18478)[0m Epoch: 6, Steps: 132 | Train Loss: 0.3573283 Vali Loss: 0.7008655 Best vali loss: 0.7008655
Trial status: 4 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:19:13. Total running time: 4min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00002 with best_valid_loss=0.6816124145326943 and params={'batch_size': 16, 'learning_rate': 0.009405484035133074, 'down_sampling_method': 'avg', 'd_model': 16, 'decomp_method': 'dft_decomp', 'moving_avg': 75, 'e_layers': 4, 'dropout': 0.10242918296676143, 'd_ff': 64}
2024-08-23 05:19:17,008	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000006)
2024-08-23 05:19:21,751	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=18478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00004_4_alpha_d_ff=2,batch_size=64,d_model=256,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.1207,e_lay_2024-08-23_05-18-40/checkpoint_000007)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00004   RUNNING                64       0.00104485    conv                           256              2   dft_decomp                  55            1    0.120698        6           29.3354        0.357328       0.700866            0.700866 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089         0.349154       0.70033             0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429        6           48.1969        0.295096       0.717921            0.681612 â”‚
â”‚ trial-e8d8b_00003   TERMINATED             32       0.00686432    avg                            256              2   moving_avg                  55            1    0.138301        8           40.4461        0.357968       0.697302            0.695051 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 7 | loss: 0.3772280
[36m(_train_fn pid=18478)[0m 	speed: 0.0475s/iter; left time: 7.8436s
[36m(_train_fn pid=18478)[0m Updating learning rate to 1.6325719082943588e-05
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Validation loss decreased (0.7009 --> 0.6994).  Saving model state dict ...
[36m(_train_fn pid=18478)[0m Epoch: 7 cost time: 4.113435506820679
[36m(_train_fn pid=18478)[0m Epoch: 7, Steps: 132 | Train Loss: 0.3563642 Vali Loss: 0.6994302 Best vali loss: 0.6994302
[36m(_train_fn pid=18478)[0m 	iters: 100, epoch: 8 | loss: 0.3682047
[36m(_train_fn pid=18478)[0m 	speed: 0.0475s/iter; left time: 1.5675s

Trial trial-e8d8b_00004 completed after 8 iterations at 2024-08-23 05:19:21. Total running time: 4min 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              4.7396 â”‚
â”‚ time_total_s                                38.82482 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.69943 â”‚
â”‚ train_loss                                   0.35617 â”‚
â”‚ valid_loss                                   0.69943 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=18478)[0m Updating learning rate to 8.162859541471794e-06
[36m(_train_fn pid=18478)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=18478)[0m saving checkpoint...
[36m(_train_fn pid=18478)[0m Epoch: 8 cost time: 4.1124701499938965
[36m(_train_fn pid=18478)[0m Epoch: 8, Steps: 132 | Train Loss: 0.3561682 Vali Loss: 0.6994346 Best vali loss: 0.6994302

Trial trial-e8d8b_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00005 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  2 â”‚
â”‚ batch_size                                 16 â”‚
â”‚ d_model                                    32 â”‚
â”‚ decomp_method                      moving_avg â”‚
â”‚ down_sampling_method                      avg â”‚
â”‚ dropout                               0.12685 â”‚
â”‚ e_layers                                    1 â”‚
â”‚ learning_rate                         0.00319 â”‚
â”‚ moving_avg                                 15 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=19158)[0m configuration
[36m(_train_fn pid=19158)[0m {'batch_size': 16, 'learning_rate': 0.003186053793189858, 'down_sampling_method': 'avg', 'd_model': 32, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 1, 'dropout': 0.12685076554929858, 'd_ff': 64}
[36m(_train_fn pid=19158)[0m Use GPU: cuda:0
[36m(_train_fn pid=19158)[0m train 8449
[36m(_train_fn pid=19158)[0m val 2785
[36m(_train_fn pid=19158)[0m start_epoch 0
[36m(_train_fn pid=19158)[0m max_epoch 8
[36m(_train_fn pid=19158)[0m 	iters: 100, epoch: 1 | loss: 0.5610927
[36m(_train_fn pid=19158)[0m 	speed: 0.0122s/iter; left time: 50.3348s
[36m(_train_fn pid=19158)[0m 	iters: 200, epoch: 1 | loss: 0.5133489
[36m(_train_fn pid=19158)[0m 	speed: 0.0063s/iter; left time: 25.1815s
[36m(_train_fn pid=19158)[0m 	iters: 300, epoch: 1 | loss: 0.4785228
[36m(_train_fn pid=19158)[0m 	speed: 0.0063s/iter; left time: 24.7036s
[36m(_train_fn pid=19158)[0m 	iters: 400, epoch: 1 | loss: 0.5232438
[36m(_train_fn pid=19158)[0m 	speed: 0.0064s/iter; left time: 24.3908s
[36m(_train_fn pid=19158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00005_5_alpha_d_ff=2,batch_size=16,d_model=32,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1269,e_layer_2024-08-23_05-19-21/checkpoint_000000)
2024-08-23 05:19:32,234	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=19158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00005_5_alpha_d_ff=2,batch_size=16,d_model=32,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1269,e_layer_2024-08-23_05-19-21/checkpoint_000001)
2024-08-23 05:19:35,727	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=19158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00005_5_alpha_d_ff=2,batch_size=16,d_model=32,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1269,e_layer_2024-08-23_05-19-21/checkpoint_000002)
2024-08-23 05:19:39,386	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=19158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00005_5_alpha_d_ff=2,batch_size=16,d_model=32,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1269,e_layer_2024-08-23_05-19-21/checkpoint_000003)
2024-08-23 05:19:42,808	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=19158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00005_5_alpha_d_ff=2,batch_size=16,d_model=32,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1269,e_layer_2024-08-23_05-19-21/checkpoint_000004)
[36m(_train_fn pid=19158)[0m 	iters: 500, epoch: 1 | loss: 0.4166098
[36m(_train_fn pid=19158)[0m 	speed: 0.0064s/iter; left time: 23.6727s
[36m(_train_fn pid=19158)[0m Updating learning rate to 0.003186053793189858
[36m(_train_fn pid=19158)[0m saving checkpoint...
[36m(_train_fn pid=19158)[0m Validation loss decreased (inf --> 0.7389).  Saving model state dict ...
[36m(_train_fn pid=19158)[0m Epoch: 1 cost time: 3.615464210510254
[36m(_train_fn pid=19158)[0m Epoch: 1, Steps: 528 | Train Loss: 0.5091972 Vali Loss: 0.7388693 Best vali loss: 0.7388693
[36m(_train_fn pid=19158)[0m 	iters: 100, epoch: 2 | loss: 0.4812087
[36m(_train_fn pid=19158)[0m 	speed: 0.0131s/iter; left time: 47.1739s
[36m(_train_fn pid=19158)[0m 	iters: 200, epoch: 2 | loss: 0.4405942
[36m(_train_fn pid=19158)[0m 	speed: 0.0063s/iter; left time: 22.0210s
[36m(_train_fn pid=19158)[0m 	iters: 300, epoch: 2 | loss: 0.3781609
[36m(_train_fn pid=19158)[0m 	speed: 0.0064s/iter; left time: 21.5759s
[36m(_train_fn pid=19158)[0m 	iters: 400, epoch: 2 | loss: 0.4066679
[36m(_train_fn pid=19158)[0m 	speed: 0.0064s/iter; left time: 20.9391s
[36m(_train_fn pid=19158)[0m 	iters: 500, epoch: 2 | loss: 0.3218037
[36m(_train_fn pid=19158)[0m 	speed: 0.0063s/iter; left time: 20.3000s
[36m(_train_fn pid=19158)[0m Updating learning rate to 0.001593026896594929
[36m(_train_fn pid=19158)[0m saving checkpoint...
[36m(_train_fn pid=19158)[0m Validation loss decreased (0.7389 --> 0.6977).  Saving model state dict ...
[36m(_train_fn pid=19158)[0m Epoch: 2 cost time: 3.3873302936553955
[36m(_train_fn pid=19158)[0m Epoch: 2, Steps: 528 | Train Loss: 0.3701287 Vali Loss: 0.6976531 Best vali loss: 0.6976531
[36m(_train_fn pid=19158)[0m 	iters: 100, epoch: 3 | loss: 0.3043881
[36m(_train_fn pid=19158)[0m 	speed: 0.0122s/iter; left time: 37.4276s
[36m(_train_fn pid=19158)[0m 	iters: 200, epoch: 3 | loss: 0.2809246
[36m(_train_fn pid=19158)[0m 	speed: 0.0056s/iter; left time: 16.6863s
[36m(_train_fn pid=19158)[0m 	iters: 300, epoch: 3 | loss: 0.3339178
[36m(_train_fn pid=19158)[0m 	speed: 0.0056s/iter; left time: 15.9292s
[36m(_train_fn pid=19158)[0m 	iters: 400, epoch: 3 | loss: 0.3770324
[36m(_train_fn pid=19158)[0m 	speed: 0.0056s/iter; left time: 15.5652s
[36m(_train_fn pid=19158)[0m 	iters: 500, epoch: 3 | loss: 0.3858248
[36m(_train_fn pid=19158)[0m 	speed: 0.0057s/iter; left time: 15.2141s
[36m(_train_fn pid=19158)[0m Updating learning rate to 0.0007965134482974645
[36m(_train_fn pid=19158)[0m saving checkpoint...
[36m(_train_fn pid=19158)[0m Validation loss decreased (0.6977 --> 0.6968).  Saving model state dict ...
[36m(_train_fn pid=19158)[0m Epoch: 3 cost time: 3.00730562210083
[36m(_train_fn pid=19158)[0m Epoch: 3, Steps: 528 | Train Loss: 0.3498917 Vali Loss: 0.6967854 Best vali loss: 0.6967854
[36m(_train_fn pid=19158)[0m 	iters: 100, epoch: 4 | loss: 0.4028542
[36m(_train_fn pid=19158)[0m 	speed: 0.0132s/iter; left time: 33.5483s
[36m(_train_fn pid=19158)[0m 	iters: 200, epoch: 4 | loss: 0.3859899
[36m(_train_fn pid=19158)[0m 	speed: 0.0064s/iter; left time: 15.7170s
[36m(_train_fn pid=19158)[0m 	iters: 300, epoch: 4 | loss: 0.3219083
[36m(_train_fn pid=19158)[0m 	speed: 0.0061s/iter; left time: 14.2697s
[36m(_train_fn pid=19158)[0m 	iters: 400, epoch: 4 | loss: 0.3096859
[36m(_train_fn pid=19158)[0m 	speed: 0.0057s/iter; left time: 12.7276s
[36m(_train_fn pid=19158)[0m 	iters: 500, epoch: 4 | loss: 0.3362450
[36m(_train_fn pid=19158)[0m 	speed: 0.0057s/iter; left time: 12.1948s
[36m(_train_fn pid=19158)[0m Updating learning rate to 0.00039825672414873223
[36m(_train_fn pid=19158)[0m saving checkpoint...
[36m(_train_fn pid=19158)[0m Validation loss decreased (0.6968 --> 0.6809).  Saving model state dict ...
[36m(_train_fn pid=19158)[0m Epoch: 4 cost time: 3.219968795776367
[36m(_train_fn pid=19158)[0m Epoch: 4, Steps: 528 | Train Loss: 0.3414921 Vali Loss: 0.6808662 Best vali loss: 0.6808662
[36m(_train_fn pid=19158)[0m 	iters: 100, epoch: 5 | loss: 0.3842625
[36m(_train_fn pid=19158)[0m 	speed: 0.0119s/iter; left time: 23.9065s
[36m(_train_fn pid=19158)[0m 	iters: 200, epoch: 5 | loss: 0.2764683
[36m(_train_fn pid=19158)[0m 	speed: 0.0055s/iter; left time: 10.5608s
[36m(_train_fn pid=19158)[0m 	iters: 300, epoch: 5 | loss: 0.2422689
[36m(_train_fn pid=19158)[0m 	speed: 0.0055s/iter; left time: 10.0204s
[36m(_train_fn pid=19158)[0m 	iters: 400, epoch: 5 | loss: 0.4622053
[36m(_train_fn pid=19158)[0m 	speed: 0.0056s/iter; left time: 9.5679s
[36m(_train_fn pid=19158)[0m 	iters: 500, epoch: 5 | loss: 0.4269703
[36m(_train_fn pid=19158)[0m 	speed: 0.0056s/iter; left time: 8.9913s
[36m(_train_fn pid=19158)[0m Updating learning rate to 0.00019912836207436612
[36m(_train_fn pid=19158)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=19158)[0m saving checkpoint...
[36m(_train_fn pid=19158)[0m Epoch: 5 cost time: 2.973109245300293
[36m(_train_fn pid=19158)[0m Epoch: 5, Steps: 528 | Train Loss: 0.3358702 Vali Loss: 0.6964654 Best vali loss: 0.6808662
[36m(_train_fn pid=19158)[0m 	iters: 100, epoch: 6 | loss: 0.2701177
[36m(_train_fn pid=19158)[0m 	speed: 0.0120s/iter; left time: 17.8469s

Trial status: 5 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:19:43. Total running time: 4min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00005 with best_valid_loss=0.6808662353747192 and params={'batch_size': 16, 'learning_rate': 0.003186053793189858, 'down_sampling_method': 'avg', 'd_model': 32, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 1, 'dropout': 0.12685076554929858, 'd_ff': 64}
2024-08-23 05:19:46,267	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=19158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00005_5_alpha_d_ff=2,batch_size=16,d_model=32,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1269,e_layer_2024-08-23_05-19-21/checkpoint_000005)
2024-08-23 05:19:49,798	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=19158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00005_5_alpha_d_ff=2,batch_size=16,d_model=32,decomp_method=moving_avg,down_sampling_method=avg,dropout=0.1269,e_layer_2024-08-23_05-19-21/checkpoint_000006)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00005   RUNNING                16       0.00318605    avg                             32              2   moving_avg                  15            1    0.126851        5           18.901         0.33587        0.696465            0.680866 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042       0.339293       0.698135            0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089         0.349154       0.70033             0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429        6           48.1969        0.295096       0.717921            0.681612 â”‚
â”‚ trial-e8d8b_00003   TERMINATED             32       0.00686432    avg                            256              2   moving_avg                  55            1    0.138301        8           40.4461        0.357968       0.697302            0.695051 â”‚
â”‚ trial-e8d8b_00004   TERMINATED             64       0.00104485    conv                           256              2   dft_decomp                  55            1    0.120698        8           38.8248        0.356168       0.699435            0.69943  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=19158)[0m 	iters: 200, epoch: 6 | loss: 0.3617267
[36m(_train_fn pid=19158)[0m 	speed: 0.0055s/iter; left time: 7.6721s
[36m(_train_fn pid=19158)[0m 	iters: 300, epoch: 6 | loss: 0.3327368
[36m(_train_fn pid=19158)[0m 	speed: 0.0056s/iter; left time: 7.2185s
[36m(_train_fn pid=19158)[0m 	iters: 400, epoch: 6 | loss: 0.3363102
[36m(_train_fn pid=19158)[0m 	speed: 0.0057s/iter; left time: 6.7160s
[36m(_train_fn pid=19158)[0m 	iters: 500, epoch: 6 | loss: 0.3087023
[36m(_train_fn pid=19158)[0m 	speed: 0.0058s/iter; left time: 6.2578s
[36m(_train_fn pid=19158)[0m Updating learning rate to 9.956418103718306e-05
[36m(_train_fn pid=19158)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=19158)[0m saving checkpoint...
[36m(_train_fn pid=19158)[0m Epoch: 6 cost time: 3.0156965255737305
[36m(_train_fn pid=19158)[0m Epoch: 6, Steps: 528 | Train Loss: 0.3324850 Vali Loss: 0.6948600 Best vali loss: 0.6808662
[36m(_train_fn pid=19158)[0m 	iters: 100, epoch: 7 | loss: 0.2971365
[36m(_train_fn pid=19158)[0m 	speed: 0.0122s/iter; left time: 11.6991s
[36m(_train_fn pid=19158)[0m 	iters: 200, epoch: 7 | loss: 0.2892811
[36m(_train_fn pid=19158)[0m 	speed: 0.0058s/iter; left time: 4.9447s
[36m(_train_fn pid=19158)[0m 	iters: 300, epoch: 7 | loss: 0.3394414
[36m(_train_fn pid=19158)[0m 	speed: 0.0058s/iter; left time: 4.3988s
[36m(_train_fn pid=19158)[0m 	iters: 400, epoch: 7 | loss: 0.2485960
[36m(_train_fn pid=19158)[0m 	speed: 0.0057s/iter; left time: 3.7451s
[36m(_train_fn pid=19158)[0m 	iters: 500, epoch: 7 | loss: 0.3748299
[36m(_train_fn pid=19158)[0m 	speed: 0.0057s/iter; left time: 3.1831s

Trial trial-e8d8b_00005 completed after 7 iterations at 2024-08-23 05:19:49. Total running time: 4min 36s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             3.52814 â”‚
â”‚ time_total_s                                25.88587 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.68087 â”‚
â”‚ train_loss                                   0.33058 â”‚
â”‚ valid_loss                                   0.70166 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=19158)[0m Updating learning rate to 4.978209051859153e-05
[36m(_train_fn pid=19158)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=19158)[0m saving checkpoint...
[36m(_train_fn pid=19158)[0m Epoch: 7 cost time: 3.0765552520751953
[36m(_train_fn pid=19158)[0m Epoch: 7, Steps: 528 | Train Loss: 0.3305772 Vali Loss: 0.7016590 Best vali loss: 0.6808662
[36m(_train_fn pid=19158)[0m Early stopping

Trial trial-e8d8b_00006 started with configuration:
2024-08-23 05:20:00,506	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=19760)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00006_6_alpha_d_ff=3,batch_size=32,d_model=128,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1307,e_laye_2024-08-23_05-19-49/checkpoint_000000)
[36m(_train_fn pid=19760)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00006_6_alpha_d_ff=3,batch_size=32,d_model=128,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1307,e_laye_2024-08-23_05-19-49/checkpoint_000001)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00006 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  3 â”‚
â”‚ batch_size                                 32 â”‚
â”‚ d_model                                   128 â”‚
â”‚ decomp_method                      dft_decomp â”‚
â”‚ down_sampling_method                      avg â”‚
â”‚ dropout                               0.13066 â”‚
â”‚ e_layers                                    4 â”‚
â”‚ learning_rate                         0.00926 â”‚
â”‚ moving_avg                                 55 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=19760)[0m configuration
[36m(_train_fn pid=19760)[0m {'batch_size': 32, 'learning_rate': 0.009264729198598883, 'down_sampling_method': 'avg', 'd_model': 128, 'decomp_method': 'dft_decomp', 'moving_avg': 55, 'e_layers': 4, 'dropout': 0.13065573232052877, 'd_ff': 384}
[36m(_train_fn pid=19760)[0m Use GPU: cuda:0
[36m(_train_fn pid=19760)[0m train 8449
[36m(_train_fn pid=19760)[0m val 2785
[36m(_train_fn pid=19760)[0m start_epoch 0
[36m(_train_fn pid=19760)[0m max_epoch 8
[36m(_train_fn pid=19760)[0m 	iters: 100, epoch: 1 | loss: 0.3813448
[36m(_train_fn pid=19760)[0m 	speed: 0.0331s/iter; left time: 66.5979s
[36m(_train_fn pid=19760)[0m 	iters: 200, epoch: 1 | loss: 0.3799119
[36m(_train_fn pid=19760)[0m 	speed: 0.0251s/iter; left time: 47.9632s
[36m(_train_fn pid=19760)[0m Updating learning rate to 0.009264729198598883
[36m(_train_fn pid=19760)[0m saving checkpoint...
[36m(_train_fn pid=19760)[0m Validation loss decreased (inf --> 0.7153).  Saving model state dict ...
[36m(_train_fn pid=19760)[0m Epoch: 1 cost time: 7.136327743530273
[36m(_train_fn pid=19760)[0m Epoch: 1, Steps: 264 | Train Loss: 0.4370176 Vali Loss: 0.7152825 Best vali loss: 0.7152825
[36m(_train_fn pid=19760)[0m 	iters: 100, epoch: 2 | loss: 4058214.2500000
[36m(_train_fn pid=19760)[0m 	speed: 0.0518s/iter; left time: 90.5789s
[36m(_train_fn pid=19760)[0m 	iters: 200, epoch: 2 | loss: 194262108995584.0000000
[36m(_train_fn pid=19760)[0m 	speed: 0.0246s/iter; left time: 40.5509s
[36m(_train_fn pid=19760)[0m Updating learning rate to 0.004632364599299442
[36m(_train_fn pid=19760)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=19760)[0m saving checkpoint...
[36m(_train_fn pid=19760)[0m Epoch: 2 cost time: 6.543374061584473
[36m(_train_fn pid=19760)[0m Epoch: 2, Steps: 264 | Train Loss: 47401889815074.9140625 Vali Loss: 966671092677.1494141 Best vali loss: 0.7152825
[36m(_train_fn pid=19760)[0m 	iters: 100, epoch: 3 | loss: 18070790144.0000000
[36m(_train_fn pid=19760)[0m 	speed: 0.0507s/iter; left time: 75.3577s
[36m(_train_fn pid=19760)[0m 	iters: 200, epoch: 3 | loss: 33605339136.0000000
[36m(_train_fn pid=19760)[0m 	speed: 0.0245s/iter; left time: 33.8818s

Trial status: 6 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:20:13. Total running time: 5min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00005 with best_valid_loss=0.6808662353747192 and params={'batch_size': 16, 'learning_rate': 0.003186053793189858, 'down_sampling_method': 'avg', 'd_model': 32, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 1, 'dropout': 0.12685076554929858, 'd_ff': 64}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00006   RUNNING                32       0.00926473    avg                            128              3   dft_decomp                  55            4    0.130656        2           16.1804     4.74019e+13    9.66671e+11            0.715283 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042    0.339293       0.698135               0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089      0.349154       0.70033                0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429        6           48.1969     0.295096       0.717921               0.681612 â”‚
â”‚ trial-e8d8b_00003   TERMINATED             32       0.00686432    avg                            256              2   moving_avg                  55            1    0.138301        8           40.4461     0.357968       0.697302               0.695051 â”‚
â”‚ trial-e8d8b_00004   TERMINATED             64       0.00104485    conv                           256              2   dft_decomp                  55            1    0.120698        8           38.8248     0.356168       0.699435               0.69943  â”‚
â”‚ trial-e8d8b_00005   TERMINATED             16       0.00318605    avg                             32              2   moving_avg                  15            1    0.126851        7           25.8859     0.330577       0.701659               0.680866 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=19760)[0m Updating learning rate to 0.002316182299649721
[36m(_train_fn pid=19760)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00006_6_alpha_d_ff=3,batch_size=32,d_model=128,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1307,e_laye_2024-08-23_05-19-49/checkpoint_000002)
[36m(_train_fn pid=19760)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00006_6_alpha_d_ff=3,batch_size=32,d_model=128,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1307,e_laye_2024-08-23_05-19-49/checkpoint_000003)
[36m(_train_fn pid=20145)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00007_7_alpha_d_ff=2,batch_size=16,d_model=64,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1248,e_laye_2024-08-23_05-20-23/checkpoint_000000)
[36m(_train_fn pid=20145)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00007_7_alpha_d_ff=2,batch_size=16,d_model=64,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1248,e_laye_2024-08-23_05-20-23/checkpoint_000001)
[36m(_train_fn pid=20145)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00007_7_alpha_d_ff=2,batch_size=16,d_model=64,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1248,e_laye_2024-08-23_05-20-23/checkpoint_000002)
[36m(_train_fn pid=19760)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=19760)[0m saving checkpoint...
[36m(_train_fn pid=19760)[0m Epoch: 3 cost time: 6.496967315673828
[36m(_train_fn pid=19760)[0m Epoch: 3, Steps: 264 | Train Loss: 67986095185.4545441 Vali Loss: 11259169206.4367809 Best vali loss: 0.7152825
[36m(_train_fn pid=19760)[0m 	iters: 100, epoch: 4 | loss: 6600213504.0000000
[36m(_train_fn pid=19760)[0m 	speed: 0.0515s/iter; left time: 62.9212s
[36m(_train_fn pid=19760)[0m 	iters: 200, epoch: 4 | loss: 5863773696.0000000
[36m(_train_fn pid=19760)[0m 	speed: 0.0250s/iter; left time: 27.9901s

Trial trial-e8d8b_00006 completed after 4 iterations at 2024-08-23 05:20:23. Total running time: 5min 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             7.65654 â”‚
â”‚ time_total_s                                31.37621 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.71528 â”‚
â”‚ train_loss                          5744594927.51515 â”‚
â”‚ valid_loss                          7451824388.41379 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=19760)[0m Updating learning rate to 0.0011580911498248604
[36m(_train_fn pid=19760)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=19760)[0m saving checkpoint...
[36m(_train_fn pid=19760)[0m Epoch: 4 cost time: 6.630794286727905
[36m(_train_fn pid=19760)[0m Epoch: 4, Steps: 264 | Train Loss: 5744594927.5151520 Vali Loss: 7451824388.4137936 Best vali loss: 0.7152825
[36m(_train_fn pid=19760)[0m Early stopping

Trial trial-e8d8b_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00007 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  2 â”‚
â”‚ batch_size                                 16 â”‚
â”‚ d_model                                    64 â”‚
â”‚ decomp_method                      moving_avg â”‚
â”‚ down_sampling_method                     conv â”‚
â”‚ dropout                               0.12483 â”‚
â”‚ e_layers                                    2 â”‚
â”‚ learning_rate                          0.0051 â”‚
â”‚ moving_avg                                 55 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=20145)[0m configuration
[36m(_train_fn pid=20145)[0m {'batch_size': 16, 'learning_rate': 0.005099904478550495, 'down_sampling_method': 'conv', 'd_model': 64, 'decomp_method': 'moving_avg', 'moving_avg': 55, 'e_layers': 2, 'dropout': 0.12483309439879528, 'd_ff': 128}
[36m(_train_fn pid=20145)[0m Use GPU: cuda:0
[36m(_train_fn pid=20145)[0m train 8449
[36m(_train_fn pid=20145)[0m val 2785
[36m(_train_fn pid=20145)[0m start_epoch 0
[36m(_train_fn pid=20145)[0m max_epoch 8
[36m(_train_fn pid=20145)[0m 	iters: 100, epoch: 1 | loss: 0.4213813
[36m(_train_fn pid=20145)[0m 	speed: 0.0146s/iter; left time: 60.3675s
[36m(_train_fn pid=20145)[0m 	iters: 200, epoch: 1 | loss: 0.3697754
[36m(_train_fn pid=20145)[0m 	speed: 0.0087s/iter; left time: 35.0794s
[36m(_train_fn pid=20145)[0m 	iters: 300, epoch: 1 | loss: 0.4619356
[36m(_train_fn pid=20145)[0m 	speed: 0.0087s/iter; left time: 34.0046s
[36m(_train_fn pid=20145)[0m 	iters: 400, epoch: 1 | loss: 0.3432378
[36m(_train_fn pid=20145)[0m 	speed: 0.0087s/iter; left time: 33.2760s
[36m(_train_fn pid=20145)[0m 	iters: 500, epoch: 1 | loss: 0.3757495
[36m(_train_fn pid=20145)[0m 	speed: 0.0087s/iter; left time: 32.3593s
[36m(_train_fn pid=20145)[0m Updating learning rate to 0.005099904478550495
[36m(_train_fn pid=20145)[0m saving checkpoint...
[36m(_train_fn pid=20145)[0m Validation loss decreased (inf --> 0.7434).  Saving model state dict ...
[36m(_train_fn pid=20145)[0m Epoch: 1 cost time: 4.8890297412872314
[36m(_train_fn pid=20145)[0m Epoch: 1, Steps: 528 | Train Loss: 0.4672092 Vali Loss: 0.7434145 Best vali loss: 0.7434145
[36m(_train_fn pid=20145)[0m 	iters: 100, epoch: 2 | loss: 0.3525161
[36m(_train_fn pid=20145)[0m 	speed: 0.0180s/iter; left time: 64.6677s
[36m(_train_fn pid=20145)[0m 	iters: 200, epoch: 2 | loss: 0.2981959
[36m(_train_fn pid=20145)[0m 	speed: 0.0093s/iter; left time: 32.3965s
[36m(_train_fn pid=20145)[0m 	iters: 300, epoch: 2 | loss: 0.4304588
[36m(_train_fn pid=20145)[0m 	speed: 0.0093s/iter; left time: 31.4370s
[36m(_train_fn pid=20145)[0m 	iters: 400, epoch: 2 | loss: 0.2872742
[36m(_train_fn pid=20145)[0m 	speed: 0.0093s/iter; left time: 30.5060s
[36m(_train_fn pid=20145)[0m 	iters: 500, epoch: 2 | loss: 0.3326675
[36m(_train_fn pid=20145)[0m 	speed: 0.0093s/iter; left time: 29.8530s
[36m(_train_fn pid=20145)[0m Updating learning rate to 0.0025499522392752475
[36m(_train_fn pid=20145)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=20145)[0m saving checkpoint...
[36m(_train_fn pid=20145)[0m Epoch: 2 cost time: 4.944778680801392
[36m(_train_fn pid=20145)[0m Epoch: 2, Steps: 528 | Train Loss: 0.3700751 Vali Loss: 0.7491548 Best vali loss: 0.7434145
[36m(_train_fn pid=20145)[0m 	iters: 100, epoch: 3 | loss: 0.3600641
[36m(_train_fn pid=20145)[0m 	speed: 0.0174s/iter; left time: 53.4255s
[36m(_train_fn pid=20145)[0m 	iters: 200, epoch: 3 | loss: 0.2310399
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 25.6219s
[36m(_train_fn pid=20145)[0m 	iters: 300, epoch: 3 | loss: 0.3033015
[36m(_train_fn pid=20145)[0m 	speed: 0.0087s/iter; left time: 25.0783s
[36m(_train_fn pid=20145)[0m 	iters: 400, epoch: 3 | loss: 0.2993367
[36m(_train_fn pid=20145)[0m 	speed: 0.0087s/iter; left time: 24.1980s
[36m(_train_fn pid=20145)[0m 	iters: 500, epoch: 3 | loss: 0.3897696
[36m(_train_fn pid=20145)[0m 	speed: 0.0089s/iter; left time: 23.8806s
[36m(_train_fn pid=20145)[0m Updating learning rate to 0.0012749761196376237
[36m(_train_fn pid=20145)[0m saving checkpoint...
[36m(_train_fn pid=20145)[0m Validation loss decreased (0.7434 --> 0.6831).  Saving model state dict ...
[36m(_train_fn pid=20145)[0m Epoch: 3 cost time: 4.664748430252075
[36m(_train_fn pid=20145)[0m Epoch: 3, Steps: 528 | Train Loss: 0.3395861 Vali Loss: 0.6831015 Best vali loss: 0.6831015
[36m(_train_fn pid=20145)[0m 	iters: 100, epoch: 4 | loss: 0.3686146
[36m(_train_fn pid=20145)[0m 	speed: 0.0176s/iter; left time: 44.7671s

Trial status: 7 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:20:43. Total running time: 5min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00005 with best_valid_loss=0.6808662353747192 and params={'batch_size': 16, 'learning_rate': 0.003186053793189858, 'down_sampling_method': 'avg', 'd_model': 32, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 1, 'dropout': 0.12685076554929858, 'd_ff': 64}
[36m(_train_fn pid=20145)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00007_7_alpha_d_ff=2,batch_size=16,d_model=64,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1248,e_laye_2024-08-23_05-20-23/checkpoint_000003)
[36m(_train_fn pid=20145)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00007_7_alpha_d_ff=2,batch_size=16,d_model=64,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1248,e_laye_2024-08-23_05-20-23/checkpoint_000004)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00007   RUNNING                16       0.0050999     conv                            64              2   moving_avg                  55            2    0.124833        3           16.6861     0.339586       0.683102               0.683102 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2    0.1             8            6.67042    0.339293       0.698135               0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2    0.114428        5          103.089      0.349154       0.70033                0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4    0.102429        6           48.1969     0.295096       0.717921               0.681612 â”‚
â”‚ trial-e8d8b_00003   TERMINATED             32       0.00686432    avg                            256              2   moving_avg                  55            1    0.138301        8           40.4461     0.357968       0.697302               0.695051 â”‚
â”‚ trial-e8d8b_00004   TERMINATED             64       0.00104485    conv                           256              2   dft_decomp                  55            1    0.120698        8           38.8248     0.356168       0.699435               0.69943  â”‚
â”‚ trial-e8d8b_00005   TERMINATED             16       0.00318605    avg                             32              2   moving_avg                  15            1    0.126851        7           25.8859     0.330577       0.701659               0.680866 â”‚
â”‚ trial-e8d8b_00006   TERMINATED             32       0.00926473    avg                            128              3   dft_decomp                  55            4    0.130656        4           31.3762     5.74459e+09    7.45182e+09            0.715283 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=20145)[0m 	iters: 200, epoch: 4 | loss: 0.3242646
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 20.9383s
[36m(_train_fn pid=20145)[0m 	iters: 300, epoch: 4 | loss: 0.3203993
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 20.1137s
[36m(_train_fn pid=20145)[0m 	iters: 400, epoch: 4 | loss: 0.2722224
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 19.2283s
[36m(_train_fn pid=20145)[0m 	iters: 500, epoch: 4 | loss: 0.3701843
[36m(_train_fn pid=20145)[0m 	speed: 0.0087s/iter; left time: 18.5559s
[36m(_train_fn pid=20145)[0m Updating learning rate to 0.0006374880598188119
[36m(_train_fn pid=20145)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=20145)[0m saving checkpoint...
[36m(_train_fn pid=20145)[0m Epoch: 4 cost time: 4.579430341720581
[36m(_train_fn pid=20145)[0m Epoch: 4, Steps: 528 | Train Loss: 0.3240325 Vali Loss: 0.6838379 Best vali loss: 0.6831015
[36m(_train_fn pid=20145)[0m 	iters: 100, epoch: 5 | loss: 0.2738612
[36m(_train_fn pid=20145)[0m 	speed: 0.0173s/iter; left time: 34.7400s
[36m(_train_fn pid=20145)[0m 	iters: 200, epoch: 5 | loss: 0.2355609
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 16.4407s
[36m(_train_fn pid=20145)[0m 	iters: 300, epoch: 5 | loss: 0.2460091
[36m(_train_fn pid=20145)[0m 	speed: 0.0085s/iter; left time: 15.4924s
[36m(_train_fn pid=20145)[0m 	iters: 400, epoch: 5 | loss: 0.2999986
[36m(_train_fn pid=20145)[0m 	speed: 0.0085s/iter; left time: 14.5769s
[36m(_train_fn pid=20145)[0m 	iters: 500, epoch: 5 | loss: 0.3377827
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 13.8299s
[36m(_train_fn pid=20145)[0m Updating learning rate to 0.00031874402990940594
[36m(_train_fn pid=20145)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=20145)[0m saving checkpoint...
[36m(_train_fn pid=20145)[0m Epoch: 5 cost time: 4.563551902770996
[36m(_train_fn pid=20145)[0m Epoch: 5, Steps: 528 | Train Loss: 0.3110421 Vali Loss: 0.7013762 Best vali loss: 0.6831015
[36m(_train_fn pid=20145)[0m 	iters: 100, epoch: 6 | loss: 0.3141783
[36m(_train_fn pid=20145)[0m 	speed: 0.0173s/iter; left time: 25.6226s
[36m(_train_fn pid=20145)[0m 	iters: 200, epoch: 6 | loss: 0.3260747
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 11.9753s
[36m(_train_fn pid=20145)[0m 	iters: 300, epoch: 6 | loss: 0.3082586
[36m(_train_fn pid=20145)[0m 	speed: 0.0086s/iter; left time: 11.0853s
[36m(_train_fn pid=20145)[0m 	iters: 400, epoch: 6 | loss: 0.2992797
[36m(_train_fn pid=20145)[0m 	speed: 0.0085s/iter; left time: 10.1080s
[36m(_train_fn pid=20145)[0m 	iters: 500, epoch: 6 | loss: 0.3232689
[36m(_train_fn pid=20145)[0m 	speed: 0.0085s/iter; left time: 9.2611s

Trial trial-e8d8b_00007 completed after 6 iterations at 2024-08-23 05:20:58. Total running time: 5min 44s
[36m(_train_fn pid=20145)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00007_7_alpha_d_ff=2,batch_size=16,d_model=64,decomp_method=moving_avg,down_sampling_method=conv,dropout=0.1248,e_laye_2024-08-23_05-20-23/checkpoint_000005)
2024-08-23 05:21:01,491	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000000)
2024-08-23 05:21:02,112	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000001)
2024-08-23 05:21:02,746	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000002)
2024-08-23 05:21:03,386	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000003)
2024-08-23 05:21:04,016	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000004)
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000005)
2024-08-23 05:21:04,643	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:21:05,294	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000006)
2024-08-23 05:21:05,954	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=20686)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00008_8_alpha_d_ff=3,batch_size=128,d_model=8,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0961,e_laye_2024-08-23_05-20-58/checkpoint_000007)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.16724 â”‚
â”‚ time_total_s                                32.15212 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.6831 â”‚
â”‚ train_loss                                   0.30267 â”‚
â”‚ valid_loss                                   0.69781 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=20145)[0m Updating learning rate to 0.00015937201495470297
[36m(_train_fn pid=20145)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=20145)[0m saving checkpoint...
[36m(_train_fn pid=20145)[0m Epoch: 6 cost time: 4.583155632019043
[36m(_train_fn pid=20145)[0m Epoch: 6, Steps: 528 | Train Loss: 0.3026690 Vali Loss: 0.6978060 Best vali loss: 0.6831015
[36m(_train_fn pid=20145)[0m Early stopping

Trial trial-e8d8b_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00008 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  3 â”‚
â”‚ batch_size                                128 â”‚
â”‚ d_model                                     8 â”‚
â”‚ decomp_method                      dft_decomp â”‚
â”‚ down_sampling_method                     conv â”‚
â”‚ dropout                               0.09609 â”‚
â”‚ e_layers                                    1 â”‚
â”‚ learning_rate                         0.00267 â”‚
â”‚ moving_avg                                 75 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=20686)[0m configuration
[36m(_train_fn pid=20686)[0m {'batch_size': 128, 'learning_rate': 0.0026693124592344062, 'down_sampling_method': 'conv', 'd_model': 8, 'decomp_method': 'dft_decomp', 'moving_avg': 75, 'e_layers': 1, 'dropout': 0.09608811687926487, 'd_ff': 24}
[36m(_train_fn pid=20686)[0m Use GPU: cuda:0
[36m(_train_fn pid=20686)[0m train 8449
[36m(_train_fn pid=20686)[0m val 2785
[36m(_train_fn pid=20686)[0m start_epoch 0
[36m(_train_fn pid=20686)[0m max_epoch 8
[36m(_train_fn pid=20686)[0m Updating learning rate to 0.0026693124592344062
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Validation loss decreased (inf --> 1.3584).  Saving model state dict ...
[36m(_train_fn pid=20686)[0m Epoch: 1 cost time: 1.0444722175598145
[36m(_train_fn pid=20686)[0m Epoch: 1, Steps: 66 | Train Loss: 0.7826947 Vali Loss: 1.3583846 Best vali loss: 1.3583846
[36m(_train_fn pid=20686)[0m Updating learning rate to 0.0013346562296172031
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Validation loss decreased (1.3584 --> 0.7549).  Saving model state dict ...
[36m(_train_fn pid=20686)[0m Epoch: 2 cost time: 0.49797892570495605
[36m(_train_fn pid=20686)[0m Epoch: 2, Steps: 66 | Train Loss: 0.4475757 Vali Loss: 0.7548522 Best vali loss: 0.7548522
[36m(_train_fn pid=20686)[0m Updating learning rate to 0.0006673281148086016
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Validation loss decreased (0.7549 --> 0.7376).  Saving model state dict ...
[36m(_train_fn pid=20686)[0m Epoch: 3 cost time: 0.5115892887115479
[36m(_train_fn pid=20686)[0m Epoch: 3, Steps: 66 | Train Loss: 0.3823198 Vali Loss: 0.7376064 Best vali loss: 0.7376064
[36m(_train_fn pid=20686)[0m Updating learning rate to 0.0003336640574043008
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Validation loss decreased (0.7376 --> 0.7259).  Saving model state dict ...
[36m(_train_fn pid=20686)[0m Epoch: 4 cost time: 0.5193355083465576
[36m(_train_fn pid=20686)[0m Epoch: 4, Steps: 66 | Train Loss: 0.3750423 Vali Loss: 0.7258890 Best vali loss: 0.7258890
[36m(_train_fn pid=20686)[0m Updating learning rate to 0.0001668320287021504
[36m(_train_fn pid=20686)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Epoch: 5 cost time: 0.5105116367340088
[36m(_train_fn pid=20686)[0m Epoch: 5, Steps: 66 | Train Loss: 0.3719487 Vali Loss: 0.7269788 Best vali loss: 0.7258890
[36m(_train_fn pid=20686)[0m Updating learning rate to 8.34160143510752e-05
[36m(_train_fn pid=20686)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Epoch: 6 cost time: 0.5071918964385986
[36m(_train_fn pid=20686)[0m Epoch: 6, Steps: 66 | Train Loss: 0.3709482 Vali Loss: 0.7259012 Best vali loss: 0.7258890
[36m(_train_fn pid=20686)[0m Updating learning rate to 4.17080071755376e-05
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Validation loss decreased (0.7259 --> 0.7253).  Saving model state dict ...
[36m(_train_fn pid=20686)[0m Epoch: 7 cost time: 0.5281364917755127
[36m(_train_fn pid=20686)[0m Epoch: 7, Steps: 66 | Train Loss: 0.3699479 Vali Loss: 0.7253303 Best vali loss: 0.7253303

Trial trial-e8d8b_00008 completed after 8 iterations at 2024-08-23 05:21:05. Total running time: 5min 52s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.65737 â”‚
â”‚ time_total_s                                 6.03503 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.72514 â”‚
â”‚ train_loss                                   0.36961 â”‚
â”‚ valid_loss                                   0.72514 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=20686)[0m Updating learning rate to 2.08540035877688e-05
[36m(_train_fn pid=20686)[0m saving checkpoint...
[36m(_train_fn pid=20686)[0m Validation loss decreased (0.7253 --> 0.7251).  Saving model state dict ...
[36m(_train_fn pid=20686)[0m Epoch: 8 cost time: 0.5331966876983643
[36m(_train_fn pid=20686)[0m Epoch: 8, Steps: 66 | Train Loss: 0.3696144 Vali Loss: 0.7251356 Best vali loss: 0.7251356

Trial trial-e8d8b_00009 started with configuration:
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000000)
2024-08-23 05:21:10,621	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:21:12,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000001)
2024-08-23 05:21:14,305	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00009 config                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                                  2 â”‚
â”‚ batch_size                                128 â”‚
â”‚ d_model                                    64 â”‚
â”‚ decomp_method                      dft_decomp â”‚
â”‚ down_sampling_method                      avg â”‚
â”‚ dropout                               0.08205 â”‚
â”‚ e_layers                                    2 â”‚
â”‚ learning_rate                         0.00135 â”‚
â”‚ moving_avg                                 15 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=21353)[0m configuration
[36m(_train_fn pid=21353)[0m {'batch_size': 128, 'learning_rate': 0.001350891229764074, 'down_sampling_method': 'avg', 'd_model': 64, 'decomp_method': 'dft_decomp', 'moving_avg': 15, 'e_layers': 2, 'dropout': 0.0820454713029851, 'd_ff': 128}
[36m(_train_fn pid=21353)[0m Use GPU: cuda:0
[36m(_train_fn pid=21353)[0m train 8449
[36m(_train_fn pid=21353)[0m val 2785
[36m(_train_fn pid=21353)[0m start_epoch 0
[36m(_train_fn pid=21353)[0m max_epoch 8
[36m(_train_fn pid=21353)[0m Updating learning rate to 0.001350891229764074
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Validation loss decreased (inf --> 1.2588).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 1 cost time: 1.9925339221954346
[36m(_train_fn pid=21353)[0m Epoch: 1, Steps: 66 | Train Loss: 0.6761797 Vali Loss: 1.2587813 Best vali loss: 1.2587813
[36m(_train_fn pid=21353)[0m Updating learning rate to 0.000675445614882037
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Validation loss decreased (1.2588 --> 0.7452).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 2 cost time: 1.5391900539398193
[36m(_train_fn pid=21353)[0m Epoch: 2, Steps: 66 | Train Loss: 0.4540431 Vali Loss: 0.7451842 Best vali loss: 0.7451842

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:21:13. Total running time: 6min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00005 with best_valid_loss=0.6808662353747192 and params={'batch_size': 16, 'learning_rate': 0.003186053793189858, 'down_sampling_method': 'avg', 'd_model': 32, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 1, 'dropout': 0.12685076554929858, 'd_ff': 64}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00009   RUNNING               128       0.00135089    avg                             64              2   dft_decomp                  15            2   0.0820455        2            4.55739    0.454043       0.745184               0.745184 â”‚
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2   0.1              8            6.67042    0.339293       0.698135               0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2   0.114428         5          103.089      0.349154       0.70033                0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4   0.102429         6           48.1969     0.295096       0.717921               0.681612 â”‚
â”‚ trial-e8d8b_00003   TERMINATED             32       0.00686432    avg                            256              2   moving_avg                  55            1   0.138301         8           40.4461     0.357968       0.697302               0.695051 â”‚
â”‚ trial-e8d8b_00004   TERMINATED             64       0.00104485    conv                           256              2   dft_decomp                  55            1   0.120698         8           38.8248     0.356168       0.699435               0.69943  â”‚
â”‚ trial-e8d8b_00005   TERMINATED             16       0.00318605    avg                             32              2   moving_avg                  15            1   0.126851         7           25.8859     0.330577       0.701659               0.680866 â”‚
â”‚ trial-e8d8b_00006   TERMINATED             32       0.00926473    avg                            128              3   dft_decomp                  55            4   0.130656         4           31.3762     5.74459e+09    7.45182e+09            0.715283 â”‚
â”‚ trial-e8d8b_00007   TERMINATED             16       0.0050999     conv                            64              2   moving_avg                  55            2   0.124833         6           32.1521     0.302669       0.697806               0.683102 â”‚
â”‚ trial-e8d8b_00008   TERMINATED            128       0.00266931    conv                             8              3   dft_decomp                  75            1   0.0960881        8            6.03503    0.369614       0.725136               0.725136 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=21353)[0m Updating learning rate to 0.0003377228074410185
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000002)
2024-08-23 05:21:16,156	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000003)
2024-08-23 05:21:17,998	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000004)
2024-08-23 05:21:19,837	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000005)
2024-08-23 05:21:21,689	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000006)
2024-08-23 05:21:23,549	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:21:23,560	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test' in 0.0088s.
[36m(_train_fn pid=21353)[0m Validation loss decreased (0.7452 --> 0.7234).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 3 cost time: 1.5378086566925049
[36m(_train_fn pid=21353)[0m Epoch: 3, Steps: 66 | Train Loss: 0.3747021 Vali Loss: 0.7234425 Best vali loss: 0.7234425
[36m(_train_fn pid=21353)[0m Updating learning rate to 0.00016886140372050925
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Validation loss decreased (0.7234 --> 0.7197).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 4 cost time: 1.5475215911865234
[36m(_train_fn pid=21353)[0m Epoch: 4, Steps: 66 | Train Loss: 0.3667892 Vali Loss: 0.7196978 Best vali loss: 0.7196978
[36m(_train_fn pid=21353)[0m Updating learning rate to 8.443070186025462e-05
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Validation loss decreased (0.7197 --> 0.7158).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 5 cost time: 1.5379638671875
[36m(_train_fn pid=21353)[0m Epoch: 5, Steps: 66 | Train Loss: 0.3643693 Vali Loss: 0.7157893 Best vali loss: 0.7157893
[36m(_train_fn pid=21353)[0m Updating learning rate to 4.221535093012731e-05
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Validation loss decreased (0.7158 --> 0.7125).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 6 cost time: 1.5425817966461182
[36m(_train_fn pid=21353)[0m Epoch: 6, Steps: 66 | Train Loss: 0.3625066 Vali Loss: 0.7124663 Best vali loss: 0.7124663
[36m(_train_fn pid=21353)[0m Updating learning rate to 2.1107675465063656e-05
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Validation loss decreased (0.7125 --> 0.7109).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 7 cost time: 1.548133373260498
[36m(_train_fn pid=21353)[0m Epoch: 7, Steps: 66 | Train Loss: 0.3615438 Vali Loss: 0.7108824 Best vali loss: 0.7108824

Trial trial-e8d8b_00009 completed after 8 iterations at 2024-08-23 05:21:23. Total running time: 6min 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e8d8b_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.85726 â”‚
â”‚ time_total_s                                15.62433 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.70997 â”‚
â”‚ train_loss                                   0.36104 â”‚
â”‚ valid_loss                                   0.70997 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-23 05:21:23. Total running time: 6min 10s
Logical resource usage: 0/32 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e8d8b_00005 with best_valid_loss=0.6808662353747192 and params={'batch_size': 16, 'learning_rate': 0.003186053793189858, 'down_sampling_method': 'avg', 'd_model': 32, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 1, 'dropout': 0.12685076554929858, 'd_ff': 64}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate   down_sampling_method       d_model     alpha_d_ff   decomp_method       moving_avg     e_layers     dropout     iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e8d8b_00000   TERMINATED            128       0.01          avg                             16              2   moving_avg                  25            2   0.1              8            6.67042    0.339293       0.698135               0.698135 â”‚
â”‚ trial-e8d8b_00001   TERMINATED             16       0.000593273   conv                           512              2   moving_avg                  15            2   0.114428         5          103.089      0.349154       0.70033                0.698605 â”‚
â”‚ trial-e8d8b_00002   TERMINATED             16       0.00940548    avg                             16              4   dft_decomp                  75            4   0.102429         6           48.1969     0.295096       0.717921               0.681612 â”‚
â”‚ trial-e8d8b_00003   TERMINATED             32       0.00686432    avg                            256              2   moving_avg                  55            1   0.138301         8           40.4461     0.357968       0.697302               0.695051 â”‚
â”‚ trial-e8d8b_00004   TERMINATED             64       0.00104485    conv                           256              2   dft_decomp                  55            1   0.120698         8           38.8248     0.356168       0.699435               0.69943  â”‚
â”‚ trial-e8d8b_00005   TERMINATED             16       0.00318605    avg                             32              2   moving_avg                  15            1   0.126851         7           25.8859     0.330577       0.701659               0.680866 â”‚
â”‚ trial-e8d8b_00006   TERMINATED             32       0.00926473    avg                            128              3   dft_decomp                  55            4   0.130656         4           31.3762     5.74459e+09    7.45182e+09            0.715283 â”‚
â”‚ trial-e8d8b_00007   TERMINATED             16       0.0050999     conv                            64              2   moving_avg                  55            2   0.124833         6           32.1521     0.302669       0.697806               0.683102 â”‚
â”‚ trial-e8d8b_00008   TERMINATED            128       0.00266931    conv                             8              3   dft_decomp                  75            1   0.0960881        8            6.03503    0.369614       0.725136               0.725136 â”‚
â”‚ trial-e8d8b_00009   TERMINATED            128       0.00135089    avg                             64              2   dft_decomp                  15            2   0.0820455        8           15.6243     0.361041       0.709971               0.709971 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 16, 'learning_rate': 0.003186053793189858, 'down_sampling_method': 'avg', 'd_model': 32, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'e_layers': 1, 'dropout': 0.12685076554929858, 'd_ff': 64}
[36m(_train_fn pid=21353)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/random_search/ETTh1_96_96_test/trial-e8d8b_00009_9_alpha_d_ff=2,batch_size=128,d_model=64,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0820,e_laye_2024-08-23_05-21-05/checkpoint_000007)
[36m(_train_fn pid=21353)[0m Updating learning rate to 1.0553837732531828e-05
[36m(_train_fn pid=21353)[0m saving checkpoint...
[36m(_train_fn pid=21353)[0m Validation loss decreased (0.7109 --> 0.7100).  Saving model state dict ...
[36m(_train_fn pid=21353)[0m Epoch: 8 cost time: 1.5479309558868408
[36m(_train_fn pid=21353)[0m Epoch: 8, Steps: 66 | Train Loss: 0.3610411 Vali Loss: 0.7099709 Best vali loss: 0.7099709


Time taken (1 parallel trials): 375 seconds


bash: tfm_scripts/tune_long_term_forecast/ETT_script/ETTh1_96_96/random_search.sh: line 69: syntax error near unexpected token `done'
bash: tfm_scripts/tune_long_term_forecast/ETT_script/ETTh1_96_96/random_search.sh: line 69: `done'
