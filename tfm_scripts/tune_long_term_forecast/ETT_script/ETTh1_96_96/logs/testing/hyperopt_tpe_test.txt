horizon=96
maxconcurrent=1
gpu_fraction=$(echo "scale=2; 1/$maxconcurrent" | bc)  # Calculate GPU fraction with 2 decimal places
start_time=$(date +%s)  # Get the current time in seconds
python3 tune_timemixer.py \
    --model TimeMixer \
    --data ETTh1 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh1.csv \
    --features M \
    --seq_len 96 \
    --label_len 0 \
    --pred_len $horizon \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --d_model 16 \
    --d_ff 32 \
    --down_sampling_layers 3 \
    --down_sampling_window 2 \
    --down_sampling_method avg \
    --decomp_method moving_avg \
    --moving_avg 25 \
    --train_epochs 8 \
    --patience 3 \
    --num_workers 1 \
    --gpu 0 \
    --tune_search_algorithm hyperopt_tpe \
    --tune_trial_scheduler fifo \
    --tune_storage_path ./checkpoints/hptunning/hyperopt_tpe/ \
    --tune_experiment_name ETTh1_96_${horizon} \
    --tune_objective best_valid_loss \
    --tune_num_samples 10 \
    --tune_max_trial_time_s 100 \
    --tune_time_budget_s 14400 \
    --tune_max_concurrent $maxconcurrent \
    --tune_gpu_resources $gpu_fraction \
    --tune_cpu_resources 1 \
    --tune_default_config "{
        \"batch_size\": 128, \
        \"learning_rate\": 0.01, \
        \"down_sampling_method\": \"avg\", \
        \"d_model\": 16, \
        \"alpha_d_ff\": 2, \
        \"decomp_method\": \"moving_avg\", \
        \"moving_avg\": 25, \
        \"e_layers\": 2, \
        \"dropout\": 0.1
    }" \
    --tune_param_space "{
        \"batch_size\": [\"choice\", [16, 32, 64, 128]], \
        \"learning_rate\": [\"loguniform\", [0.0005, 0.012]], \
        \"down_sampling_method\": [\"choice\", [\"avg\", \"conv\"]], \
        \"d_model\": [\"choice\", [8, 16, 32, 64, 128, 256, 512]], \
        \"alpha_d_ff\": [\"choice\", [2, 3, 4]], \
        \"decomp_method\": [\"choice\", [[\"moving_avg\", \"moving_avg\", [15, 25, 35, 55, 75]], [\"dft_decomp\"]]], \
        \"e_layers\": [\"choice\", [1, 2, 3, 4]], \
        \"dropout\": [\"normal\", [0.1, 0.025]]
    }" \
    --tune_hyperopt_n_initial_points 150 \
    --tune_hyperopt_gamma 0.25 \
    --seed 123;
end_time=$(date +%s)  # Get the current time in seconds
elapsed_time=$((end_time - start_time))  # Calculate the elapsed time
echo ""
echo ""
echo "Time taken ($maxconcurrent parallel trials): $elapsed_time seconds"
echo ""
echo ""2024-08-23 05:21:28,077	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-23 05:21:28,492	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-23 05:21:33,322	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000000)
2024-08-23 05:21:35,351	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000001)
2024-08-23 05:21:37,382	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000002)
2024-08-23 05:21:39,419	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000003)
2024-08-23 05:21:41,593	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000004)
2024-08-23 05:21:43,625	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000005)
2024-08-23 05:21:45,661	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000006)
2024-08-23 05:21:47,691	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24232)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-e4f57f3b_1_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.0990,e_layers=4,_2024-08-23_05-21-28/checkpoint_000007)
╭──────────────────────────────────────────────────╮
│ Configuration for experiment     ETTh1_96_96     │
├──────────────────────────────────────────────────┤
│ Search algorithm                 SearchGenerator │
│ Scheduler                        FIFOScheduler   │
│ Number of trials                 10              │
╰──────────────────────────────────────────────────╯

View detailed results here: /home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-23_05-21-27_557019_22038/artifacts/2024-08-23_05-21-28/ETTh1_96_96/driver_artifacts`

Trial status: 1 PENDING
Current time: 2024-08-23 05:21:28. Total running time: 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
╭───────────────────────────╮
│ Trial name       status   │
├───────────────────────────┤
│ trial-e4f57f3b   PENDING  │
╰───────────────────────────╯

Trial trial-e4f57f3b started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-e4f57f3b config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               4 │
│ batch_size                              64 │
│ d_model                                  8 │
│ decomp_method/decomp_method     dft_decomp │
│ down_sampling_method                   avg │
│ dropout                            0.09904 │
│ e_layers                                 4 │
│ learning_rate                      0.00163 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=24232)[0m configuration
[36m(_train_fn pid=24232)[0m {'batch_size': 64, 'd_model': 8, 'decomp_method': 'dft_decomp', 'down_sampling_method': 'avg', 'dropout': 0.0990448800935579, 'e_layers': 4, 'learning_rate': 0.001631158227501476, 'd_ff': 32}
[36m(_train_fn pid=24232)[0m Use GPU: cuda:0
[36m(_train_fn pid=24232)[0m train 8449
[36m(_train_fn pid=24232)[0m val 2785
[36m(_train_fn pid=24232)[0m start_epoch 0
[36m(_train_fn pid=24232)[0m max_epoch 8
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 1 | loss: 0.8266081
[36m(_train_fn pid=24232)[0m 	speed: 0.0219s/iter; left time: 20.9554s
[36m(_train_fn pid=24232)[0m Updating learning rate to 0.001631158227501476
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (inf --> 1.4663).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 1 cost time: 2.3472604751586914
[36m(_train_fn pid=24232)[0m Epoch: 1, Steps: 132 | Train Loss: 0.8622507 Vali Loss: 1.4662864 Best vali loss: 1.4662864
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 2 | loss: 0.3285539
[36m(_train_fn pid=24232)[0m 	speed: 0.0206s/iter; left time: 17.0024s
[36m(_train_fn pid=24232)[0m Updating learning rate to 0.000815579113750738
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (1.4663 --> 0.7166).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 2 cost time: 1.7598047256469727
[36m(_train_fn pid=24232)[0m Epoch: 2, Steps: 132 | Train Loss: 0.4166836 Vali Loss: 0.7166485 Best vali loss: 0.7166485
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 3 | loss: 0.3805627
[36m(_train_fn pid=24232)[0m 	speed: 0.0203s/iter; left time: 14.0742s
[36m(_train_fn pid=24232)[0m Updating learning rate to 0.000407789556875369
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (0.7166 --> 0.7003).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 3 cost time: 1.759779691696167
[36m(_train_fn pid=24232)[0m Epoch: 3, Steps: 132 | Train Loss: 0.3646684 Vali Loss: 0.7002883 Best vali loss: 0.7002883
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 4 | loss: 0.3567885
[36m(_train_fn pid=24232)[0m 	speed: 0.0204s/iter; left time: 11.4182s
[36m(_train_fn pid=24232)[0m Updating learning rate to 0.0002038947784376845
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (0.7003 --> 0.6990).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 4 cost time: 1.7686872482299805
[36m(_train_fn pid=24232)[0m Epoch: 4, Steps: 132 | Train Loss: 0.3589351 Vali Loss: 0.6990472 Best vali loss: 0.6990472
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 5 | loss: 0.3487061
[36m(_train_fn pid=24232)[0m 	speed: 0.0214s/iter; left time: 9.1990s
[36m(_train_fn pid=24232)[0m Updating learning rate to 0.00010194738921884225
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (0.6990 --> 0.6970).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 5 cost time: 1.9085569381713867
[36m(_train_fn pid=24232)[0m Epoch: 5, Steps: 132 | Train Loss: 0.3560314 Vali Loss: 0.6969894 Best vali loss: 0.6969894
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 6 | loss: 0.3484287
[36m(_train_fn pid=24232)[0m 	speed: 0.0206s/iter; left time: 6.1286s
[36m(_train_fn pid=24232)[0m Updating learning rate to 5.097369460942112e-05
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (0.6970 --> 0.6950).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 6 cost time: 1.7624406814575195
[36m(_train_fn pid=24232)[0m Epoch: 6, Steps: 132 | Train Loss: 0.3543659 Vali Loss: 0.6950189 Best vali loss: 0.6950189
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 7 | loss: 0.2976305
[36m(_train_fn pid=24232)[0m 	speed: 0.0203s/iter; left time: 3.3538s
[36m(_train_fn pid=24232)[0m Updating learning rate to 2.548684730471056e-05
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (0.6950 --> 0.6947).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 7 cost time: 1.7655057907104492
[36m(_train_fn pid=24232)[0m Epoch: 7, Steps: 132 | Train Loss: 0.3533731 Vali Loss: 0.6946513 Best vali loss: 0.6946513
[36m(_train_fn pid=24232)[0m 	iters: 100, epoch: 8 | loss: 0.4348508
[36m(_train_fn pid=24232)[0m 	speed: 0.0204s/iter; left time: 0.6728s

Trial trial-e4f57f3b completed after 8 iterations at 2024-08-23 05:21:47. Total running time: 19s
╭───────────────────────────────────────────────────╮
│ Trial trial-e4f57f3b result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000007 │
│ time_this_iter_s                          2.02827 │
│ time_total_s                             17.39345 │
│ training_iteration                              8 │
│ best_valid_loss                           0.69442 │
│ train_loss                                0.35325 │
│ valid_loss                                0.69442 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=24232)[0m Updating learning rate to 1.274342365235528e-05
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000000)
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000001)
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000002)
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000003)
[36m(_train_fn pid=24232)[0m saving checkpoint...
[36m(_train_fn pid=24232)[0m Validation loss decreased (0.6947 --> 0.6944).  Saving model state dict ...
[36m(_train_fn pid=24232)[0m Epoch: 8 cost time: 1.7650022506713867
[36m(_train_fn pid=24232)[0m Epoch: 8, Steps: 132 | Train Loss: 0.3532528 Vali Loss: 0.6944198 Best vali loss: 0.6944198

Trial trial-66aadf09 started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-66aadf09 config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               3 │
│ batch_size                              16 │
│ d_model                                 16 │
│ decomp_method/decomp_method     dft_decomp │
│ down_sampling_method                  conv │
│ dropout                            0.09013 │
│ e_layers                                 2 │
│ learning_rate                      0.01064 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=24973)[0m configuration
[36m(_train_fn pid=24973)[0m {'batch_size': 16, 'd_model': 16, 'decomp_method': 'dft_decomp', 'down_sampling_method': 'conv', 'dropout': 0.09012666779719704, 'e_layers': 2, 'learning_rate': 0.01064263000037022, 'd_ff': 48}
[36m(_train_fn pid=24973)[0m Use GPU: cuda:0
[36m(_train_fn pid=24973)[0m train 8449
[36m(_train_fn pid=24973)[0m val 2785
[36m(_train_fn pid=24973)[0m start_epoch 0
[36m(_train_fn pid=24973)[0m max_epoch 8
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 1 | loss: 0.4941253
[36m(_train_fn pid=24973)[0m 	speed: 0.0170s/iter; left time: 70.1554s
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 1 | loss: 0.4416884
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 36.2139s
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 1 | loss: 0.5636677
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 35.2298s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 1 | loss: 0.2957281
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 34.3458s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 1 | loss: 0.3560708
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 33.5500s
[36m(_train_fn pid=24973)[0m Updating learning rate to 0.01064263000037022
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Validation loss decreased (inf --> 0.7217).  Saving model state dict ...
[36m(_train_fn pid=24973)[0m Epoch: 1 cost time: 5.245222568511963
[36m(_train_fn pid=24973)[0m Epoch: 1, Steps: 528 | Train Loss: 0.4280933 Vali Loss: 0.7216913 Best vali loss: 0.7216913
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 2 | loss: 0.3266970
[36m(_train_fn pid=24973)[0m 	speed: 0.0181s/iter; left time: 65.1507s
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 2 | loss: 0.2622954
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 31.5399s

Trial status: 1 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:21:58. Total running time: 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e4f57f3b with best_valid_loss=0.6944197995025058 and params={'alpha_d_ff': 4, 'batch_size': 64, 'd_model': 8, 'decomp_method': {'decomp_method': 'dft_decomp'}, 'down_sampling_method': 'avg', 'dropout': 0.0990448800935579, 'e_layers': 4, 'learning_rate': 0.001631158227501476}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-66aadf09   RUNNING           1            6.28543       0.428093       0.721691            0.721691 │
│ trial-e4f57f3b   TERMINATED        8           17.3934        0.353253       0.69442             0.69442  │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 2 | loss: 0.2521690
[36m(_train_fn pid=24973)[0m 	speed: 0.0085s/iter; left time: 29.0104s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 2 | loss: 0.4768372
[36m(_train_fn pid=24973)[0m 	speed: 0.0083s/iter; left time: 27.3861s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 2 | loss: 0.3807485
[36m(_train_fn pid=24973)[0m 	speed: 0.0083s/iter; left time: 26.4432s
[36m(_train_fn pid=24973)[0m Updating learning rate to 0.00532131500018511
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Validation loss decreased (0.7217 --> 0.7203).  Saving model state dict ...
[36m(_train_fn pid=24973)[0m Epoch: 2 cost time: 4.5898118019104
[36m(_train_fn pid=24973)[0m Epoch: 2, Steps: 528 | Train Loss: 0.3653543 Vali Loss: 0.7203389 Best vali loss: 0.7203389
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 3 | loss: 0.4029551
[36m(_train_fn pid=24973)[0m 	speed: 0.0183s/iter; left time: 56.2330s
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 3 | loss: 0.2995920
[36m(_train_fn pid=24973)[0m 	speed: 0.0091s/iter; left time: 26.9511s
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 3 | loss: 0.3210671
[36m(_train_fn pid=24973)[0m 	speed: 0.0084s/iter; left time: 24.1329s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 3 | loss: 0.3564851
[36m(_train_fn pid=24973)[0m 	speed: 0.0083s/iter; left time: 22.8922s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 3 | loss: 0.2808388
[36m(_train_fn pid=24973)[0m 	speed: 0.0082s/iter; left time: 21.8907s
[36m(_train_fn pid=24973)[0m Updating learning rate to 0.002660657500092555
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Validation loss decreased (0.7203 --> 0.7179).  Saving model state dict ...
[36m(_train_fn pid=24973)[0m Epoch: 3 cost time: 4.576367616653442
[36m(_train_fn pid=24973)[0m Epoch: 3, Steps: 528 | Train Loss: 0.3388322 Vali Loss: 0.7179317 Best vali loss: 0.7179317
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 4 | loss: 0.4145930
[36m(_train_fn pid=24973)[0m 	speed: 0.0172s/iter; left time: 43.6572s
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 4 | loss: 0.3701388
[36m(_train_fn pid=24973)[0m 	speed: 0.0082s/iter; left time: 20.0396s
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 4 | loss: 0.3984303
[36m(_train_fn pid=24973)[0m 	speed: 0.0083s/iter; left time: 19.3295s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 4 | loss: 0.4611123
[36m(_train_fn pid=24973)[0m 	speed: 0.0082s/iter; left time: 18.4863s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 4 | loss: 0.3128424
[36m(_train_fn pid=24973)[0m 	speed: 0.0083s/iter; left time: 17.7177s
[36m(_train_fn pid=24973)[0m Updating learning rate to 0.0013303287500462775
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Validation loss decreased (0.7179 --> 0.7105).  Saving model state dict ...
[36m(_train_fn pid=24973)[0m Epoch: 4 cost time: 4.4025444984436035
[36m(_train_fn pid=24973)[0m Epoch: 4, Steps: 528 | Train Loss: 0.3230690 Vali Loss: 0.7104551 Best vali loss: 0.7104551
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000004)
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000005)
2024-08-23 05:22:27,646	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000006)
[36m(_train_fn pid=24973)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-66aadf09_2_alpha_d_ff=3,batch_size=16,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0901,e_layers=_2024-08-23_05-21-47/checkpoint_000007)
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 5 | loss: 0.2959537
[36m(_train_fn pid=24973)[0m 	speed: 0.0180s/iter; left time: 36.2836s
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 5 | loss: 0.2890739
[36m(_train_fn pid=24973)[0m 	speed: 0.0091s/iter; left time: 17.3161s
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 5 | loss: 0.4074879
[36m(_train_fn pid=24973)[0m 	speed: 0.0091s/iter; left time: 16.4656s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 5 | loss: 0.2721756
[36m(_train_fn pid=24973)[0m 	speed: 0.0085s/iter; left time: 14.5824s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 5 | loss: 0.2633626
[36m(_train_fn pid=24973)[0m 	speed: 0.0082s/iter; left time: 13.2968s
[36m(_train_fn pid=24973)[0m Updating learning rate to 0.0006651643750231387
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Validation loss decreased (0.7105 --> 0.7091).  Saving model state dict ...
[36m(_train_fn pid=24973)[0m Epoch: 5 cost time: 4.672535419464111
[36m(_train_fn pid=24973)[0m Epoch: 5, Steps: 528 | Train Loss: 0.3088878 Vali Loss: 0.7091024 Best vali loss: 0.7091024
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 6 | loss: 0.3265457
[36m(_train_fn pid=24973)[0m 	speed: 0.0180s/iter; left time: 26.7667s
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 6 | loss: 0.2677941
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 12.4932s
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 6 | loss: 0.2461347
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 11.5881s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 6 | loss: 0.3129528
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 10.6622s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 6 | loss: 0.2852623
[36m(_train_fn pid=24973)[0m 	speed: 0.0090s/iter; left time: 9.7819s
[36m(_train_fn pid=24973)[0m Updating learning rate to 0.00033258218751156936
[36m(_train_fn pid=24973)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Epoch: 6 cost time: 4.806168079376221
[36m(_train_fn pid=24973)[0m Epoch: 6, Steps: 528 | Train Loss: 0.2979982 Vali Loss: 0.7353089 Best vali loss: 0.7091024
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 7 | loss: 0.2756332
[36m(_train_fn pid=24973)[0m 	speed: 0.0171s/iter; left time: 16.3911s
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 7 | loss: 0.3528218
[36m(_train_fn pid=24973)[0m 	speed: 0.0081s/iter; left time: 6.9002s
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 7 | loss: 0.2996983
[36m(_train_fn pid=24973)[0m 	speed: 0.0081s/iter; left time: 6.0963s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 7 | loss: 0.2806444
[36m(_train_fn pid=24973)[0m 	speed: 0.0081s/iter; left time: 5.3113s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 7 | loss: 0.3181641
[36m(_train_fn pid=24973)[0m 	speed: 0.0082s/iter; left time: 4.5624s
[36m(_train_fn pid=24973)[0m Updating learning rate to 0.00016629109375578468
[36m(_train_fn pid=24973)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Epoch: 7 cost time: 4.318148374557495
[36m(_train_fn pid=24973)[0m Epoch: 7, Steps: 528 | Train Loss: 0.2918712 Vali Loss: 0.7452586 Best vali loss: 0.7091024
[36m(_train_fn pid=24973)[0m 	iters: 100, epoch: 8 | loss: 0.3529081
[36m(_train_fn pid=24973)[0m 	speed: 0.0174s/iter; left time: 7.4605s
Trial status: 1 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:22:28. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e4f57f3b with best_valid_loss=0.6944197995025058 and params={'alpha_d_ff': 4, 'batch_size': 64, 'd_model': 8, 'decomp_method': {'decomp_method': 'dft_decomp'}, 'down_sampling_method': 'avg', 'dropout': 0.0990448800935579, 'e_layers': 4, 'learning_rate': 0.001631158227501476}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-66aadf09   RUNNING           7            37.3507       0.291871       0.745259            0.709102 │
│ trial-e4f57f3b   TERMINATED        8            17.3934       0.353253       0.69442             0.69442  │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=24973)[0m 	iters: 200, epoch: 8 | loss: 0.3038021
[36m(_train_fn pid=24973)[0m 	speed: 0.0085s/iter; left time: 2.7873s
[36m(_train_fn pid=24973)[0m 	iters: 300, epoch: 8 | loss: 0.2455770
[36m(_train_fn pid=24973)[0m 	speed: 0.0080s/iter; left time: 1.8404s
[36m(_train_fn pid=24973)[0m 	iters: 400, epoch: 8 | loss: 0.3245448
[36m(_train_fn pid=24973)[0m 	speed: 0.0081s/iter; left time: 1.0489s
[36m(_train_fn pid=24973)[0m 	iters: 500, epoch: 8 | loss: 0.3422377
[36m(_train_fn pid=24973)[0m 	speed: 0.0082s/iter; left time: 0.2364s

Trial trial-66aadf09 completed after 8 iterations at 2024-08-23 05:22:32. Total running time: 1min 4s
╭───────────────────────────────────────────────────╮
│ Trial trial-66aadf09 result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000007 │
│ time_this_iter_s                          5.01726 │
│ time_total_s                             42.36799 │
│ training_iteration                              8 │
│ best_valid_loss                            0.7091 │
│ train_loss                                0.28872 │
│ valid_loss                                0.74747 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=24973)[0m Updating learning rate to 8.314554687789234e-05
[36m(_train_fn pid=24973)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=24973)[0m saving checkpoint...
[36m(_train_fn pid=24973)[0m Epoch: 8 cost time: 4.415288686752319
[36m(_train_fn pid=24973)[0m Epoch: 8, Steps: 528 | Train Loss: 0.2887193 Vali Loss: 0.7474691 Best vali loss: 0.7091024
[36m(_train_fn pid=24973)[0m Early stopping

Trial trial-b4e5e021 started with configuration:
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000000)
2024-08-23 05:22:44,188	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000001)
2024-08-23 05:22:48,203	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000002)
2024-08-23 05:22:52,234	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000003)
2024-08-23 05:22:56,260	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000004)
2024-08-23 05:23:00,293	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000005)
2024-08-23 05:23:04,331	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000006)
2024-08-23 05:23:08,379	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
╭────────────────────────────────────────────╮
│ Trial trial-b4e5e021 config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               3 │
│ batch_size                              64 │
│ d_model                                128 │
│ decomp_method/decomp_method     dft_decomp │
│ down_sampling_method                  conv │
│ dropout                            0.09252 │
│ e_layers                                 2 │
│ learning_rate                      0.00126 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=25658)[0m configuration
[36m(_train_fn pid=25658)[0m {'batch_size': 64, 'd_model': 128, 'decomp_method': 'dft_decomp', 'down_sampling_method': 'conv', 'dropout': 0.0925162833765015, 'e_layers': 2, 'learning_rate': 0.001259391180831956, 'd_ff': 384}
[36m(_train_fn pid=25658)[0m Use GPU: cuda:0
[36m(_train_fn pid=25658)[0m train 8449
[36m(_train_fn pid=25658)[0m val 2785
[36m(_train_fn pid=25658)[0m start_epoch 0
[36m(_train_fn pid=25658)[0m max_epoch 8
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 1 | loss: 0.6370572
[36m(_train_fn pid=25658)[0m 	speed: 0.0337s/iter; left time: 32.2903s
[36m(_train_fn pid=25658)[0m Updating learning rate to 0.001259391180831956
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Validation loss decreased (inf --> 1.1714).  Saving model state dict ...
[36m(_train_fn pid=25658)[0m Epoch: 1 cost time: 3.903332471847534
[36m(_train_fn pid=25658)[0m Epoch: 1, Steps: 132 | Train Loss: 0.6055743 Vali Loss: 1.1713877 Best vali loss: 1.1713877
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 2 | loss: 0.3862733
[36m(_train_fn pid=25658)[0m 	speed: 0.0400s/iter; left time: 32.9950s
[36m(_train_fn pid=25658)[0m Updating learning rate to 0.000629695590415978
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Validation loss decreased (1.1714 --> 0.7166).  Saving model state dict ...
[36m(_train_fn pid=25658)[0m Epoch: 2 cost time: 3.4493656158447266
[36m(_train_fn pid=25658)[0m Epoch: 2, Steps: 132 | Train Loss: 0.4021584 Vali Loss: 0.7165840 Best vali loss: 0.7165840
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 3 | loss: 0.3522307
[36m(_train_fn pid=25658)[0m 	speed: 0.0401s/iter; left time: 27.7825s
[36m(_train_fn pid=25658)[0m Updating learning rate to 0.000314847795207989
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Validation loss decreased (0.7166 --> 0.7009).  Saving model state dict ...
[36m(_train_fn pid=25658)[0m Epoch: 3 cost time: 3.4591684341430664
[36m(_train_fn pid=25658)[0m Epoch: 3, Steps: 132 | Train Loss: 0.3634642 Vali Loss: 0.7009469 Best vali loss: 0.7009469
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 4 | loss: 0.4186909
[36m(_train_fn pid=25658)[0m 	speed: 0.0403s/iter; left time: 22.5947s
[36m(_train_fn pid=25658)[0m Updating learning rate to 0.0001574238976039945
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Validation loss decreased (0.7009 --> 0.6988).  Saving model state dict ...
[36m(_train_fn pid=25658)[0m Epoch: 4 cost time: 3.4764552116394043
[36m(_train_fn pid=25658)[0m Epoch: 4, Steps: 132 | Train Loss: 0.3555694 Vali Loss: 0.6988080 Best vali loss: 0.6988080
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 5 | loss: 0.3382341
[36m(_train_fn pid=25658)[0m 	speed: 0.0403s/iter; left time: 17.2827s
[36m(_train_fn pid=25658)[0m Updating learning rate to 7.871194880199725e-05
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Validation loss decreased (0.6988 --> 0.6987).  Saving model state dict ...
[36m(_train_fn pid=25658)[0m Epoch: 5 cost time: 3.470827102661133
[36m(_train_fn pid=25658)[0m Epoch: 5, Steps: 132 | Train Loss: 0.3510278 Vali Loss: 0.6987347 Best vali loss: 0.6987347

Trial status: 2 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:22:58. Total running time: 1min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e4f57f3b with best_valid_loss=0.6944197995025058 and params={'alpha_d_ff': 4, 'batch_size': 64, 'd_model': 8, 'decomp_method': {'decomp_method': 'dft_decomp'}, 'down_sampling_method': 'avg', 'dropout': 0.0990448800935579, 'e_layers': 4, 'learning_rate': 0.001631158227501476}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-b4e5e021   RUNNING           5            20.9555       0.351028       0.698735            0.698735 │
│ trial-e4f57f3b   TERMINATED        8            17.3934       0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8            42.368        0.288719       0.747469            0.709102 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 6 | loss: 0.3294412
[36m(_train_fn pid=25658)[0m 	speed: 0.0402s/iter; left time: 11.9533s
[36m(_train_fn pid=25658)[0m Updating learning rate to 3.935597440099863e-05
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Validation loss decreased (0.6987 --> 0.6953).  Saving model state dict ...
[36m(_train_fn pid=25658)[0m Epoch: 6 cost time: 3.470508098602295
[36m(_train_fn pid=25658)[0m Epoch: 6, Steps: 132 | Train Loss: 0.3483928 Vali Loss: 0.6953467 Best vali loss: 0.6953467
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 7 | loss: 0.3485956
[36m(_train_fn pid=25658)[0m 	speed: 0.0405s/iter; left time: 6.6797s
[36m(_train_fn pid=25658)[0m Updating learning rate to 1.9677987200499314e-05
[36m(_train_fn pid=25658)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Epoch: 7 cost time: 3.4898221492767334
[36m(_train_fn pid=25658)[0m Epoch: 7, Steps: 132 | Train Loss: 0.3470726 Vali Loss: 0.6954597 Best vali loss: 0.6953467
[36m(_train_fn pid=25658)[0m 	iters: 100, epoch: 8 | loss: 0.3520051
[36m(_train_fn pid=25658)[0m 	speed: 0.0404s/iter; left time: 1.3328s

Trial trial-b4e5e021 completed after 8 iterations at 2024-08-23 05:23:08. Total running time: 1min 39s
[36m(_train_fn pid=25658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-b4e5e021_3_alpha_d_ff=3,batch_size=64,d_model=128,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0925,e_layers_2024-08-23_05-22-32/checkpoint_000007)
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000000)
2024-08-23 05:23:39,112	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000001)
2024-08-23 05:23:53,140	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
╭───────────────────────────────────────────────────╮
│ Trial trial-b4e5e021 result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000007 │
│ time_this_iter_s                          4.04611 │
│ time_total_s                              33.0653 │
│ training_iteration                              8 │
│ best_valid_loss                           0.69369 │
│ train_loss                                0.34641 │
│ valid_loss                                0.69369 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=25658)[0m Updating learning rate to 9.838993600249657e-06
[36m(_train_fn pid=25658)[0m saving checkpoint...
[36m(_train_fn pid=25658)[0m Validation loss decreased (0.6953 --> 0.6937).  Saving model state dict ...
[36m(_train_fn pid=25658)[0m Epoch: 8 cost time: 3.49344801902771
[36m(_train_fn pid=25658)[0m Epoch: 8, Steps: 132 | Train Loss: 0.3464122 Vali Loss: 0.6936917 Best vali loss: 0.6936917

Trial trial-c6399b0a started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-c6399b0a config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               3 │
│ batch_size                              16 │
│ d_model                                512 │
│ decomp_method/decomp_method     dft_decomp │
│ down_sampling_method                   avg │
│ dropout                            0.10005 │
│ e_layers                                 1 │
│ learning_rate                      0.00083 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=26337)[0m configuration
[36m(_train_fn pid=26337)[0m {'batch_size': 16, 'd_model': 512, 'decomp_method': 'dft_decomp', 'down_sampling_method': 'avg', 'dropout': 0.10005171002606002, 'e_layers': 1, 'learning_rate': 0.0008318929334924678, 'd_ff': 1536}
[36m(_train_fn pid=26337)[0m Use GPU: cuda:0
[36m(_train_fn pid=26337)[0m train 8449
[36m(_train_fn pid=26337)[0m val 2785
[36m(_train_fn pid=26337)[0m start_epoch 0
[36m(_train_fn pid=26337)[0m max_epoch 8
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 1 | loss: 0.4732232
[36m(_train_fn pid=26337)[0m 	speed: 0.0307s/iter; left time: 126.4983s
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 1 | loss: 0.5189321
[36m(_train_fn pid=26337)[0m 	speed: 0.0232s/iter; left time: 93.4159s
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 1 | loss: 0.4723047
[36m(_train_fn pid=26337)[0m 	speed: 0.0232s/iter; left time: 91.1794s
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 1 | loss: 0.4142588
[36m(_train_fn pid=26337)[0m 	speed: 0.0232s/iter; left time: 88.8552s
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 1 | loss: 0.4619200
[36m(_train_fn pid=26337)[0m 	speed: 0.0232s/iter; left time: 86.5821s
[36m(_train_fn pid=26337)[0m Updating learning rate to 0.0008318929334924678
[36m(_train_fn pid=26337)[0m saving checkpoint...
[36m(_train_fn pid=26337)[0m Validation loss decreased (inf --> 0.8466).  Saving model state dict ...
[36m(_train_fn pid=26337)[0m Epoch: 1 cost time: 12.712460279464722
[36m(_train_fn pid=26337)[0m Epoch: 1, Steps: 528 | Train Loss: 0.5114447 Vali Loss: 0.8465961 Best vali loss: 0.8465961
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 2 | loss: 0.4360324
[36m(_train_fn pid=26337)[0m 	speed: 0.0469s/iter; left time: 168.6140s

Trial status: 3 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:23:28. Total running time: 2min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: b4e5e021 with best_valid_loss=0.6936917429746583 and params={'alpha_d_ff': 3, 'batch_size': 64, 'd_model': 128, 'decomp_method': {'decomp_method': 'dft_decomp'}, 'down_sampling_method': 'conv', 'dropout': 0.0925162833765015, 'e_layers': 2, 'learning_rate': 0.001259391180831956}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-c6399b0a   RUNNING           1            14.823        0.511445       0.846596            0.846596 │
│ trial-e4f57f3b   TERMINATED        8            17.3934       0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8            42.368        0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8            33.0653       0.346412       0.693692            0.693692 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 2 | loss: 0.4025235
[36m(_train_fn pid=26337)[0m 	speed: 0.0232s/iter; left time: 81.2945s
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 2 | loss: 0.3737104
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 79.0351s
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 2 | loss: 0.4125703
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 76.6871s
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 2 | loss: 0.3558795
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 74.4132s
[36m(_train_fn pid=26337)[0m Updating learning rate to 0.0004159464667462339
[36m(_train_fn pid=26337)[0m saving checkpoint...
[36m(_train_fn pid=26337)[0m Validation loss decreased (0.8466 --> 0.7147).  Saving model state dict ...
[36m(_train_fn pid=26337)[0m Epoch: 2 cost time: 12.308334827423096
[36m(_train_fn pid=26337)[0m Epoch: 2, Steps: 528 | Train Loss: 0.4089476 Vali Loss: 0.7147011 Best vali loss: 0.7147011
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 3 | loss: 0.2650615
[36m(_train_fn pid=26337)[0m 	speed: 0.0471s/iter; left time: 144.4290s
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 3 | loss: 0.3303971
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 69.3111s
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 3 | loss: 0.3442867
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 66.8922s
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 3 | loss: 0.4105042
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 64.6436s
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 3 | loss: 0.2392066
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 62.2397s
[36m(_train_fn pid=26337)[0m Updating learning rate to 0.00020797323337311694
[36m(_train_fn pid=26337)[0m saving checkpoint...
[36m(_train_fn pid=26337)[0m Validation loss decreased (0.7147 --> 0.7081).  Saving model state dict ...
[36m(_train_fn pid=26337)[0m Epoch: 3 cost time: 12.349606275558472
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000002)
2024-08-23 05:24:07,163	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000003)
2024-08-23 05:24:21,205	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000004)
2024-08-23 05:24:35,215	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000005)
[36m(_train_fn pid=26337)[0m Epoch: 3, Steps: 528 | Train Loss: 0.3632436 Vali Loss: 0.7080612 Best vali loss: 0.7080612
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 4 | loss: 0.3807920
[36m(_train_fn pid=26337)[0m 	speed: 0.0468s/iter; left time: 119.0451s
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 4 | loss: 0.3359260
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 56.9075s
Trial status: 3 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:23:58. Total running time: 2min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: b4e5e021 with best_valid_loss=0.6936917429746583 and params={'alpha_d_ff': 3, 'batch_size': 64, 'd_model': 128, 'decomp_method': {'decomp_method': 'dft_decomp'}, 'down_sampling_method': 'conv', 'dropout': 0.0925162833765015, 'e_layers': 2, 'learning_rate': 0.001259391180831956}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-c6399b0a   RUNNING           3            42.8384       0.363244       0.708061            0.708061 │
│ trial-e4f57f3b   TERMINATED        8            17.3934       0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8            42.368        0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8            33.0653       0.346412       0.693692            0.693692 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 4 | loss: 0.4602890
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 54.5967s
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 4 | loss: 0.3283759
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 52.2922s
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 4 | loss: 0.3178524
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 50.0455s
[36m(_train_fn pid=26337)[0m Updating learning rate to 0.00010398661668655847
[36m(_train_fn pid=26337)[0m saving checkpoint...
[36m(_train_fn pid=26337)[0m Validation loss decreased (0.7081 --> 0.6975).  Saving model state dict ...
[36m(_train_fn pid=26337)[0m Epoch: 4 cost time: 12.345232486724854
[36m(_train_fn pid=26337)[0m Epoch: 4, Steps: 528 | Train Loss: 0.3574211 Vali Loss: 0.6975005 Best vali loss: 0.6975005
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 5 | loss: 0.2743003
[36m(_train_fn pid=26337)[0m 	speed: 0.0469s/iter; left time: 94.3959s
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 5 | loss: 0.2941247
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 44.6598s
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 5 | loss: 0.5382348
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 42.3620s
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 5 | loss: 0.2974628
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 40.0444s
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 5 | loss: 0.2972210
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 37.6982s
[36m(_train_fn pid=26337)[0m Updating learning rate to 5.1993308343279236e-05
[36m(_train_fn pid=26337)[0m saving checkpoint...
[36m(_train_fn pid=26337)[0m Validation loss decreased (0.6975 --> 0.6922).  Saving model state dict ...
[36m(_train_fn pid=26337)[0m Epoch: 5 cost time: 12.358354091644287
[36m(_train_fn pid=26337)[0m Epoch: 5, Steps: 528 | Train Loss: 0.3543097 Vali Loss: 0.6921945 Best vali loss: 0.6921945
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 6 | loss: 0.2957139
[36m(_train_fn pid=26337)[0m 	speed: 0.0470s/iter; left time: 69.7381s
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 6 | loss: 0.3624782
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 32.3257s
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 6 | loss: 0.2750915
[36m(_train_fn pid=26337)[0m 	speed: 0.0233s/iter; left time: 29.9934s
Trial status: 3 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:24:28. Total running time: 3min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c6399b0a with best_valid_loss=0.6921944594246218 and params={'alpha_d_ff': 3, 'batch_size': 16, 'd_model': 512, 'decomp_method': {'decomp_method': 'dft_decomp'}, 'down_sampling_method': 'avg', 'dropout': 0.10005171002606002, 'e_layers': 1, 'learning_rate': 0.0008318929334924678}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-c6399b0a   RUNNING           5            70.8931       0.35431        0.692194            0.692194 │
│ trial-e4f57f3b   TERMINATED        8            17.3934       0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8            42.368        0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8            33.0653       0.346412       0.693692            0.693692 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 6 | loss: 0.4129314
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 27.6833s
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 6 | loss: 0.2879390
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 25.3359s
[36m(_train_fn pid=26337)[0m Updating learning rate to 2.5996654171639618e-05
[36m(_train_fn pid=26337)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=26337)[0m saving checkpoint...
[36m(_train_fn pid=26337)[0m Epoch: 6 cost time: 12.35193943977356
[36m(_train_fn pid=26337)[0m Epoch: 6, Steps: 528 | Train Loss: 0.3523454 Vali Loss: 0.6935804 Best vali loss: 0.6921945
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 7 | loss: 0.3229888
[36m(_train_fn pid=26337)[0m 	speed: 0.0467s/iter; left time: 44.7366s
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 7 | loss: 0.3970594
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 20.0668s
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 7 | loss: 0.3639104
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 17.7170s
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 7 | loss: 0.3190286
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 15.3821s
2024-08-23 05:24:49,281	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000006)
2024-08-23 05:25:03,337	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=26337)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-c6399b0a_4_alpha_d_ff=3,batch_size=16,d_model=512,decomp_method=dft_decomp,down_sampling_method=avg,dropout=0.1001,e_layers=_2024-08-23_05-23-08/checkpoint_000007)
2024-08-23 05:25:07,917	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:25:09,805	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27101)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-654a3d52_5_alpha_d_ff=2,batch_size=64,d_model=16,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=0._2024-08-23_05-25-03/checkpoint_000001)[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
2024-08-23 05:25:11,523	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:25:13,249	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 7 | loss: 0.2581976
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 13.0425s
[36m(_train_fn pid=26337)[0m Updating learning rate to 1.2998327085819809e-05
[36m(_train_fn pid=26337)[0m saving checkpoint...
[36m(_train_fn pid=26337)[0m Validation loss decreased (0.6922 --> 0.6912).  Saving model state dict ...
[36m(_train_fn pid=26337)[0m Epoch: 7 cost time: 12.384982109069824
[36m(_train_fn pid=26337)[0m Epoch: 7, Steps: 528 | Train Loss: 0.3513557 Vali Loss: 0.6911591 Best vali loss: 0.6911591
[36m(_train_fn pid=26337)[0m 	iters: 100, epoch: 8 | loss: 0.3116591
[36m(_train_fn pid=26337)[0m 	speed: 0.0470s/iter; left time: 20.1686s
[36m(_train_fn pid=26337)[0m 	iters: 200, epoch: 8 | loss: 0.4424406
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 7.7084s
[36m(_train_fn pid=26337)[0m 	iters: 300, epoch: 8 | loss: 0.2727496
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 5.3658s
[36m(_train_fn pid=26337)[0m 	iters: 400, epoch: 8 | loss: 0.3932614
[36m(_train_fn pid=26337)[0m 	speed: 0.0235s/iter; left time: 3.0294s
Trial status: 3 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:24:58. Total running time: 3min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c6399b0a with best_valid_loss=0.6911591098311989 and params={'alpha_d_ff': 3, 'batch_size': 16, 'd_model': 512, 'decomp_method': {'decomp_method': 'dft_decomp'}, 'down_sampling_method': 'avg', 'dropout': 0.10005171002606002, 'e_layers': 1, 'learning_rate': 0.0008318929334924678}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-c6399b0a   RUNNING           7            98.9611       0.351356       0.691159            0.691159 │
│ trial-e4f57f3b   TERMINATED        8            17.3934       0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8            42.368        0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8            33.0653       0.346412       0.693692            0.693692 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=26337)[0m 	iters: 500, epoch: 8 | loss: 0.2919381
[36m(_train_fn pid=26337)[0m 	speed: 0.0234s/iter; left time: 0.6797s

Trial trial-c6399b0a completed after 8 iterations at 2024-08-23 05:25:03. Total running time: 3min 34s
╭───────────────────────────────────────────────────╮
│ Trial trial-c6399b0a result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000007 │
│ time_this_iter_s                         14.05093 │
│ time_total_s                            113.01203 │
│ training_iteration                              8 │
│ best_valid_loss                           0.69116 │
│ train_loss                                0.35081 │
│ valid_loss                                0.69207 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=26337)[0m Updating learning rate to 6.4991635429099045e-06
[36m(_train_fn pid=26337)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=26337)[0m saving checkpoint...

Trial trial-654a3d52 started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-654a3d52 config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               2 │
│ batch_size                              64 │
│ d_model                                 16 │
│ decomp_method/decomp_method     moving_avg │
│ decomp_method/moving_avg                15 │
│ down_sampling_method                  conv │
│ dropout                            0.07432 │
│ e_layers                                 3 │
│ learning_rate                      0.00673 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=27101)[0m configuration
[36m(_train_fn pid=27101)[0m {'batch_size': 64, 'd_model': 16, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'down_sampling_method': 'conv', 'dropout': 0.07431613933182805, 'e_layers': 3, 'learning_rate': 0.006730655699341551, 'd_ff': 32}
[36m(_train_fn pid=27101)[0m Use GPU: cuda:0
[36m(_train_fn pid=27101)[0m train 8449
[36m(_train_fn pid=27101)[0m val 2785
[36m(_train_fn pid=27101)[0m start_epoch 0
[36m(_train_fn pid=27101)[0m max_epoch 8
[36m(_train_fn pid=27101)[0m 	iters: 100, epoch: 1 | loss: 0.4946643
[36m(_train_fn pid=27101)[0m 	speed: 0.0183s/iter; left time: 17.4743s
[36m(_train_fn pid=27101)[0m Validation loss decreased (inf --> 0.8772).  Saving model state dict ...
[36m(_train_fn pid=27101)[0m Epoch: 1 cost time: 1.920018196105957
[36m(_train_fn pid=27101)[0m Epoch: 1, Steps: 132 | Train Loss: 0.5608039 Vali Loss: 0.8771929 Best vali loss: 0.8771929
[36m(_train_fn pid=27101)[0m 	iters: 100, epoch: 2 | loss: 0.3183860
[36m(_train_fn pid=27101)[0m 	speed: 0.0190s/iter; left time: 15.6583s
[36m(_train_fn pid=27101)[0m Updating learning rate to 0.006730655699341551
[36m(_train_fn pid=27101)[0m saving checkpoint...
[36m(_train_fn pid=27101)[0m Updating learning rate to 0.0033653278496707756
[36m(_train_fn pid=27101)[0m saving checkpoint...
[36m(_train_fn pid=27101)[0m Validation loss decreased (0.8772 --> 0.7081).  Saving model state dict ...
[36m(_train_fn pid=27101)[0m Epoch: 2 cost time: 1.6653950214385986
[36m(_train_fn pid=27101)[0m Epoch: 2, Steps: 132 | Train Loss: 0.3762383 Vali Loss: 0.7081194 Best vali loss: 0.7081194
[36m(_train_fn pid=27101)[0m 	iters: 100, epoch: 3 | loss: 0.3528064
[36m(_train_fn pid=27101)[0m 	speed: 0.0177s/iter; left time: 12.2456s
[36m(_train_fn pid=27101)[0m Updating learning rate to 0.0016826639248353878
[36m(_train_fn pid=27101)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=27101)[0m saving checkpoint...
[36m(_train_fn pid=27101)[0m Epoch: 3 cost time: 1.5043158531188965
[36m(_train_fn pid=27101)[0m Epoch: 3, Steps: 132 | Train Loss: 0.3435614 Vali Loss: 0.7135988 Best vali loss: 0.7081194
[36m(_train_fn pid=27101)[0m 	iters: 100, epoch: 4 | loss: 0.3286270
[36m(_train_fn pid=27101)[0m 	speed: 0.0173s/iter; left time: 9.6912s
[36m(_train_fn pid=27101)[0m Updating learning rate to 0.0008413319624176939
[36m(_train_fn pid=27101)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=27101)[0m saving checkpoint...
[36m(_train_fn pid=27101)[0m Epoch: 4 cost time: 1.5115923881530762
[36m(_train_fn pid=27101)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-654a3d52_5_alpha_d_ff=2,batch_size=64,d_model=16,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=0._2024-08-23_05-25-03/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-23 05:25:14,991	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27548)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-718a218a_6_alpha_d_ff=3,batch_size=32,d_model=64,decomp_method=moving_avg,moving_avg=35,down_sampling_method=avg,dropout=0.0_2024-08-23_05-25-14/checkpoint_000000)
2024-08-23 05:25:25,056	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27548)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-718a218a_6_alpha_d_ff=3,batch_size=32,d_model=64,decomp_method=moving_avg,moving_avg=35,down_sampling_method=avg,dropout=0.0_2024-08-23_05-25-14/checkpoint_000001)
2024-08-23 05:25:28,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27548)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-718a218a_6_alpha_d_ff=3,batch_size=32,d_model=64,decomp_method=moving_avg,moving_avg=35,down_sampling_method=avg,dropout=0.0_2024-08-23_05-25-14/checkpoint_000002)
2024-08-23 05:25:31,916	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27548)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-718a218a_6_alpha_d_ff=3,batch_size=32,d_model=64,decomp_method=moving_avg,moving_avg=35,down_sampling_method=avg,dropout=0.0_2024-08-23_05-25-14/checkpoint_000003)
2024-08-23 05:25:35,545	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27548)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-718a218a_6_alpha_d_ff=3,batch_size=32,d_model=64,decomp_method=moving_avg,moving_avg=35,down_sampling_method=avg,dropout=0.0_2024-08-23_05-25-14/checkpoint_000004)
[36m(_train_fn pid=27101)[0m Epoch: 4, Steps: 132 | Train Loss: 0.3306072 Vali Loss: 0.7380131 Best vali loss: 0.7081194
[36m(_train_fn pid=27101)[0m 	iters: 100, epoch: 5 | loss: 0.3402910
[36m(_train_fn pid=27101)[0m 	speed: 0.0172s/iter; left time: 7.3892s
[36m(_train_fn pid=27101)[0m Updating learning rate to 0.00042066598120884695
[36m(_train_fn pid=27101)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=27101)[0m saving checkpoint...
[36m(_train_fn pid=27101)[0m Epoch: 5 cost time: 1.516296148300171
[36m(_train_fn pid=27101)[0m Epoch: 5, Steps: 132 | Train Loss: 0.3207083 Vali Loss: 0.7203683 Best vali loss: 0.7081194
[36m(_train_fn pid=27101)[0m Early stopping

Trial trial-654a3d52 completed after 5 iterations at 2024-08-23 05:25:14. Total running time: 3min 46s
╭───────────────────────────────────────────────────╮
│ Trial trial-654a3d52 result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000004 │
│ time_this_iter_s                          1.73965 │
│ time_total_s                              9.64573 │
│ training_iteration                              5 │
│ best_valid_loss                           0.70812 │
│ train_loss                                0.32071 │
│ valid_loss                                0.72037 │
╰───────────────────────────────────────────────────╯

Trial trial-718a218a started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-718a218a config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               3 │
│ batch_size                              32 │
│ d_model                                 64 │
│ decomp_method/decomp_method     moving_avg │
│ decomp_method/moving_avg                35 │
│ down_sampling_method                   avg │
│ dropout                            0.07684 │
│ e_layers                                 3 │
│ learning_rate                      0.00117 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=27548)[0m configuration
[36m(_train_fn pid=27548)[0m {'batch_size': 32, 'd_model': 64, 'decomp_method': 'moving_avg', 'moving_avg': 35, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074, 'd_ff': 192}
[36m(_train_fn pid=27548)[0m Use GPU: cuda:0
[36m(_train_fn pid=27548)[0m train 8449
[36m(_train_fn pid=27548)[0m val 2785
[36m(_train_fn pid=27548)[0m start_epoch 0
[36m(_train_fn pid=27548)[0m max_epoch 8
[36m(_train_fn pid=27548)[0m 	iters: 200, epoch: 1 | loss: 0.6671316[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=27548)[0m 	speed: 0.0119s/iter; left time: 22.6989s[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=27548)[0m Updating learning rate to 0.0011650014804827074
[36m(_train_fn pid=27548)[0m saving checkpoint...
[36m(_train_fn pid=27548)[0m Validation loss decreased (inf --> 0.9941).  Saving model state dict ...
[36m(_train_fn pid=27548)[0m Epoch: 1 cost time: 3.4091928005218506
[36m(_train_fn pid=27548)[0m Epoch: 1, Steps: 264 | Train Loss: 0.5800912 Vali Loss: 0.9941272 Best vali loss: 0.9941272
[36m(_train_fn pid=27548)[0m Updating learning rate to 0.0005825007402413537
[36m(_train_fn pid=27548)[0m saving checkpoint...
[36m(_train_fn pid=27548)[0m Validation loss decreased (0.9941 --> 0.7089).  Saving model state dict ...
[36m(_train_fn pid=27548)[0m Epoch: 2 cost time: 2.9904520511627197
[36m(_train_fn pid=27548)[0m Epoch: 2, Steps: 264 | Train Loss: 0.3785497 Vali Loss: 0.7089093 Best vali loss: 0.7089093
[36m(_train_fn pid=27548)[0m 	iters: 100, epoch: 3 | loss: 0.3533227[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=27548)[0m 	speed: 0.0230s/iter; left time: 34.1654s[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=27548)[0m Updating learning rate to 0.00029125037012067684
[36m(_train_fn pid=27548)[0m saving checkpoint...
[36m(_train_fn pid=27548)[0m Validation loss decreased (0.7089 --> 0.6876).  Saving model state dict ...
[36m(_train_fn pid=27548)[0m Epoch: 3 cost time: 2.96903657913208
[36m(_train_fn pid=27548)[0m Epoch: 3, Steps: 264 | Train Loss: 0.3532748 Vali Loss: 0.6876169 Best vali loss: 0.6876169

Trial status: 5 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:25:29. Total running time: 4min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 718a218a with best_valid_loss=0.6876169071457852 and params={'alpha_d_ff': 3, 'batch_size': 32, 'd_model': 64, 'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-718a218a   RUNNING           3           11.1315        0.353275       0.687617            0.687617 │
│ trial-e4f57f3b   TERMINATED        8           17.3934        0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8           42.368         0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8           33.0653        0.346412       0.693692            0.693692 │
│ trial-c6399b0a   TERMINATED        8          113.012         0.350811       0.692065            0.691159 │
│ trial-654a3d52   TERMINATED        5            9.64573       0.320708       0.720368            0.708119 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=27548)[0m Updating learning rate to 0.00014562518506033842
[36m(_train_fn pid=27548)[0m saving checkpoint...
[36m(_train_fn pid=27548)[0m Validation loss decreased (0.6876 --> 0.6834).  Saving model state dict ...
[36m(_train_fn pid=27548)[0m Epoch: 4 cost time: 3.017209053039551
[36m(_train_fn pid=27548)[0m Epoch: 4, Steps: 264 | Train Loss: 0.3468847 Vali Loss: 0.6834154 Best vali loss: 0.6834154
[36m(_train_fn pid=27548)[0m 	iters: 200, epoch: 4 | loss: 0.4719048[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=27548)[0m 	speed: 0.0111s/iter; left time: 12.4542s[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=27548)[0m Updating learning rate to 7.281259253016921e-05
[36m(_train_fn pid=27548)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=27548)[0m saving checkpoint...
[36m(_train_fn pid=27548)[0m Epoch: 5 cost time: 3.201714277267456
2024-08-23 05:25:39,160	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27548)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-718a218a_6_alpha_d_ff=3,batch_size=32,d_model=64,decomp_method=moving_avg,moving_avg=35,down_sampling_method=avg,dropout=0.0_2024-08-23_05-25-14/checkpoint_000005)
2024-08-23 05:25:42,566	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=27548)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-718a218a_6_alpha_d_ff=3,batch_size=32,d_model=64,decomp_method=moving_avg,moving_avg=35,down_sampling_method=avg,dropout=0.0_2024-08-23_05-25-14/checkpoint_000006)
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000000)
2024-08-23 05:25:47,270	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:25:48,257	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000001)
2024-08-23 05:25:49,256	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000002)
2024-08-23 05:25:50,234	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000003)
2024-08-23 05:25:51,175	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000004)
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000005)
2024-08-23 05:25:52,130	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:25:53,122	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000006)
2024-08-23 05:25:54,113	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5bba4959_7_alpha_d_ff=4,batch_size=128,d_model=16,decomp_method=dft_decomp,down_sampling_method=conv,dropout=0.0754,e_layers_2024-08-23_05-25-42/checkpoint_000007)
[36m(_train_fn pid=27548)[0m Epoch: 5, Steps: 264 | Train Loss: 0.3433198 Vali Loss: 0.6887759 Best vali loss: 0.6834154
[36m(_train_fn pid=27548)[0m 	iters: 200, epoch: 6 | loss: 0.2816381[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=27548)[0m 	speed: 0.0119s/iter; left time: 7.0600s[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=27548)[0m Updating learning rate to 3.6406296265084605e-05
[36m(_train_fn pid=27548)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=27548)[0m saving checkpoint...
[36m(_train_fn pid=27548)[0m Epoch: 6 cost time: 3.1892075538635254
[36m(_train_fn pid=27548)[0m Epoch: 6, Steps: 264 | Train Loss: 0.3409856 Vali Loss: 0.6859739 Best vali loss: 0.6834154

Trial trial-718a218a completed after 7 iterations at 2024-08-23 05:25:42. Total running time: 4min 14s
╭───────────────────────────────────────────────────╮
│ Trial trial-718a218a result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000006 │
│ time_this_iter_s                          3.40304 │
│ time_total_s                             25.22256 │
│ training_iteration                              7 │
│ best_valid_loss                           0.68342 │
│ train_loss                                0.34004 │
│ valid_loss                                 0.6854 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=27548)[0m Updating learning rate to 1.8203148132542303e-05
[36m(_train_fn pid=27548)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=27548)[0m saving checkpoint...
[36m(_train_fn pid=27548)[0m Epoch: 7 cost time: 2.9757003784179688
[36m(_train_fn pid=27548)[0m Epoch: 7, Steps: 264 | Train Loss: 0.3400393 Vali Loss: 0.6853969 Best vali loss: 0.6834154
[36m(_train_fn pid=27548)[0m Early stopping

Trial trial-5bba4959 started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-5bba4959 config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               4 │
│ batch_size                             128 │
│ d_model                                 16 │
│ decomp_method/decomp_method     dft_decomp │
│ down_sampling_method                  conv │
│ dropout                            0.07536 │
│ e_layers                                 2 │
│ learning_rate                      0.00251 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=28158)[0m configuration
[36m(_train_fn pid=28158)[0m {'batch_size': 128, 'd_model': 16, 'decomp_method': 'dft_decomp', 'down_sampling_method': 'conv', 'dropout': 0.0753590163357007, 'e_layers': 2, 'learning_rate': 0.002511404254926715, 'd_ff': 64}
[36m(_train_fn pid=28158)[0m Use GPU: cuda:0
[36m(_train_fn pid=27548)[0m 	iters: 200, epoch: 7 | loss: 0.4242671[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=27548)[0m 	speed: 0.0111s/iter; left time: 3.6420s[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=28158)[0m train 8449
[36m(_train_fn pid=28158)[0m val 2785
[36m(_train_fn pid=28158)[0m start_epoch 0
[36m(_train_fn pid=28158)[0m max_epoch 8
[36m(_train_fn pid=28158)[0m Validation loss decreased (inf --> 1.9891).  Saving model state dict ...
[36m(_train_fn pid=28158)[0m Validation loss decreased (1.9891 --> 0.7923).  Saving model state dict ...
[36m(_train_fn pid=28158)[0m Updating learning rate to 0.0012557021274633574[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=28158)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=28158)[0m Epoch: 2 cost time: 0.8045637607574463[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=28158)[0m Epoch: 2, Steps: 66 | Train Loss: 0.5360563 Vali Loss: 0.7923441 Best vali loss: 0.7923441[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=28158)[0m Validation loss decreased (0.7923 --> 0.7537).  Saving model state dict ...
[36m(_train_fn pid=28158)[0m Validation loss decreased (0.7537 --> 0.7400).  Saving model state dict ...
[36m(_train_fn pid=28158)[0m Validation loss decreased (0.7400 --> 0.7362).  Saving model state dict ...
[36m(_train_fn pid=28158)[0m Validation loss decreased (0.7362 --> 0.7337).  Saving model state dict ...
[36m(_train_fn pid=28158)[0m Validation loss decreased (0.7337 --> 0.7333).  Saving model state dict ...

Trial trial-5bba4959 completed after 8 iterations at 2024-08-23 05:25:54. Total running time: 4min 25s
╭───────────────────────────────────────────────────╮
│ Trial trial-5bba4959 result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000007 │
│ time_this_iter_s                          0.98829 │
│ time_total_s                              8.77779 │
│ training_iteration                              8 │
│ best_valid_loss                           0.73249 │
│ train_loss                                0.37242 │
│ valid_loss                                0.73249 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=28158)[0m Validation loss decreased (0.7333 --> 0.7325).  Saving model state dict ...
[36m(_train_fn pid=28158)[0m Updating learning rate to 1.962034574161496e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=28158)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=28158)[0m Epoch: 8 cost time: 0.8074593544006348[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=28158)[0m Epoch: 8, Steps: 66 | Train Loss: 0.3724199 Vali Loss: 0.7324914 Best vali loss: 0.7324914[32m [repeated 6x across cluster][0m

Trial trial-006275bf started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-006275bf config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               4 │
│ batch_size                              64 │
│ d_model                                  8 │
│ decomp_method/decomp_method     moving_avg │
│ decomp_method/moving_avg                55 │
│ down_sampling_method                   avg │
│ dropout                            0.08181 │
│ e_layers                                 3 │
│ learning_rate                      0.01005 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=28816)[0m configuration
[36m(_train_fn pid=28816)[0m {'batch_size': 64, 'd_model': 8, 'decomp_method': 'moving_avg', 'moving_avg': 55, 'down_sampling_method': 'avg', 'dropout': 0.08180975226279208, 'e_layers': 3, 'learning_rate': 0.010052790919426012, 'd_ff': 32}
[36m(_train_fn pid=28816)[0m Use GPU: cuda:0
[36m(_train_fn pid=28816)[0m train 8449
[36m(_train_fn pid=28816)[0m val 2785
[36m(_train_fn pid=28816)[0m start_epoch 0
[36m(_train_fn pid=28816)[0m max_epoch 8
2024-08-23 05:25:58,859	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:26:00,663	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28816)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-006275bf_8_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=moving_avg,moving_avg=55,down_sampling_method=avg,dropout=0.08_2024-08-23_05-25-54/checkpoint_000001)[32m [repeated 2x across cluster][0m
2024-08-23 05:26:02,344	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:26:03,980	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:26:05,717	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28816)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-006275bf_8_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=moving_avg,moving_avg=55,down_sampling_method=avg,dropout=0.08_2024-08-23_05-25-54/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-23 05:26:07,544	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:26:09,361	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:26:11,016	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=28816)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-006275bf_8_alpha_d_ff=4,batch_size=64,d_model=8,decomp_method=moving_avg,moving_avg=55,down_sampling_method=avg,dropout=0.08_2024-08-23_05-25-54/checkpoint_000007)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 1 | loss: 0.4144189
[36m(_train_fn pid=28816)[0m 	speed: 0.0178s/iter; left time: 17.0115s
[36m(_train_fn pid=28816)[0m Validation loss decreased (inf --> 0.7729).  Saving model state dict ...

Trial status: 7 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:25:59. Total running time: 4min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 718a218a with best_valid_loss=0.6834153614338787 and params={'alpha_d_ff': 3, 'batch_size': 32, 'd_model': 64, 'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-006275bf   RUNNING           1            2.5307        0.595032       0.772929            0.772929 │
│ trial-e4f57f3b   TERMINATED        8           17.3934        0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8           42.368         0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8           33.0653        0.346412       0.693692            0.693692 │
│ trial-c6399b0a   TERMINATED        8          113.012         0.350811       0.692065            0.691159 │
│ trial-654a3d52   TERMINATED        5            9.64573       0.320708       0.720368            0.708119 │
│ trial-718a218a   TERMINATED        7           25.2226        0.340039       0.685397            0.683415 │
│ trial-5bba4959   TERMINATED        8            8.77779       0.37242        0.732491            0.732491 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 2 | loss: 0.3843991
[36m(_train_fn pid=28816)[0m 	speed: 0.0184s/iter; left time: 15.1850s
[36m(_train_fn pid=28816)[0m Updating learning rate to 0.010052790919426012
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Epoch: 1 cost time: 1.84637451171875
[36m(_train_fn pid=28816)[0m Epoch: 1, Steps: 132 | Train Loss: 0.5950325 Vali Loss: 0.7729292 Best vali loss: 0.7729292
[36m(_train_fn pid=28816)[0m Updating learning rate to 0.005026395459713006
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Validation loss decreased (0.7729 --> 0.6950).  Saving model state dict ...
[36m(_train_fn pid=28816)[0m Epoch: 2 cost time: 1.58119535446167
[36m(_train_fn pid=28816)[0m Epoch: 2, Steps: 132 | Train Loss: 0.3772474 Vali Loss: 0.6949706 Best vali loss: 0.6949706
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 3 | loss: 0.3740329
[36m(_train_fn pid=28816)[0m 	speed: 0.0170s/iter; left time: 11.7493s
[36m(_train_fn pid=28816)[0m Updating learning rate to 0.002513197729856503
[36m(_train_fn pid=28816)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Epoch: 3 cost time: 1.4645779132843018
[36m(_train_fn pid=28816)[0m Epoch: 3, Steps: 132 | Train Loss: 0.3472134 Vali Loss: 0.7059370 Best vali loss: 0.6949706
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 4 | loss: 0.3148952
[36m(_train_fn pid=28816)[0m 	speed: 0.0165s/iter; left time: 9.2394s
[36m(_train_fn pid=28816)[0m Updating learning rate to 0.0012565988649282515
[36m(_train_fn pid=28816)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Epoch: 4 cost time: 1.420762538909912
[36m(_train_fn pid=28816)[0m Epoch: 4, Steps: 132 | Train Loss: 0.3354204 Vali Loss: 0.6987151 Best vali loss: 0.6949706
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 5 | loss: 0.3280377
[36m(_train_fn pid=28816)[0m 	speed: 0.0168s/iter; left time: 7.2083s
[36m(_train_fn pid=28816)[0m Updating learning rate to 0.0006282994324641257
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Validation loss decreased (0.6950 --> 0.6933).  Saving model state dict ...
[36m(_train_fn pid=28816)[0m Epoch: 5 cost time: 1.514389991760254
[36m(_train_fn pid=28816)[0m Epoch: 5, Steps: 132 | Train Loss: 0.3274652 Vali Loss: 0.6932861 Best vali loss: 0.6932861
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 6 | loss: 0.3247622
[36m(_train_fn pid=28816)[0m 	speed: 0.0183s/iter; left time: 5.4471s
[36m(_train_fn pid=28816)[0m Updating learning rate to 0.00031414971623206287
[36m(_train_fn pid=28816)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Epoch: 6 cost time: 1.6075818538665771
[36m(_train_fn pid=28816)[0m Epoch: 6, Steps: 132 | Train Loss: 0.3218097 Vali Loss: 0.6979974 Best vali loss: 0.6932861
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 7 | loss: 0.3140236
[36m(_train_fn pid=28816)[0m 	speed: 0.0183s/iter; left time: 3.0119s
[36m(_train_fn pid=28816)[0m Updating learning rate to 0.00015707485811603143
[36m(_train_fn pid=28816)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Epoch: 7 cost time: 1.6044833660125732
[36m(_train_fn pid=28816)[0m Epoch: 7, Steps: 132 | Train Loss: 0.3183838 Vali Loss: 0.6971052 Best vali loss: 0.6932861
[36m(_train_fn pid=28816)[0m 	iters: 100, epoch: 8 | loss: 0.3832439
[36m(_train_fn pid=28816)[0m 	speed: 0.0169s/iter; left time: 0.5590s

Trial trial-006275bf completed after 8 iterations at 2024-08-23 05:26:11. Total running time: 4min 42s
╭───────────────────────────────────────────────────╮
│ Trial trial-006275bf result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000007 │
│ time_this_iter_s                          1.65188 │
│ time_total_s                             14.66807 │
│ training_iteration                              8 │
│ best_valid_loss                           0.69329 │
│ train_loss                                0.31666 │
│ valid_loss                                0.69583 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=28816)[0m Updating learning rate to 7.853742905801572e-05
[36m(_train_fn pid=28816)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=28816)[0m saving checkpoint...
[36m(_train_fn pid=28816)[0m Epoch: 8 cost time: 1.440575361251831
[36m(_train_fn pid=28816)[0m Epoch: 8, Steps: 132 | Train Loss: 0.3166603 Vali Loss: 0.6958329 Best vali loss: 0.6932861
[36m(_train_fn pid=28816)[0m Early stopping

Trial trial-f649905d started with configuration:
[36m(_train_fn pid=29495)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-f649905d_9_alpha_d_ff=4,batch_size=16,d_model=512,decomp_method=moving_avg,moving_avg=35,down_sampling_method=conv,dropout=0_2024-08-23_05-26-11/checkpoint_000000)
[36m(_train_fn pid=29495)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-f649905d_9_alpha_d_ff=4,batch_size=16,d_model=512,decomp_method=moving_avg,moving_avg=35,down_sampling_method=conv,dropout=0_2024-08-23_05-26-11/checkpoint_000001)
╭────────────────────────────────────────────╮
│ Trial trial-f649905d config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               4 │
│ batch_size                              16 │
│ d_model                                512 │
│ decomp_method/decomp_method     moving_avg │
│ decomp_method/moving_avg                35 │
│ down_sampling_method                  conv │
│ dropout                            0.05999 │
│ e_layers                                 1 │
│ learning_rate                      0.00607 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=29495)[0m configuration
[36m(_train_fn pid=29495)[0m {'batch_size': 16, 'd_model': 512, 'decomp_method': 'moving_avg', 'moving_avg': 35, 'down_sampling_method': 'conv', 'dropout': 0.05999454279983519, 'e_layers': 1, 'learning_rate': 0.006070584790668805, 'd_ff': 2048}
[36m(_train_fn pid=29495)[0m Use GPU: cuda:0
[36m(_train_fn pid=29495)[0m train 8449
[36m(_train_fn pid=29495)[0m val 2785
[36m(_train_fn pid=29495)[0m start_epoch 0
[36m(_train_fn pid=29495)[0m max_epoch 8
[36m(_train_fn pid=29495)[0m 	iters: 100, epoch: 1 | loss: 0.4444007
[36m(_train_fn pid=29495)[0m 	speed: 0.0338s/iter; left time: 139.5511s
[36m(_train_fn pid=29495)[0m 	iters: 200, epoch: 1 | loss: 0.5032885
[36m(_train_fn pid=29495)[0m 	speed: 0.0283s/iter; left time: 113.8838s
[36m(_train_fn pid=29495)[0m 	iters: 300, epoch: 1 | loss: 0.3375348
[36m(_train_fn pid=29495)[0m 	speed: 0.0283s/iter; left time: 111.0878s
[36m(_train_fn pid=29495)[0m 	iters: 400, epoch: 1 | loss: 0.4395502
[36m(_train_fn pid=29495)[0m 	speed: 0.0283s/iter; left time: 108.3597s
[36m(_train_fn pid=29495)[0m 	iters: 500, epoch: 1 | loss: 0.3075776
[36m(_train_fn pid=29495)[0m 	speed: 0.0284s/iter; left time: 105.8855s

Trial status: 8 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:26:29. Total running time: 5min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 718a218a with best_valid_loss=0.6834153614338787 and params={'alpha_d_ff': 3, 'batch_size': 32, 'd_model': 64, 'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-f649905d   RUNNING                                                                                  │
│ trial-e4f57f3b   TERMINATED        8           17.3934        0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8           42.368         0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8           33.0653        0.346412       0.693692            0.693692 │
│ trial-c6399b0a   TERMINATED        8          113.012         0.350811       0.692065            0.691159 │
│ trial-654a3d52   TERMINATED        5            9.64573       0.320708       0.720368            0.708119 │
│ trial-718a218a   TERMINATED        7           25.2226        0.340039       0.685397            0.683415 │
│ trial-5bba4959   TERMINATED        8            8.77779       0.37242        0.732491            0.732491 │
│ trial-006275bf   TERMINATED        8           14.6681        0.31666        0.695833            0.693286 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=29495)[0m Updating learning rate to 0.006070584790668805
[36m(_train_fn pid=29495)[0m saving checkpoint...
[36m(_train_fn pid=29495)[0m Validation loss decreased (inf --> 0.7831).  Saving model state dict ...
[36m(_train_fn pid=29495)[0m Epoch: 1 cost time: 15.209660291671753
[36m(_train_fn pid=29495)[0m Epoch: 1, Steps: 528 | Train Loss: 0.4505775 Vali Loss: 0.7831347 Best vali loss: 0.7831347
[36m(_train_fn pid=29495)[0m 	iters: 100, epoch: 2 | loss: 0.7201061
[36m(_train_fn pid=29495)[0m 	speed: 0.0549s/iter; left time: 197.4655s
[36m(_train_fn pid=29495)[0m 	iters: 200, epoch: 2 | loss: 0.5747356
[36m(_train_fn pid=29495)[0m 	speed: 0.0272s/iter; left time: 95.0213s
[36m(_train_fn pid=29495)[0m 	iters: 300, epoch: 2 | loss: 0.6451284
[36m(_train_fn pid=29495)[0m 	speed: 0.0272s/iter; left time: 92.2801s
[36m(_train_fn pid=29495)[0m 	iters: 400, epoch: 2 | loss: 0.4392723
[36m(_train_fn pid=29495)[0m 	speed: 0.0272s/iter; left time: 89.6572s
[36m(_train_fn pid=29495)[0m 	iters: 500, epoch: 2 | loss: 0.4897413
[36m(_train_fn pid=29495)[0m 	speed: 0.0272s/iter; left time: 86.9742s
[36m(_train_fn pid=29495)[0m Updating learning rate to 0.0030352923953344026
[36m(_train_fn pid=29495)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=29495)[0m saving checkpoint...
[36m(_train_fn pid=29495)[0m Epoch: 2 cost time: 14.403797626495361
[36m(_train_fn pid=29495)[0m Epoch: 2, Steps: 528 | Train Loss: 45.5357991 Vali Loss: 1.1504277 Best vali loss: 0.7831347
[36m(_train_fn pid=29495)[0m 	iters: 100, epoch: 3 | loss: 0.6295324
[36m(_train_fn pid=29495)[0m 	speed: 0.0534s/iter; left time: 163.8996s
[36m(_train_fn pid=29495)[0m 	iters: 200, epoch: 3 | loss: 0.6395198
[36m(_train_fn pid=29495)[0m 	speed: 0.0272s/iter; left time: 80.7810s
[36m(_train_fn pid=29495)[0m 	iters: 300, epoch: 3 | loss: 0.6483856
[36m(_train_fn pid=29495)[0m 	speed: 0.0272s/iter; left time: 78.0773s
[36m(_train_fn pid=29495)[0m 	iters: 400, epoch: 3 | loss: 0.5772153
[36m(_train_fn pid=29495)[0m 	speed: 0.0273s/iter; left time: 75.5539s
Trial status: 8 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:26:59. Total running time: 5min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 718a218a with best_valid_loss=0.6834153614338787 and params={'alpha_d_ff': 3, 'batch_size': 32, 'd_model': 64, 'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074}
[36m(_train_fn pid=29495)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-f649905d_9_alpha_d_ff=4,batch_size=16,d_model=512,decomp_method=moving_avg,moving_avg=35,down_sampling_method=conv,dropout=0_2024-08-23_05-26-11/checkpoint_000002)
[36m(_train_fn pid=29495)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-f649905d_9_alpha_d_ff=4,batch_size=16,d_model=512,decomp_method=moving_avg,moving_avg=35,down_sampling_method=conv,dropout=0_2024-08-23_05-26-11/checkpoint_000003)
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000000)
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-f649905d   RUNNING           2           33.8235       45.5358         1.15043             0.783135 │
│ trial-e4f57f3b   TERMINATED        8           17.3934        0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8           42.368         0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8           33.0653        0.346412       0.693692            0.693692 │
│ trial-c6399b0a   TERMINATED        8          113.012         0.350811       0.692065            0.691159 │
│ trial-654a3d52   TERMINATED        5            9.64573       0.320708       0.720368            0.708119 │
│ trial-718a218a   TERMINATED        7           25.2226        0.340039       0.685397            0.683415 │
│ trial-5bba4959   TERMINATED        8            8.77779       0.37242        0.732491            0.732491 │
│ trial-006275bf   TERMINATED        8           14.6681        0.31666        0.695833            0.693286 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=29495)[0m 	iters: 500, epoch: 3 | loss: 0.5955914
[36m(_train_fn pid=29495)[0m 	speed: 0.0274s/iter; left time: 73.1305s
[36m(_train_fn pid=29495)[0m Updating learning rate to 0.0015176461976672013
[36m(_train_fn pid=29495)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=29495)[0m saving checkpoint...
[36m(_train_fn pid=29495)[0m Epoch: 3 cost time: 14.417705297470093
[36m(_train_fn pid=29495)[0m Epoch: 3, Steps: 528 | Train Loss: 0.5471280 Vali Loss: 1.1481844 Best vali loss: 0.7831347
[36m(_train_fn pid=29495)[0m 	iters: 100, epoch: 4 | loss: 0.7729089
[36m(_train_fn pid=29495)[0m 	speed: 0.0536s/iter; left time: 136.1071s
[36m(_train_fn pid=29495)[0m 	iters: 200, epoch: 4 | loss: 0.4732138
[36m(_train_fn pid=29495)[0m 	speed: 0.0273s/iter; left time: 66.5883s
[36m(_train_fn pid=29495)[0m 	iters: 300, epoch: 4 | loss: 0.4802732
[36m(_train_fn pid=29495)[0m 	speed: 0.0273s/iter; left time: 63.8123s
[36m(_train_fn pid=29495)[0m 	iters: 400, epoch: 4 | loss: 0.6814998
[36m(_train_fn pid=29495)[0m 	speed: 0.0273s/iter; left time: 61.0765s
[36m(_train_fn pid=29495)[0m 	iters: 500, epoch: 4 | loss: 0.6024806
[36m(_train_fn pid=29495)[0m 	speed: 0.0278s/iter; left time: 59.4312s

Trial trial-f649905d completed after 4 iterations at 2024-08-23 05:27:19. Total running time: 5min 51s
╭───────────────────────────────────────────────────╮
│ Trial trial-f649905d result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000003 │
│ time_this_iter_s                         16.47663 │
│ time_total_s                             66.56241 │
│ training_iteration                              4 │
│ best_valid_loss                           0.78313 │
│ train_loss                                0.54616 │
│ valid_loss                                1.14647 │
╰───────────────────────────────────────────────────╯
[36m(_train_fn pid=29495)[0m Updating learning rate to 0.0007588230988336006
[36m(_train_fn pid=29495)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=29495)[0m saving checkpoint...
[36m(_train_fn pid=29495)[0m Epoch: 4 cost time: 14.520416975021362
[36m(_train_fn pid=29495)[0m Epoch: 4, Steps: 528 | Train Loss: 0.5461573 Vali Loss: 1.1464696 Best vali loss: 0.7831347
[36m(_train_fn pid=29495)[0m Early stopping

Trial trial-5e3611a0 started with configuration:
╭────────────────────────────────────────────╮
│ Trial trial-5e3611a0 config                │
├────────────────────────────────────────────┤
│ alpha_d_ff                               4 │
│ batch_size                             128 │
│ d_model                                 64 │
│ decomp_method/decomp_method     moving_avg │
│ decomp_method/moving_avg                15 │
│ down_sampling_method                  conv │
│ dropout                            0.11885 │
│ e_layers                                 4 │
│ learning_rate                      0.00322 │
╰────────────────────────────────────────────╯
[36m(_train_fn pid=29999)[0m configuration
[36m(_train_fn pid=29999)[0m {'batch_size': 128, 'd_model': 64, 'decomp_method': 'moving_avg', 'moving_avg': 15, 'down_sampling_method': 'conv', 'dropout': 0.11884684425154694, 'e_layers': 4, 'learning_rate': 0.0032240924334661727, 'd_ff': 256}
[36m(_train_fn pid=29999)[0m Use GPU: cuda:0
[36m(_train_fn pid=29999)[0m train 8449
[36m(_train_fn pid=29999)[0m val 2785
[36m(_train_fn pid=29999)[0m start_epoch 0
[36m(_train_fn pid=29999)[0m max_epoch 8
[36m(_train_fn pid=29999)[0m Validation loss decreased (inf --> 1.1323).  Saving model state dict ...
[36m(_train_fn pid=29999)[0m Updating learning rate to 0.0032240924334661727
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Epoch: 1 cost time: 3.6552681922912598
[36m(_train_fn pid=29999)[0m Epoch: 1, Steps: 66 | Train Loss: 0.6074951 Vali Loss: 1.1322856 Best vali loss: 1.1322856

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-23 05:27:29. Total running time: 6min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 718a218a with best_valid_loss=0.6834153614338787 and params={'alpha_d_ff': 3, 'batch_size': 32, 'd_model': 64, 'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074}
2024-08-23 05:27:30,710	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000001)
2024-08-23 05:27:34,597	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000002)
2024-08-23 05:27:38,435	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000003)
2024-08-23 05:27:42,183	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000004)
2024-08-23 05:27:46,031	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000005)
2024-08-23 05:27:49,847	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000006)
2024-08-23 05:27:53,620	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-23 05:27:53,629	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96' in 0.0032s.
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-5e3611a0   RUNNING           1            4.58774       0.607495       1.13229             1.13229  │
│ trial-e4f57f3b   TERMINATED        8           17.3934        0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8           42.368         0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8           33.0653        0.346412       0.693692            0.693692 │
│ trial-c6399b0a   TERMINATED        8          113.012         0.350811       0.692065            0.691159 │
│ trial-654a3d52   TERMINATED        5            9.64573       0.320708       0.720368            0.708119 │
│ trial-718a218a   TERMINATED        7           25.2226        0.340039       0.685397            0.683415 │
│ trial-5bba4959   TERMINATED        8            8.77779       0.37242        0.732491            0.732491 │
│ trial-006275bf   TERMINATED        8           14.6681        0.31666        0.695833            0.693286 │
│ trial-f649905d   TERMINATED        4           66.5624        0.546157       1.14647             0.783135 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯
[36m(_train_fn pid=29999)[0m Updating learning rate to 0.0016120462167330863
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Validation loss decreased (1.1323 --> 0.7313).  Saving model state dict ...
[36m(_train_fn pid=29999)[0m Epoch: 2 cost time: 3.2863481044769287
[36m(_train_fn pid=29999)[0m Epoch: 2, Steps: 66 | Train Loss: 0.4069561 Vali Loss: 0.7312578 Best vali loss: 0.7312578
[36m(_train_fn pid=29999)[0m Updating learning rate to 0.0008060231083665432
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Validation loss decreased (0.7313 --> 0.7116).  Saving model state dict ...
[36m(_train_fn pid=29999)[0m Epoch: 3 cost time: 3.3884732723236084
[36m(_train_fn pid=29999)[0m Epoch: 3, Steps: 66 | Train Loss: 0.3597055 Vali Loss: 0.7116386 Best vali loss: 0.7116386
[36m(_train_fn pid=29999)[0m Updating learning rate to 0.0004030115541832716
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Validation loss decreased (0.7116 --> 0.7108).  Saving model state dict ...
[36m(_train_fn pid=29999)[0m Epoch: 4 cost time: 3.337001323699951
[36m(_train_fn pid=29999)[0m Epoch: 4, Steps: 66 | Train Loss: 0.3501215 Vali Loss: 0.7108318 Best vali loss: 0.7108318
[36m(_train_fn pid=29999)[0m Updating learning rate to 0.0002015057770916358
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Validation loss decreased (0.7108 --> 0.7106).  Saving model state dict ...
[36m(_train_fn pid=29999)[0m Epoch: 5 cost time: 3.267230272293091
[36m(_train_fn pid=29999)[0m Epoch: 5, Steps: 66 | Train Loss: 0.3459009 Vali Loss: 0.7105508 Best vali loss: 0.7105508
[36m(_train_fn pid=29999)[0m Updating learning rate to 0.0001007528885458179
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Validation loss decreased (0.7106 --> 0.7051).  Saving model state dict ...
[36m(_train_fn pid=29999)[0m Epoch: 6 cost time: 3.3599746227264404
[36m(_train_fn pid=29999)[0m Epoch: 6, Steps: 66 | Train Loss: 0.3434840 Vali Loss: 0.7050859 Best vali loss: 0.7050859
[36m(_train_fn pid=29999)[0m Updating learning rate to 5.037644427290895e-05
[36m(_train_fn pid=29999)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Epoch: 7 cost time: 3.346937417984009
[36m(_train_fn pid=29999)[0m Epoch: 7, Steps: 66 | Train Loss: 0.3419608 Vali Loss: 0.7083469 Best vali loss: 0.7050859

Trial trial-5e3611a0 completed after 8 iterations at 2024-08-23 05:27:53. Total running time: 6min 25s
╭───────────────────────────────────────────────────╮
│ Trial trial-5e3611a0 result                       │
├───────────────────────────────────────────────────┤
│ checkpoint_dir_name             checkpoint_000007 │
│ time_this_iter_s                          3.77057 │
│ time_total_s                             31.23469 │
│ training_iteration                              8 │
│ best_valid_loss                           0.70509 │
│ train_loss                                0.34108 │
│ valid_loss                                0.70654 │
╰───────────────────────────────────────────────────╯

Trial status: 10 TERMINATED
Current time: 2024-08-23 05:27:53. Total running time: 6min 25s
Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 718a218a with best_valid_loss=0.6834153614338787 and params={'alpha_d_ff': 3, 'batch_size': 32, 'd_model': 64, 'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074}
╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name       status         iter     total time (s)     train_loss     valid_loss     best_valid_loss │
├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ trial-e4f57f3b   TERMINATED        8           17.3934        0.353253       0.69442             0.69442  │
│ trial-66aadf09   TERMINATED        8           42.368         0.288719       0.747469            0.709102 │
│ trial-b4e5e021   TERMINATED        8           33.0653        0.346412       0.693692            0.693692 │
│ trial-c6399b0a   TERMINATED        8          113.012         0.350811       0.692065            0.691159 │
│ trial-654a3d52   TERMINATED        5            9.64573       0.320708       0.720368            0.708119 │
│ trial-718a218a   TERMINATED        7           25.2226        0.340039       0.685397            0.683415 │
│ trial-5bba4959   TERMINATED        8            8.77779       0.37242        0.732491            0.732491 │
│ trial-006275bf   TERMINATED        8           14.6681        0.31666        0.695833            0.693286 │
│ trial-f649905d   TERMINATED        4           66.5624        0.546157       1.14647             0.783135 │
│ trial-5e3611a0   TERMINATED        8           31.2347        0.341079       0.706544            0.705086 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Best hyperparameters found were:  {'alpha_d_ff': 3, 'batch_size': 32, 'd_model': 64, 'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}, 'down_sampling_method': 'avg', 'dropout': 0.07683623441236416, 'e_layers': 3, 'learning_rate': 0.0011650014804827074}
[36m(_train_fn pid=29999)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/TimeMixer/checkpoints/hptunning/hyperopt_tpe/ETTh1_96_96/trial-5e3611a0_10_alpha_d_ff=4,batch_size=128,d_model=64,decomp_method=moving_avg,moving_avg=15,down_sampling_method=conv,dropout=_2024-08-23_05-27-19/checkpoint_000007)
[36m(_train_fn pid=29999)[0m Updating learning rate to 2.5188222136454474e-05
[36m(_train_fn pid=29999)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=29999)[0m saving checkpoint...
[36m(_train_fn pid=29999)[0m Epoch: 8 cost time: 3.280031204223633
[36m(_train_fn pid=29999)[0m Epoch: 8, Steps: 66 | Train Loss: 0.3410790 Vali Loss: 0.7065440 Best vali loss: 0.7050859


Time taken (1 parallel trials): 390 seconds


