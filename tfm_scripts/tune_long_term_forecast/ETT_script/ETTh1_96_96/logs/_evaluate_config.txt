
Testing Random Search for ETTh1_96_96

Current best trial: e8d8b_00005 with best_valid_loss=0.6808662353747192 and params:
    'batch_size': 16
    'learning_rate': 0.003186053793189858
    'down_sampling_method': 'avg'
    'd_model': 32
    'decomp_method': 'moving_avg'
    'moving_avg': 15
    'e_layers': 1
    'dropout': 0.12685076554929858
    'd_ff': 64


python3 train_timemixer.py \
    --model TimeMixer \
    --data ETTh1 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh1.csv \
    --features M \
    --save_dir ./checkpoints/hptunning/evaluate_config_test \
    --seq_len 96 \
    --label_len 0 \
    --pred_len 96 \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --d_model 32 \
    --d_ff 64 \
    --e_layers 1 \
    --down_sampling_layers 3 \
    --down_sampling_window 2 \
    --down_sampling_method avg \
    --decomp_method moving_avg \
    --moving_avg 15 \
    --train_epochs 8 \
    --patience 0 \
    --learning_rate 0.003186053793189858 \
    --dropout 0.12685076554929858 \
    --batch_size 16 \
    --num_workers 1 \
    --gpu 0 \
    --seed 123;


Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=123, data='ETTh1', root_path='./dataset/ETT-small/', data_path='ETTh1.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='moving_avg', top_k=5, moving_avg=15, d_model=32, d_ff=64, e_layers=1, dropout=0.12685076554929858, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=1, train_epochs=8, batch_size=16, patience=0, delta=0.0, learning_rate=0.003186053793189858, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/hptunning/evaluate_config_test', save_last=False, break_at=0)
Use GPU: cuda:0
train 8449
val 2785
test 2785
	iters: 100, epoch: 1 | loss: 0.5610927
	speed: 0.0121s/iter; left time: 50.0971s
	iters: 200, epoch: 1 | loss: 0.5133489
	speed: 0.0066s/iter; left time: 26.5910s
	iters: 300, epoch: 1 | loss: 0.4785228
	speed: 0.0067s/iter; left time: 26.2780s
	iters: 400, epoch: 1 | loss: 0.5232438
	speed: 0.0066s/iter; left time: 25.1697s
	iters: 500, epoch: 1 | loss: 0.4166098
	speed: 0.0065s/iter; left time: 24.0903s
Epoch: 1 cost time: 3.7435874938964844
Epoch: 1, Steps: 528 | Train Loss: 0.5091972 Vali Loss: 0.7388693
Updating learning rate to 0.003186053793189858
Validation loss decreased (inf --> 0.7389).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.4812087
	speed: 0.0136s/iter; left time: 48.8731s
	iters: 200, epoch: 2 | loss: 0.4405942
	speed: 0.0066s/iter; left time: 23.1199s
	iters: 300, epoch: 2 | loss: 0.3781609
	speed: 0.0065s/iter; left time: 21.9551s
	iters: 400, epoch: 2 | loss: 0.4066679
	speed: 0.0064s/iter; left time: 21.1714s
	iters: 500, epoch: 2 | loss: 0.3218037
	speed: 0.0067s/iter; left time: 21.3065s
Epoch: 2 cost time: 3.500488758087158
Epoch: 2, Steps: 528 | Train Loss: 0.3701287 Vali Loss: 0.6976531
Updating learning rate to 0.001593026896594929
Validation loss decreased (0.7389 --> 0.6977).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.3043881
	speed: 0.0128s/iter; left time: 39.3773s
	iters: 200, epoch: 3 | loss: 0.2809246
	speed: 0.0060s/iter; left time: 17.6819s
	iters: 300, epoch: 3 | loss: 0.3339178
	speed: 0.0060s/iter; left time: 17.1829s
	iters: 400, epoch: 3 | loss: 0.3770324
	speed: 0.0058s/iter; left time: 16.0656s
	iters: 500, epoch: 3 | loss: 0.3858248
	speed: 0.0059s/iter; left time: 15.6924s
Epoch: 3 cost time: 3.1629154682159424
Epoch: 3, Steps: 528 | Train Loss: 0.3498917 Vali Loss: 0.6967854
Updating learning rate to 0.0007965134482974645
Validation loss decreased (0.6977 --> 0.6968).  Saving model state dict ...
	iters: 100, epoch: 4 | loss: 0.4028542
	speed: 0.0123s/iter; left time: 31.1291s
	iters: 200, epoch: 4 | loss: 0.3859899
	speed: 0.0058s/iter; left time: 14.2524s
	iters: 300, epoch: 4 | loss: 0.3219083
	speed: 0.0055s/iter; left time: 12.9745s
	iters: 400, epoch: 4 | loss: 0.3096860
	speed: 0.0055s/iter; left time: 12.3435s
	iters: 500, epoch: 4 | loss: 0.3362450
	speed: 0.0056s/iter; left time: 11.9507s
Epoch: 4 cost time: 3.0156493186950684
Epoch: 4, Steps: 528 | Train Loss: 0.3414921 Vali Loss: 0.6808662
Updating learning rate to 0.00039825672414873223
Validation loss decreased (0.6968 --> 0.6809).  Saving model state dict ...
	iters: 100, epoch: 5 | loss: 0.3842625
	speed: 0.0124s/iter; left time: 25.0332s
	iters: 200, epoch: 5 | loss: 0.2764683
	speed: 0.0059s/iter; left time: 11.2692s
	iters: 300, epoch: 5 | loss: 0.2422689
	speed: 0.0059s/iter; left time: 10.7203s
	iters: 400, epoch: 5 | loss: 0.4622053
	speed: 0.0059s/iter; left time: 10.0588s
	iters: 500, epoch: 5 | loss: 0.4269703
	speed: 0.0060s/iter; left time: 9.5980s
Epoch: 5 cost time: 3.1621005535125732
Epoch: 5, Steps: 528 | Train Loss: 0.3358702 Vali Loss: 0.6964654
Updating learning rate to 0.00019912836207436612
	iters: 100, epoch: 6 | loss: 0.2701177
	speed: 0.0125s/iter; left time: 18.5705s
	iters: 200, epoch: 6 | loss: 0.3617267
	speed: 0.0059s/iter; left time: 8.1356s
	iters: 300, epoch: 6 | loss: 0.3327368
	speed: 0.0059s/iter; left time: 7.5173s
	iters: 400, epoch: 6 | loss: 0.3363102
	speed: 0.0060s/iter; left time: 7.1354s
	iters: 500, epoch: 6 | loss: 0.3087023
	speed: 0.0060s/iter; left time: 6.4969s
Epoch: 6 cost time: 3.164569139480591
Epoch: 6, Steps: 528 | Train Loss: 0.3324850 Vali Loss: 0.6948599
Updating learning rate to 9.956418103718306e-05
	iters: 100, epoch: 7 | loss: 0.2971365
	speed: 0.0124s/iter; left time: 11.8456s
	iters: 200, epoch: 7 | loss: 0.2892811
	speed: 0.0060s/iter; left time: 5.1617s
	iters: 300, epoch: 7 | loss: 0.3394414
	speed: 0.0060s/iter; left time: 4.5592s
	iters: 400, epoch: 7 | loss: 0.2485960
	speed: 0.0060s/iter; left time: 3.9564s
	iters: 500, epoch: 7 | loss: 0.3748299
	speed: 0.0060s/iter; left time: 3.3218s
Epoch: 7 cost time: 3.2014217376708984
Epoch: 7, Steps: 528 | Train Loss: 0.3305773 Vali Loss: 0.7016590
Updating learning rate to 4.978209051859153e-05
	iters: 100, epoch: 8 | loss: 0.3352622
	speed: 0.0124s/iter; left time: 5.3315s
	iters: 200, epoch: 8 | loss: 0.3090199
	speed: 0.0059s/iter; left time: 1.9460s
	iters: 300, epoch: 8 | loss: 0.3569971
	speed: 0.0060s/iter; left time: 1.3686s
	iters: 400, epoch: 8 | loss: 0.2796569
	speed: 0.0060s/iter; left time: 0.7693s
	iters: 500, epoch: 8 | loss: 0.2659298
	speed: 0.0060s/iter; left time: 0.1726s
Epoch: 8 cost time: 3.1823086738586426
Epoch: 8, Steps: 528 | Train Loss: 0.3296727 Vali Loss: 0.6966214
Updating learning rate to 2.4891045259295765e-05
#####   loading best weights   #####
Process: python3 (PID: 10499) is using 360 MiB of GPU memory.
Epoch: 8 | Elapsed Time: 136.75520396232605 s | VRAM usage: 0.3515625 Gb | Train MSE: 0.3355 Train MAE: 0.3976 Vali MSE: 0.6809 Vali MAE: 0.5424 Test MSE: 0.3759 Test MAE: 0.3950

Testing Hyperopt TPE for ETTh1_96_96

Current best trial: 718a218a with best_valid_loss=0.6834153614338787 and params:
    'alpha_d_ff': 3
    'batch_size': 32
    'd_model': 64
    'decomp_method': {'decomp_method': 'moving_avg', 'moving_avg': 35}
    'down_sampling_method': 'avg'
    'dropout': 0.07683623441236416
    'e_layers': 3
    'learning_rate': 0.0011650014804827074

python3 train_timemixer.py \
    --model TimeMixer \
    --data ETTh1 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh1.csv \
    --features M \
    --save_dir ./checkpoints/hptunning/evaluate_config_test \
    --seq_len 96 \
    --label_len 0 \
    --pred_len 96 \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --d_model 64 \
    --d_ff 192 \
    --e_layers 3 \
    --down_sampling_layers 3 \
    --down_sampling_window 2 \
    --down_sampling_method avg \
    --decomp_method moving_avg \
    --moving_avg 35 \
    --train_epochs 8 \
    --patience 0 \
    --learning_rate 0.0011650014804827074 \
    --dropout 0.07683623441236416\
    --batch_size 32 \
    --num_workers 1 \
    --gpu 0 \
    --seed 123;

Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=123, data='ETTh1', root_path='./dataset/ETT-small/', data_path='ETTh1.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='moving_avg', top_k=5, moving_avg=35, d_model=64, d_ff=192, e_layers=3, dropout=0.07683623441236416, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=1, train_epochs=8, batch_size=32, patience=0, delta=0.0, learning_rate=0.0011650014804827074, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/hptunning/evaluate_config_test', save_last=False, break_at=0)
Use GPU: cuda:0
train 8449
val 2785
test 2785
	iters: 100, epoch: 1 | loss: 0.6180534
	speed: 0.0175s/iter; left time: 35.2959s
	iters: 200, epoch: 1 | loss: 0.6671316
	speed: 0.0120s/iter; left time: 22.9320s
Epoch: 1 cost time: 3.444647789001465
Epoch: 1, Steps: 264 | Train Loss: 0.5800912 Vali Loss: 0.9941272
Updating learning rate to 0.0011650014804827074
Validation loss decreased (inf --> 0.9941).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.4010786
	speed: 0.0245s/iter; left time: 42.8972s
	iters: 200, epoch: 2 | loss: 0.3400997
	speed: 0.0119s/iter; left time: 19.5715s
Epoch: 2 cost time: 3.1713054180145264
Epoch: 2, Steps: 264 | Train Loss: 0.3785497 Vali Loss: 0.7089093
Updating learning rate to 0.0005825007402413537
Validation loss decreased (0.9941 --> 0.7089).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.3533227
	speed: 0.0242s/iter; left time: 35.9915s
	iters: 200, epoch: 3 | loss: 0.3159066
	speed: 0.0118s/iter; left time: 16.4036s
Epoch: 3 cost time: 3.1822121143341064
Epoch: 3, Steps: 264 | Train Loss: 0.3532748 Vali Loss: 0.6876169
Updating learning rate to 0.00029125037012067684
Validation loss decreased (0.7089 --> 0.6876).  Saving model state dict ...
	iters: 100, epoch: 4 | loss: 0.3440340
	speed: 0.0247s/iter; left time: 30.1372s
	iters: 200, epoch: 4 | loss: 0.4719048
	speed: 0.0121s/iter; left time: 13.5106s
Epoch: 4 cost time: 3.2454028129577637
Epoch: 4, Steps: 264 | Train Loss: 0.3468847 Vali Loss: 0.6834154
Updating learning rate to 0.00014562518506033842
Validation loss decreased (0.6876 --> 0.6834).  Saving model state dict ...
	iters: 100, epoch: 5 | loss: 0.3608091
	speed: 0.0243s/iter; left time: 23.2786s
	iters: 200, epoch: 5 | loss: 0.4063101
	speed: 0.0119s/iter; left time: 10.2382s
Epoch: 5 cost time: 3.19657564163208
Epoch: 5, Steps: 264 | Train Loss: 0.3433198 Vali Loss: 0.6887759
Updating learning rate to 7.281259253016921e-05
	iters: 100, epoch: 6 | loss: 0.3238847
	speed: 0.0238s/iter; left time: 16.4682s
	iters: 200, epoch: 6 | loss: 0.2816381
	speed: 0.0115s/iter; left time: 6.8045s
Epoch: 6 cost time: 3.041248321533203
Epoch: 6, Steps: 264 | Train Loss: 0.3409856 Vali Loss: 0.6859739
Updating learning rate to 3.6406296265084605e-05
	iters: 100, epoch: 7 | loss: 0.3162684
	speed: 0.0229s/iter; left time: 9.8395s
	iters: 200, epoch: 7 | loss: 0.4242671
	speed: 0.0111s/iter; left time: 3.6460s
Epoch: 7 cost time: 2.9761033058166504
Epoch: 7, Steps: 264 | Train Loss: 0.3400393 Vali Loss: 0.6853969
Updating learning rate to 1.8203148132542303e-05
	iters: 100, epoch: 8 | loss: 0.3645488
	speed: 0.0229s/iter; left time: 3.7842s
	iters: 200, epoch: 8 | loss: 0.2855204
	speed: 0.0111s/iter; left time: 0.7202s
Epoch: 8 cost time: 2.9596939086914062
Epoch: 8, Steps: 264 | Train Loss: 0.3393455 Vali Loss: 0.6837452
Updating learning rate to 9.101574066271151e-06
#####   loading best weights   #####
Process: python3 (PID: 22918) is using 826 MiB of GPU memory.
Epoch: 8 | Elapsed Time: 131.6154043674469 s | VRAM usage: 0.806640625 Gb | Train MSE: 0.3424 Train MAE: 0.4017 Vali MSE: 0.6834 Vali MAE: 0.5448 Test MSE: 0.3822 Test MAE: 0.3969

Testing BOHB for ETTh1_96_96

Current best trial: d1c9875a with best_valid_loss=0.6859459355901713 and params:
    'batch_size': 16
    'd_model': 256
    'decomp_method': 'moving_avg'
    'down_sampling_method': 'avg'
    'dropout': 0.1031939422525546
    'e_layers': 3
    'learning_rate': 0.0006777561947145852
    'moving_avg': 75
    'd_ff': 1024

python3 train_timemixer.py \
    --model TimeMixer \
    --data ETTh1 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh1.csv \
    --features M \
    --save_dir ./checkpoints/hptunning/evaluate_config_test \
    --seq_len 96 \
    --label_len 0 \
    --pred_len 96 \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --d_model 256 \
    --d_ff 1024 \
    --e_layers 3 \
    --down_sampling_layers 3 \
    --down_sampling_window 2 \
    --down_sampling_method avg \
    --decomp_method moving_avg \
    --moving_avg 75 \
    --train_epochs 8 \
    --patience 0 \
    --learning_rate 0.0006777561947145852 \
    --dropout 0.1031939422525546\
    --batch_size 16 \
    --num_workers 1 \
    --gpu 0 \
    --seed 123;

Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=123, data='ETTh1', root_path='./dataset/ETT-small/', data_path='ETTh1.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='moving_avg', top_k=5, moving_avg=75, d_model=256, d_ff=1024, e_layers=3, dropout=0.1031939422525546, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=1, train_epochs=8, batch_size=16, patience=0, delta=0.0, learning_rate=0.0006777561947145852, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/hptunning/evaluate_config_test', save_last=False, break_at=0)
Use GPU: cuda:0
train 8449
val 2785
test 2785
	iters: 100, epoch: 1 | loss: 0.5844831
	speed: 0.0350s/iter; left time: 144.4023s
	iters: 200, epoch: 1 | loss: 0.5097508
	speed: 0.0302s/iter; left time: 121.3637s
	iters: 300, epoch: 1 | loss: 0.4671035
	speed: 0.0307s/iter; left time: 120.3756s
	iters: 400, epoch: 1 | loss: 0.5100750
	speed: 0.0305s/iter; left time: 116.7267s
	iters: 500, epoch: 1 | loss: 0.5345711
	speed: 0.0303s/iter; left time: 112.8264s
Epoch: 1 cost time: 16.229362726211548
Epoch: 1, Steps: 528 | Train Loss: 0.5947430 Vali Loss: 0.9216296
Updating learning rate to 0.0006777561947145852
Validation loss decreased (inf --> 0.9216).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.4213852
	speed: 0.0588s/iter; left time: 211.5060s
	iters: 200, epoch: 2 | loss: 0.3935506
	speed: 0.0302s/iter; left time: 105.5594s
	iters: 300, epoch: 2 | loss: 0.4122412
	speed: 0.0303s/iter; left time: 102.9725s
	iters: 400, epoch: 2 | loss: 0.3429659
	speed: 0.0302s/iter; left time: 99.4981s
	iters: 500, epoch: 2 | loss: 0.4806866
	speed: 0.0304s/iter; left time: 97.1267s
Epoch: 2 cost time: 16.000346422195435
Epoch: 2, Steps: 528 | Train Loss: 0.3807891 Vali Loss: 0.7080322
Updating learning rate to 0.0003388780973572926
Validation loss decreased (0.9216 --> 0.7080).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.3567242
	speed: 0.0597s/iter; left time: 183.2193s
	iters: 200, epoch: 3 | loss: 0.3679386
	speed: 0.0308s/iter; left time: 91.3975s
	iters: 300, epoch: 3 | loss: 0.3245396
	speed: 0.0307s/iter; left time: 88.1677s
	iters: 400, epoch: 3 | loss: 0.3473721
	speed: 0.0307s/iter; left time: 84.9478s
	iters: 500, epoch: 3 | loss: 0.3094723
	speed: 0.0308s/iter; left time: 82.3008s
Epoch: 3 cost time: 16.25033211708069
Epoch: 3, Steps: 528 | Train Loss: 0.3597432 Vali Loss: 0.6955709
Updating learning rate to 0.0001694390486786463
Validation loss decreased (0.7080 --> 0.6956).  Saving model state dict ...
	iters: 100, epoch: 4 | loss: 0.3463947
	speed: 0.0594s/iter; left time: 150.9903s
	iters: 200, epoch: 4 | loss: 0.3828813
	speed: 0.0304s/iter; left time: 74.1049s
	iters: 300, epoch: 4 | loss: 0.4040558
	speed: 0.0303s/iter; left time: 70.9026s
	iters: 400, epoch: 4 | loss: 0.2887249
	speed: 0.0303s/iter; left time: 67.9530s
	iters: 500, epoch: 4 | loss: 0.3493426
	speed: 0.0303s/iter; left time: 64.8983s
Epoch: 4 cost time: 16.038312911987305
Epoch: 4, Steps: 528 | Train Loss: 0.3517461 Vali Loss: 0.6889709
Updating learning rate to 8.471952433932315e-05
Validation loss decreased (0.6956 --> 0.6890).  Saving model state dict ...
	iters: 100, epoch: 5 | loss: 0.2853508
	speed: 0.0600s/iter; left time: 120.8648s
	iters: 200, epoch: 5 | loss: 0.3377908
	speed: 0.0303s/iter; left time: 58.0303s
	iters: 300, epoch: 5 | loss: 0.3568981
	speed: 0.0304s/iter; left time: 55.0319s
	iters: 400, epoch: 5 | loss: 0.3600089
	speed: 0.0308s/iter; left time: 52.7247s
	iters: 500, epoch: 5 | loss: 0.3118772
	speed: 0.0312s/iter; left time: 50.3049s
Epoch: 5 cost time: 16.230390787124634
Epoch: 5, Steps: 528 | Train Loss: 0.3476301 Vali Loss: 0.6859459
Updating learning rate to 4.235976216966158e-05
Validation loss decreased (0.6890 --> 0.6859).  Saving model state dict ...
	iters: 100, epoch: 6 | loss: 0.2899534
	speed: 0.0593s/iter; left time: 88.0028s
	iters: 200, epoch: 6 | loss: 0.4036916
	speed: 0.0304s/iter; left time: 42.0837s
	iters: 300, epoch: 6 | loss: 0.3017064
	speed: 0.0304s/iter; left time: 39.0791s
	iters: 400, epoch: 6 | loss: 0.2714378
	speed: 0.0304s/iter; left time: 36.0307s
	iters: 500, epoch: 6 | loss: 0.3051854
	speed: 0.0304s/iter; left time: 33.0109s
Epoch: 6 cost time: 16.079944610595703
Epoch: 6, Steps: 528 | Train Loss: 0.3451108 Vali Loss: 0.6866270
Updating learning rate to 2.117988108483079e-05
	iters: 100, epoch: 7 | loss: 0.3548769
	speed: 0.0602s/iter; left time: 57.6193s
	iters: 200, epoch: 7 | loss: 0.3306108
	speed: 0.0306s/iter; left time: 26.2389s
	iters: 300, epoch: 7 | loss: 0.3758321
	speed: 0.0310s/iter; left time: 23.4872s
	iters: 400, epoch: 7 | loss: 0.3166223
	speed: 0.0312s/iter; left time: 20.4759s
	iters: 500, epoch: 7 | loss: 0.4516889
	speed: 0.0310s/iter; left time: 17.2506s
Epoch: 7 cost time: 16.427549362182617
Epoch: 7, Steps: 528 | Train Loss: 0.3433045 Vali Loss: 0.6857273
Updating learning rate to 1.0589940542415394e-05
Validation loss decreased (0.6859 --> 0.6857).  Saving model state dict ...
	iters: 100, epoch: 8 | loss: 0.3616836
	speed: 0.0606s/iter; left time: 25.9977s
	iters: 200, epoch: 8 | loss: 0.2973939
	speed: 0.0307s/iter; left time: 10.0992s
	iters: 300, epoch: 8 | loss: 0.3347520
	speed: 0.0310s/iter; left time: 7.0972s
	iters: 400, epoch: 8 | loss: 0.3435722
	speed: 0.0313s/iter; left time: 4.0352s
	iters: 500, epoch: 8 | loss: 0.3289837
	speed: 0.0312s/iter; left time: 0.9060s
Epoch: 8 cost time: 16.40378165245056
Epoch: 8, Steps: 528 | Train Loss: 0.3426156 Vali Loss: 0.6877078
Updating learning rate to 5.294970271207697e-06
#####   loading best weights   #####
Process: python3 (PID: 23237) is using 1596 MiB of GPU memory.
Epoch: 8 | Elapsed Time: 654.66304063797 s | VRAM usage: 1.55859375 Gb | Train MSE: 0.3413 Train MAE: 0.4025 Vali MSE: 0.6857 Vali MAE: 0.5481 Test MSE: 0.3791 Test MAE: 0.3978

Testing SMAC for ETTh1_96_96

Configuration(values={
  'alpha_d_ff': 3,
  'batch_size': 64,
  'd_model': 64,
  'decomp_method': 'dft_decomp',
  'down_sampling_method': 'avg',
  'dropout': 0.0901386749456,
  'e_layers': 3,
  'learning_rate': 0.0037547181247,
})
Incumbent cost: 0.6806332870971324

python3 train_timemixer.py \
    --model TimeMixer \
    --data ETTh1 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh1.csv \
    --features M \
    --save_dir ./checkpoints/hptunning/evaluate_config_test \
    --seq_len 96 \
    --label_len 0 \
    --pred_len 96 \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --d_model 64 \
    --d_ff 192 \
    --e_layers 3 \
    --down_sampling_layers 3 \
    --down_sampling_window 2 \
    --down_sampling_method avg \
    --decomp_method dft_decomp \
    --moving_avg 25 \
    --train_epochs 8 \
    --patience 0 \
    --learning_rate 0.0037547181247 \
    --dropout 0.0901386749456\
    --batch_size 64 \
    --num_workers 1 \
    --gpu 0 \
    --seed 123;

Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=123, data='ETTh1', root_path='./dataset/ETT-small/', data_path='ETTh1.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='dft_decomp', top_k=5, moving_avg=25, d_model=64, d_ff=192, e_layers=3, dropout=0.0901386749456, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=1, train_epochs=8, batch_size=64, patience=0, delta=0.0, learning_rate=0.0037547181247, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/hptunning/evaluate_config_test', save_last=False, break_at=0)
Use GPU: cuda:0
train 8449
val 2785
test 2785
	iters: 100, epoch: 1 | loss: 0.5963430
	speed: 0.0263s/iter; left time: 25.1387s
Epoch: 1 cost time: 2.9387905597686768
Epoch: 1, Steps: 132 | Train Loss: 0.5761573 Vali Loss: 0.9736871
Updating learning rate to 0.0037547181247
Validation loss decreased (inf --> 0.9737).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.3393277
	speed: 0.0289s/iter; left time: 23.8657s
Epoch: 2 cost time: 2.467181444168091
Epoch: 2, Steps: 132 | Train Loss: 0.3846107 Vali Loss: 0.7082543
Updating learning rate to 0.00187735906235
Validation loss decreased (0.9737 --> 0.7083).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.2904711
	speed: 0.0298s/iter; left time: 20.6524s
Epoch: 3 cost time: 2.4983584880828857
Epoch: 3, Steps: 132 | Train Loss: 0.3502397 Vali Loss: 0.6862415
Updating learning rate to 0.000938679531175
Validation loss decreased (0.7083 --> 0.6862).  Saving model state dict ...
	iters: 100, epoch: 4 | loss: 0.3924854
	speed: 0.0291s/iter; left time: 16.3273s
Epoch: 4 cost time: 2.485793113708496
Epoch: 4, Steps: 132 | Train Loss: 0.3413980 Vali Loss: 0.6797172
Updating learning rate to 0.0004693397655875
Validation loss decreased (0.6862 --> 0.6797).  Saving model state dict ...
	iters: 100, epoch: 5 | loss: 0.3422673
	speed: 0.0296s/iter; left time: 12.6914s
Epoch: 5 cost time: 2.4706478118896484
Epoch: 5, Steps: 132 | Train Loss: 0.3359924 Vali Loss: 0.6839120
Updating learning rate to 0.00023466988279375
	iters: 100, epoch: 6 | loss: 0.3160224
	speed: 0.0294s/iter; left time: 8.7206s
Epoch: 6 cost time: 2.5071053504943848
Epoch: 6, Steps: 132 | Train Loss: 0.3326786 Vali Loss: 0.6871151
Updating learning rate to 0.000117334941396875
	iters: 100, epoch: 7 | loss: 0.3705038
	speed: 0.0293s/iter; left time: 4.8345s
Epoch: 7 cost time: 2.475167751312256
Epoch: 7, Steps: 132 | Train Loss: 0.3309074 Vali Loss: 0.6836016
Updating learning rate to 5.86674706984375e-05
	iters: 100, epoch: 8 | loss: 0.2771703
	speed: 0.0291s/iter; left time: 0.9609s
Epoch: 8 cost time: 2.4709110260009766
Epoch: 8, Steps: 132 | Train Loss: 0.3297767 Vali Loss: 0.6828433
Updating learning rate to 2.933373534921875e-05
#####   loading best weights   #####
Process: python3 (PID: 22375) is using 1284 MiB of GPU memory.
Epoch: 8 | Elapsed Time: 109.33663511276245 s | VRAM usage: 1.25390625 Gb | Train MSE: 0.3355 Train MAE: 0.3995 Vali MSE: 0.6797 Vali MAE: 0.5463 Test MSE: 0.3797 Test MAE: 0.4012





