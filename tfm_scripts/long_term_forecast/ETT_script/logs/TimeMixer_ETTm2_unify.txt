Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=2021, data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='moving_avg', top_k=5, moving_avg=25, d_model=32, d_ff=32, e_layers=2, dropout=0.1, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=4, train_epochs=10, batch_size=128, patience=10, delta=0.0, learning_rate=0.01, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/baseline/TimeMixer/ETTm2_96_96', save_last=False, break_at=0)
Use GPU: cuda:0
train 34369
val 11425
test 11425
	iters: 100, epoch: 1 | loss: 0.1973151
	speed: 0.0205s/iter; left time: 52.8775s
	iters: 200, epoch: 1 | loss: 0.2454926
	speed: 0.0141s/iter; left time: 35.0489s
Epoch: 1 cost time: 4.1812357902526855
Epoch: 1, Steps: 268 | Train Loss: 0.2643163 Vali Loss: 0.1314692
Updating learning rate to 0.01
Validation loss decreased (inf --> 0.1315).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.4030885
	speed: 0.0320s/iter; left time: 74.0992s
	iters: 200, epoch: 2 | loss: 0.1732239
	speed: 0.0144s/iter; left time: 31.7598s
Epoch: 2 cost time: 3.9296157360076904
Epoch: 2, Steps: 268 | Train Loss: 0.2272359 Vali Loss: 0.1297768
Updating learning rate to 0.005
Validation loss decreased (0.1315 --> 0.1298).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.2133261
	speed: 0.0327s/iter; left time: 66.8399s
	iters: 200, epoch: 3 | loss: 0.2465406
	speed: 0.0145s/iter; left time: 28.2139s
Epoch: 3 cost time: 3.98577618598938
Epoch: 3, Steps: 268 | Train Loss: 0.2113842 Vali Loss: 0.1308481
Updating learning rate to 0.0025
EarlyStopping counter: 1 out of 10
	iters: 100, epoch: 4 | loss: 0.1390047
	speed: 0.0324s/iter; left time: 57.6620s
	iters: 200, epoch: 4 | loss: 0.1807029
	speed: 0.0143s/iter; left time: 24.0451s
Epoch: 4 cost time: 3.9504940509796143
Epoch: 4, Steps: 268 | Train Loss: 0.2016703 Vali Loss: 0.1295577
Updating learning rate to 0.00125
Validation loss decreased (0.1298 --> 0.1296).  Saving model state dict ...
	iters: 100, epoch: 5 | loss: 0.1827227
	speed: 0.0329s/iter; left time: 49.6286s
	iters: 200, epoch: 5 | loss: 0.1801625
	speed: 0.0143s/iter; left time: 20.1666s
Epoch: 5 cost time: 3.9360949993133545
Epoch: 5, Steps: 268 | Train Loss: 0.1947062 Vali Loss: 0.1315601
Updating learning rate to 0.000625
EarlyStopping counter: 1 out of 10
	iters: 100, epoch: 6 | loss: 0.1733964
	speed: 0.0321s/iter; left time: 39.8540s
	iters: 200, epoch: 6 | loss: 0.1639827
	speed: 0.0144s/iter; left time: 16.3754s
Epoch: 6 cost time: 3.9228522777557373
Epoch: 6, Steps: 268 | Train Loss: 0.1885113 Vali Loss: 0.1361417
Updating learning rate to 0.0003125
EarlyStopping counter: 2 out of 10
	iters: 100, epoch: 7 | loss: 0.1854834
	speed: 0.0322s/iter; left time: 31.3068s
	iters: 200, epoch: 7 | loss: 0.1751801
	speed: 0.0144s/iter; left time: 12.5343s
Epoch: 7 cost time: 3.930037498474121
Epoch: 7, Steps: 268 | Train Loss: 0.1853567 Vali Loss: 0.1376642
Updating learning rate to 0.00015625
EarlyStopping counter: 3 out of 10
	iters: 100, epoch: 8 | loss: 0.2003712
	speed: 0.0332s/iter; left time: 23.4359s
	iters: 200, epoch: 8 | loss: 0.1598203
	speed: 0.0145s/iter; left time: 8.7583s
Epoch: 8 cost time: 4.078415393829346
Epoch: 8, Steps: 268 | Train Loss: 0.1831764 Vali Loss: 0.1367023
Updating learning rate to 7.8125e-05
EarlyStopping counter: 4 out of 10
	iters: 100, epoch: 9 | loss: 0.2910018
	speed: 0.0332s/iter; left time: 14.5217s
	iters: 200, epoch: 9 | loss: 0.1634021
	speed: 0.0145s/iter; left time: 4.8922s
Epoch: 9 cost time: 3.9988038539886475
Epoch: 9, Steps: 268 | Train Loss: 0.1825109 Vali Loss: 0.1383142
Updating learning rate to 3.90625e-05
EarlyStopping counter: 5 out of 10
	iters: 100, epoch: 10 | loss: 0.2448311
	speed: 0.0328s/iter; left time: 5.5352s
	iters: 200, epoch: 10 | loss: 0.1486618
	speed: 0.0146s/iter; left time: 1.0051s
Epoch: 10 cost time: 3.9619903564453125
Epoch: 10, Steps: 268 | Train Loss: 0.1816904 Vali Loss: 0.1381423
Updating learning rate to 1.953125e-05
EarlyStopping counter: 6 out of 10
#####   loading best weights   #####
Process: python3 (PID: 545801) is using 884 MiB of GPU memory.
Epoch: 10 | Elapsed Time: 260.4635634422302 s | VRAM usage: 0.86328125 Gb | Train MSE: 0.1950 Train MAE: 0.2495 Vali MSE: 0.1296 Vali MAE: 0.2462 Test MSE: 0.1793 Test MAE: 0.2634



Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=2021, data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=192, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='moving_avg', top_k=5, moving_avg=25, d_model=32, d_ff=32, e_layers=2, dropout=0.1, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=4, train_epochs=10, batch_size=128, patience=10, delta=0.0, learning_rate=0.01, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/baseline/TimeMixer/ETTm2_96_192', save_last=False, break_at=0)
Use GPU: cuda:0
train 34273
val 11329
test 11329
	iters: 100, epoch: 1 | loss: 0.3336607
	speed: 0.0221s/iter; left time: 56.7526s
	iters: 200, epoch: 1 | loss: 0.5227312
	speed: 0.0163s/iter; left time: 40.3564s
Epoch: 1 cost time: 4.681455850601196
Epoch: 1, Steps: 267 | Train Loss: 0.3533194 Vali Loss: 0.1732703
Updating learning rate to 0.01
Validation loss decreased (inf --> 0.1733).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.3201250
	speed: 0.0359s/iter; left time: 82.6573s
	iters: 200, epoch: 2 | loss: 0.3245212
	speed: 0.0158s/iter; left time: 34.7993s
Epoch: 2 cost time: 4.332762956619263
Epoch: 2, Steps: 267 | Train Loss: 0.3274528 Vali Loss: 0.1727266
Updating learning rate to 0.005
Validation loss decreased (0.1733 --> 0.1727).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.3951710
	speed: 0.0365s/iter; left time: 74.3623s
	iters: 200, epoch: 3 | loss: 0.2297445
	speed: 0.0164s/iter; left time: 31.7426s
Epoch: 3 cost time: 4.493160247802734
Epoch: 3, Steps: 267 | Train Loss: 0.3080047 Vali Loss: 0.1722411
Updating learning rate to 0.0025
Validation loss decreased (0.1727 --> 0.1722).  Saving model state dict ...
	iters: 100, epoch: 4 | loss: 0.2844672
	speed: 0.0365s/iter; left time: 64.5648s
	iters: 200, epoch: 4 | loss: 0.3805761
	speed: 0.0159s/iter; left time: 26.6006s
Epoch: 4 cost time: 4.38339376449585
Epoch: 4, Steps: 267 | Train Loss: 0.2950844 Vali Loss: 0.1792072
Updating learning rate to 0.00125
EarlyStopping counter: 1 out of 10
	iters: 100, epoch: 5 | loss: 0.3624701
	speed: 0.0357s/iter; left time: 53.6841s
	iters: 200, epoch: 5 | loss: 0.2300258
	speed: 0.0158s/iter; left time: 22.2358s
Epoch: 5 cost time: 4.310385227203369
Epoch: 5, Steps: 267 | Train Loss: 0.2816765 Vali Loss: 0.1826931
Updating learning rate to 0.000625
EarlyStopping counter: 2 out of 10
	iters: 100, epoch: 6 | loss: 0.3268452
	speed: 0.0356s/iter; left time: 43.9625s
	iters: 200, epoch: 6 | loss: 0.2262445
	speed: 0.0160s/iter; left time: 18.1742s
Epoch: 6 cost time: 4.322856664657593
Epoch: 6, Steps: 267 | Train Loss: 0.2702624 Vali Loss: 0.1898368
Updating learning rate to 0.0003125
EarlyStopping counter: 3 out of 10
	iters: 100, epoch: 7 | loss: 0.2664555
	speed: 0.0352s/iter; left time: 34.1440s
	iters: 200, epoch: 7 | loss: 0.2461677
	speed: 0.0159s/iter; left time: 13.7988s
Epoch: 7 cost time: 4.3177406787872314
Epoch: 7, Steps: 267 | Train Loss: 0.2633050 Vali Loss: 0.1881600
Updating learning rate to 0.00015625
EarlyStopping counter: 4 out of 10
	iters: 100, epoch: 8 | loss: 0.2421191
	speed: 0.0354s/iter; left time: 24.8802s
	iters: 200, epoch: 8 | loss: 0.2520297
	speed: 0.0157s/iter; left time: 9.4729s
Epoch: 8 cost time: 4.281384229660034
Epoch: 8, Steps: 267 | Train Loss: 0.2595791 Vali Loss: 0.1909990
Updating learning rate to 7.8125e-05
EarlyStopping counter: 5 out of 10
	iters: 100, epoch: 9 | loss: 0.2791191
	speed: 0.0353s/iter; left time: 15.3769s
	iters: 200, epoch: 9 | loss: 0.2694918
	speed: 0.0158s/iter; left time: 5.3055s
Epoch: 9 cost time: 4.3235931396484375
Epoch: 9, Steps: 267 | Train Loss: 0.2575355 Vali Loss: 0.1909914
Updating learning rate to 3.90625e-05
EarlyStopping counter: 6 out of 10
	iters: 100, epoch: 10 | loss: 0.2220204
	speed: 0.0353s/iter; left time: 5.9378s
	iters: 200, epoch: 10 | loss: 0.2124097
	speed: 0.0158s/iter; left time: 1.0736s
Epoch: 10 cost time: 4.3048481941223145
Epoch: 10, Steps: 267 | Train Loss: 0.2558871 Vali Loss: 0.1917687
Updating learning rate to 1.953125e-05
EarlyStopping counter: 7 out of 10
#####   loading best weights   #####
Process: python3 (PID: 546847) is using 1012 MiB of GPU memory.
Epoch: 10 | Elapsed Time: 288.31198048591614 s | VRAM usage: 0.98828125 Gb | Train MSE: 0.2956 Train MAE: 0.3004 Vali MSE: 0.1722 Vali MAE: 0.2864 Test MSE: 0.2417 Test MAE: 0.3060



Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=2021, data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=336, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='moving_avg', top_k=5, moving_avg=25, d_model=32, d_ff=32, e_layers=2, dropout=0.1, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=4, train_epochs=10, batch_size=128, patience=10, delta=0.0, learning_rate=0.01, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/baseline/TimeMixer/ETTm2_96_336', save_last=False, break_at=0)
Use GPU: cuda:0
train 34129
val 11185
test 11185
	iters: 100, epoch: 1 | loss: 0.3427185
	speed: 0.0235s/iter; left time: 60.3083s
	iters: 200, epoch: 1 | loss: 0.3862628
	speed: 0.0175s/iter; left time: 42.9780s
Epoch: 1 cost time: 5.013296127319336
Epoch: 1, Steps: 266 | Train Loss: 0.4664964 Vali Loss: 0.2233441
Updating learning rate to 0.01
Validation loss decreased (inf --> 0.2233).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.2930429
	speed: 0.0386s/iter; left time: 88.5370s
	iters: 200, epoch: 2 | loss: 0.4218486
	speed: 0.0180s/iter; left time: 39.5840s
Epoch: 2 cost time: 4.823561668395996
Epoch: 2, Steps: 266 | Train Loss: 0.4375260 Vali Loss: 0.2231235
Updating learning rate to 0.005
Validation loss decreased (0.2233 --> 0.2231).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.5171365
	speed: 0.0385s/iter; left time: 78.0180s
	iters: 200, epoch: 3 | loss: 0.4022464
	speed: 0.0173s/iter; left time: 33.3892s
Epoch: 3 cost time: 4.692570209503174
Epoch: 3, Steps: 266 | Train Loss: 0.4166328 Vali Loss: 0.2175529
Updating learning rate to 0.0025
Validation loss decreased (0.2231 --> 0.2176).  Saving model state dict ...
	iters: 100, epoch: 4 | loss: 0.2943938
	speed: 0.0384s/iter; left time: 67.6236s
	iters: 200, epoch: 4 | loss: 0.3069604
	speed: 0.0173s/iter; left time: 28.7867s
Epoch: 4 cost time: 4.695360422134399
Epoch: 4, Steps: 266 | Train Loss: 0.3992888 Vali Loss: 0.2216877
Updating learning rate to 0.00125
EarlyStopping counter: 1 out of 10
	iters: 100, epoch: 5 | loss: 0.3405262
	speed: 0.0382s/iter; left time: 57.2237s
	iters: 200, epoch: 5 | loss: 0.4427114
	speed: 0.0174s/iter; left time: 24.3730s
Epoch: 5 cost time: 4.716553688049316
Epoch: 5, Steps: 266 | Train Loss: 0.3835346 Vali Loss: 0.2233880
Updating learning rate to 0.000625
EarlyStopping counter: 2 out of 10
	iters: 100, epoch: 6 | loss: 0.4216897
	speed: 0.0388s/iter; left time: 47.8050s
	iters: 200, epoch: 6 | loss: 0.3076049
	speed: 0.0173s/iter; left time: 19.5970s
Epoch: 6 cost time: 4.716597318649292
Epoch: 6, Steps: 266 | Train Loss: 0.3732958 Vali Loss: 0.2300489
Updating learning rate to 0.0003125
EarlyStopping counter: 3 out of 10
	iters: 100, epoch: 7 | loss: 0.4297113
	speed: 0.0387s/iter; left time: 37.3211s
	iters: 200, epoch: 7 | loss: 0.4247113
	speed: 0.0179s/iter; left time: 15.4583s
Epoch: 7 cost time: 4.741101503372192
Epoch: 7, Steps: 266 | Train Loss: 0.3649047 Vali Loss: 0.2297013
Updating learning rate to 0.00015625
EarlyStopping counter: 4 out of 10
	iters: 100, epoch: 8 | loss: 0.3599665
	speed: 0.0383s/iter; left time: 26.7878s
	iters: 200, epoch: 8 | loss: 0.4538571
	speed: 0.0174s/iter; left time: 10.4217s
Epoch: 8 cost time: 4.706955194473267
Epoch: 8, Steps: 266 | Train Loss: 0.3607670 Vali Loss: 0.2321200
Updating learning rate to 7.8125e-05
EarlyStopping counter: 5 out of 10
	iters: 100, epoch: 9 | loss: 0.3253670
	speed: 0.0384s/iter; left time: 16.6243s
	iters: 200, epoch: 9 | loss: 0.2326003
	speed: 0.0175s/iter; left time: 5.8185s
Epoch: 9 cost time: 4.710681438446045
Epoch: 9, Steps: 266 | Train Loss: 0.3584598 Vali Loss: 0.2335502
Updating learning rate to 3.90625e-05
EarlyStopping counter: 6 out of 10
	iters: 100, epoch: 10 | loss: 0.2292781
	speed: 0.0384s/iter; left time: 6.4093s
	iters: 200, epoch: 10 | loss: 0.2746338
	speed: 0.0175s/iter; left time: 1.1745s
Epoch: 10 cost time: 4.722362756729126
Epoch: 10, Steps: 266 | Train Loss: 0.3566252 Vali Loss: 0.2341242
Updating learning rate to 1.953125e-05
EarlyStopping counter: 7 out of 10
#####   loading best weights   #####
Process: python3 (PID: 547811) is using 1148 MiB of GPU memory.
Epoch: 10 | Elapsed Time: 311.0888886451721 s | VRAM usage: 1.12109375 Gb | Train MSE: 0.4047 Train MAE: 0.3476 Vali MSE: 0.2176 Vali MAE: 0.3209 Test MSE: 0.3015 Test MAE: 0.3438



Args in experiment:
Namespace(model='TimeMixer', task_name='long_term_forecast', seed=2021, data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', train_prop=1.0, seq_len=96, label_len=0, pred_len=720, seasonal_patterns='Monthly', inverse=False, enc_in=7, dec_in=7, c_out=7, decomp_method='moving_avg', top_k=5, moving_avg=25, d_model=32, d_ff=32, e_layers=2, dropout=0.1, embed='timeF', channel_independence=1, use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, output_attention=False, num_workers=4, train_epochs=10, batch_size=128, patience=10, delta=0.0, learning_rate=0.01, des='test', loss='MSE', lradj='type1', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_dir='./checkpoints/baseline/TimeMixer/ETTm2_96_720', save_last=False, break_at=0)
Use GPU: cuda:0
train 33745
val 10801
test 10801
	iters: 100, epoch: 1 | loss: 0.5736542
	speed: 0.0264s/iter; left time: 66.9037s
	iters: 200, epoch: 1 | loss: 0.4310288
	speed: 0.0207s/iter; left time: 50.2405s
Epoch: 1 cost time: 5.763657093048096
Epoch: 1, Steps: 263 | Train Loss: 0.6121526 Vali Loss: 0.2949146
Updating learning rate to 0.01
Validation loss decreased (inf --> 0.2949).  Saving model state dict ...
	iters: 100, epoch: 2 | loss: 0.4296055
	speed: 0.0444s/iter; left time: 100.6160s
	iters: 200, epoch: 2 | loss: 0.4514118
	speed: 0.0207s/iter; left time: 44.7835s
Epoch: 2 cost time: 5.522831678390503
Epoch: 2, Steps: 263 | Train Loss: 0.5841607 Vali Loss: 0.2926020
Updating learning rate to 0.005
Validation loss decreased (0.2949 --> 0.2926).  Saving model state dict ...
	iters: 100, epoch: 3 | loss: 0.4639214
	speed: 0.0447s/iter; left time: 89.6032s
	iters: 200, epoch: 3 | loss: 0.6525362
	speed: 0.0214s/iter; left time: 40.7856s
Epoch: 3 cost time: 5.6026928424835205
Epoch: 3, Steps: 263 | Train Loss: 0.5607770 Vali Loss: 0.2848588
Updating learning rate to 0.0025
Validation loss decreased (0.2926 --> 0.2849).  Saving model state dict ...
	iters: 100, epoch: 4 | loss: 0.6451524
	speed: 0.0449s/iter; left time: 78.2105s
	iters: 200, epoch: 4 | loss: 0.5907591
	speed: 0.0213s/iter; left time: 35.0472s
Epoch: 4 cost time: 5.6300342082977295
Epoch: 4, Steps: 263 | Train Loss: 0.5434437 Vali Loss: 0.2944134
Updating learning rate to 0.00125
EarlyStopping counter: 1 out of 10
	iters: 100, epoch: 5 | loss: 0.6937034
	speed: 0.0461s/iter; left time: 68.1479s
	iters: 200, epoch: 5 | loss: 0.6642023
	speed: 0.0214s/iter; left time: 29.5494s
Epoch: 5 cost time: 5.6953887939453125
Epoch: 5, Steps: 263 | Train Loss: 0.5261129 Vali Loss: 0.2839421
Updating learning rate to 0.000625
Validation loss decreased (0.2849 --> 0.2839).  Saving model state dict ...
	iters: 100, epoch: 6 | loss: 0.4813559
	speed: 0.0460s/iter; left time: 55.9101s
	iters: 200, epoch: 6 | loss: 0.4356689
	speed: 0.0215s/iter; left time: 24.0079s
Epoch: 6 cost time: 5.722512722015381
Epoch: 6, Steps: 263 | Train Loss: 0.5147612 Vali Loss: 0.2932884
Updating learning rate to 0.0003125
EarlyStopping counter: 1 out of 10
	iters: 100, epoch: 7 | loss: 0.4364207
	speed: 0.0455s/iter; left time: 43.3902s
	iters: 200, epoch: 7 | loss: 0.5044236
	speed: 0.0211s/iter; left time: 17.9769s
Epoch: 7 cost time: 5.619210243225098
Epoch: 7, Steps: 263 | Train Loss: 0.5086862 Vali Loss: 0.2922717
Updating learning rate to 0.00015625
EarlyStopping counter: 2 out of 10
	iters: 100, epoch: 8 | loss: 0.3572632
	speed: 0.0455s/iter; left time: 31.3863s
	iters: 200, epoch: 8 | loss: 0.5090182
	speed: 0.0211s/iter; left time: 12.4513s
Epoch: 8 cost time: 5.6356329917907715
Epoch: 8, Steps: 263 | Train Loss: 0.5047769 Vali Loss: 0.2929355
Updating learning rate to 7.8125e-05
EarlyStopping counter: 3 out of 10
	iters: 100, epoch: 9 | loss: 0.4724724
	speed: 0.0457s/iter; left time: 19.5116s
	iters: 200, epoch: 9 | loss: 0.4956089
	speed: 0.0219s/iter; left time: 7.1724s
Epoch: 9 cost time: 5.7740232944488525
Epoch: 9, Steps: 263 | Train Loss: 0.5032197 Vali Loss: 0.2919529
Updating learning rate to 3.90625e-05
EarlyStopping counter: 4 out of 10
	iters: 100, epoch: 10 | loss: 0.3790474
	speed: 0.0469s/iter; left time: 7.6865s
	iters: 200, epoch: 10 | loss: 0.5013262
	speed: 0.0210s/iter; left time: 1.3447s
Epoch: 10 cost time: 5.709982395172119
Epoch: 10, Steps: 263 | Train Loss: 0.5019269 Vali Loss: 0.2933248
Updating learning rate to 1.953125e-05
EarlyStopping counter: 5 out of 10
#####   loading best weights   #####
Process: python3 (PID: 548873) is using 1298 MiB of GPU memory.
Epoch: 10 | Elapsed Time: 367.2219789028168 s | VRAM usage: 1.267578125 Gb | Train MSE: 0.5144 Train MAE: 0.4029 Vali MSE: 0.2839 Vali MAE: 0.3660 Test MSE: 0.3991 Test MAE: 0.4025



